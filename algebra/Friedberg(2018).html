<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>Linear Algebra, Friedberg(2018)</title>
  <style>
    body {
      margin-left: 1.2rem;
    }
    th, td {
      padding-right: 1.2rem;
      vertical-align: text-bottom;
    }
    th {
      text-align: left;
    }
    section, article {
      margin-top: 1rem;
      margin-left: 2rem;
      margin-bottom: 1rem;
    }
    article {
      margin-right: 2rem;
    }
    .definition, .proposition, .description {
      padding-right: 1rem;
      padding-left: 1rem;
    }
    .definition {
      padding-top: 0.2rem;
      padding-bottom: 0.2rem;
      background-color: lightgrey;
    }
    .center {
      text-align: center;
    }
    .right {
      text-align: right;
    }
  </style>
</head>
<body>
<article>
<h1>Linear Algebra [線形代数]</h1>
  <section id="fields">
    <h2>Fields (Appendix C)</h2>
    <section id="definition-field" class="definition">
      <p>
        <strong>Definition (field [体])</strong>:
        A field <var>F</var> is a set on which two operations (addition and multiplication) are defined so that for each pair of elements <var>a, b</var> &in; <var>F</var>,
        <ul>
          <li>
            there is a unique element
            <var>a</var>+<var>b</var> &in; <var>F</var>
          </li>
          <li>
            there is a unique element
            <var>a</var>&middot;<var>b</var> &in; <var>F</var>
          </li>
        </ul>
        such that the field axioms hold for all elements <var>a, b, c</var> &in; <var>F</var>.
      </p>
      <p>
        <strong>Axiom (Field axioms [体の公理])</strong>:
        <table>
          <tr>
            <th></th>
            <th>addition</th>
            <th>multiplication</th>
          </tr>
          <tr>
            <td>commutativity [交換則]</td>
            <td>a+b = b+a</td>
            <td>a&middot;b = b&middot;a</td>
          </tr>
          <tr>
            <td>associativity [結合則]</td>
            <td>(a+b)+c = a+(b+c)</td>
            <td>(a&middot;b)&middot;c = a&middot;(b&middot;c)</td>
          </tr>
          <tr>
            <td>identity [単位元]</td>
            <td>&exist;0 &in; F, 0+a = a</td>
            <td>&exist;1 &in; F, 1&middot;a = a</td>
          </tr>
          <tr>
            <td>inverse [逆元]</td>
            <td>&exist;(&minus;a) &in; F, a+(&minus;a) = 0</td>
            <td>&exist;(a<sup>-1</sup>) &in; F, a&middot;a<sup>-1</sup> = 1 (if a&NotEqual;0)</td>
          </tr>
          <tr>
            <td>distributivity [分配則]</td>
            <td colspan="2" class="center">
              a&middot;(b+c) = a&middot;b+a&middot;c
            </td>
          </tr>
        </table>
      </p>
      <p>
        The zero ring can be avoided by including the zero-one law (0 &ne; 1).
      </p>
    </section>
    <section id="cancellation-laws" class="proposition">
      <p>
        <strong>Theorem (cancellation laws [簡約律])</strong>:
        Let <var>F</var> be a field. For all elements <var>a, b, c</var> &in; <var>F</var> and all <var>d</var>&ne;0 &in; <var>F</var>,
        <section>
          <table>
            <tr>
              <td>1.</td>
              <td>a+b = c+b</td>
              <td>&Implies;</td>
              <td>a = c</td>
            </tr>
            <tr>
              <td>2.</td>
              <td>a&middot;d = c&middot;d</td>
              <td>&Implies;</td>
              <td>a = c</td>
            </tr>
          </table>
        </section>
        Proof:
        <ol>
          <li>
            There exists an element <var>d</var> &in; <var>F</var> such that <var>b</var>+<var>d</var> = <var>0</var>.
            <section>
              <table>
                <tr>
                  <td>
                    (a+b)+d = (c+b)+d
                  </td>
                  <td>by adding <var>d</var> to both sides</td>
                </tr>
                <tr>
                  <td>
                    a+(b+d) = c+(b+d)
                  </td>
                  <td>associativity of addition</td>
                </tr>
                <tr>
                  <td>a+0 = c+0</td>
                  <td>&because; b+d = 0</td>
                </tr>
                <tr>
                  <td>a = c</td>
                  <td>existence of additive identity</td>
                </tr>
              </table>
            </section>
          </li>
          <li>
            If <var>d</var> &NotEqual; <var>0</var>, there exists an element <var>e</var> such that <var>d</var>&middot;<var>e</var> = <var>1</var>.
            <section>
              <table>
                <tr>
                  <td>
                    (a&middot;d)&middot;e
                    =
                    (c&middot;d)&middot;e
                  </td>
                  <td>by multiplying both sides by <var>e</td>
                </tr>
                <tr>
                  <td>
                    a&middot;(d&middot;e)
                    =
                    c&middot;(d&middot;e)
                  </td>
                  <td>associativity of multiplication</td>
                </tr>
                <tr>
                  <td>a&middot;1 = c&middot;1</td>
                  <td>&because; d&middot;e = 1</td>
                </tr>
                <tr>
                  <td>a = c</td>
                  <td>
                    existence of multiplicative identity
                    &marker;
                  </td>
                </tr>
              </table>
            </section>
          </li>
        </ol>
      </p>
    </section>
    <section id="unique-identity-inverse" class="proposition">
      <p>
        <strong>Corollary (unique identities and inverses)</strong>:
        Let <var>F</var> be a field and <var>a</var> &in; <var>F</var>, then
        <ul>
          <li>Additive identity (0) is unique.</li>
          <li>Additive inverse (<var>&minus;a</var>) is unique.</li>
          <li>Multiplicative identity (1) is unique.</li>
          <li>
            Multiplicative inverse (<var>a<sup>-1</sup></var> if <var>a</var>  &NotEqual; 0) is unique.
          </li>
        </ul>
        Proof:
        <ul>
          <li>
            Additive identity (0): Suppose that 0&prime; &in; <var>F</var> is an additive identity.
            <section>
              <table>
                <tr>
                  <td>0&prime; + a</td>
                  <td>= a</td>
                  <td>&because; 0&prime; is an additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0 + a</td>
                  <td>&because; 0 is an additive identity</td>
                </tr>
                <tr>
                  <td>0&prime;</td>
                  <td>= 0</td>
                  <td>additive cancellation law</td>
              </table>
            </section>
          </li>
          <li>
            Other three proofs are similar. &marker;
          </li>
        </ul>
      </p>
    </section>
    <section id="substraction-division" class="definition">
      <p>
        <strong>Definition (Subtraction and Division)</strong>: Subtraction (&minus;) and division (&divide; or &frasl;) can be defined in terms of addition and multiplication by using inverses. Let <var>F</var> be a field. For all <var>a</var> &in; <var>F</var> and all <var>b</var> &NotEqual; 0 &in; <var>F</var>,
        <section>
          <table>
            <tr>
              <td>a&minus;b = a+(&minus;b)</td>
            </tr>
            <tr>
              <td>
                a&divide;b
                = <sup>a</sup>&frasl;<sub>b</sub>
                = a&middot;b<sup>-1</sup>
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="multiplications-in-fields" class="proposition">
      <p>
        <strong>Corollary (multiplications in fields)</strong>:
        Let <var>F</var> be a field. For all <var>a, b</var> &in; <var>F</var>,
        <ol>
          <li>a&middot;0 = 0</li>
          <li>
            (&minus;a)&middot;b = a&middot;(&minus;b) = &minus;(a&middot;b)
          </li>
          <li>(&minus;a)&middot;(&minus;b) = a&middot;b</li>
        </ol>
        Proof:
        <ol>
          <li>
            a&middot;0 = 0
            <section>
              <table>
                <tr>
                  <td>a&middot;0+a&middot;0</td>
                  <td>= a&middot;(0+0)</td>
                  <td>distributivity of &middot; over +</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a&middot;0</td>
                  <td>&because; 0+0 = 0</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a&middot;0+0</td>
                  <td>existence of additive identity</td>
                </tr>
                <tr>
                  <td>a&middot;0</td>
                  <td>= 0</td>
                  <td>additive cancellation law</td>
                </tr>
              </table>
            </section>
          </li>
          <li>
            (&minus;a)&middot;b = &minus;(a&middot;b)
            <section>
              <table>
                <tr>
                  <td>a&middot;b+(&minus;a)&middot;b</td>
                  <td>= {a+(&minus;a)}&middot;b</td>
                  <td>distributivity of &middot; over +</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0&middot;b</td>
                  <td>existence of additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0</td>
                  <td>this corollary 1</td>
                </tr>
              </table>
            </section>
          </li>
          <li>
            (&minus;a)&middot;(&minus;b) = a&middot;b
            <section>
              <table>
                <tr>
                  <td>(&minus;a)&middot;(&minus;b)</td>
                  <td>= &minus;{a&middot;(&minus;b)}</td>
                  <td>by applying (2)</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= &minus;{&minus;(a&middot;b)}</td>
                  <td>by applying (2)</td>
                </tr>
                <tr>
                  <td></td>
                  <td>
                    = a&middot;b
                    &marker;
                  </td>
                </tr>
              </table>
            </section>
          </li>
        </ol>
      </p>
    </section>
    <section id="zero-ring" class="description">
      <p>
        <strong>Zero ring</strong>:
        In Ring theory, the zero ring is a set consisting of a single element <var>0</var> with the operations + and &middot; defined such that 0+0 = 0 and 0&middot;0 = 0. [Wikipedia: Zero ring].
      </p>
    </section>
    <section id="if-isentities-equal" class="proposition">
      <p>
        <strong>Proposition (zero ring characteristics)</strong>:
        If the additive identity <var>0</var> equals the multiplicative identity <var>1</var>, then the ring <var>R</var> has only a single element <var>0</var> = <var>1</var>.
      </p>
      <p>
        Proof: for all <var>r</var> &in; <var>R</var>,
        <section>
          <table>
            <tr>
              <td>r</td>
              <td>= 1r</td>
              <td>the multiplicative identity 1</td>
            </tr>
            <tr>
              <td></td>
              <td>= 0r</td>
              <td>since 0 = 1</td>
            </tr>
            <tr>
              <td></td>
              <td>= 0</td>
              <td>
                Corollary (multiplications in fields) #1
                &marker;
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
  </section>
  <section id="vector-spaces">
    <h2>Introduction</h2>
    <section id="parallelogram-law" class="proposition">
      <p>
        <strong>Parallelogram law [中線定理] for vector addition</strong>:
        The sum of two vectors <var>x</var> and <var>y</var> that act at the same point <var>P</var> is
        <section>
          the vector beginning at <var>P</var> that is represented by the diagonal [対角線] of parallelogram [平行四辺形] having <var>x</var> and <var>y</var> as adjacent sides [隣接する辺].
        </section>
      </p>
    </section>
    <h2>Vector Spaces</h2>
    <section id="define-vector-space" class="definition">
      <p>
        <strong>Definition (vector space [ベクトル空間])</strong>:
        A vector space <var>V</var> over a field <var>F</var> consists of a set on which two operations (vector addition and scalar multiplication [スカラー倍]) are defined so that
        <ul>
          <li>
            there is a unique element <var>x+y</var> &in; <var>V</var>
          </li>
          <li>
            there is a unique element <var>ax</var> &in; <var>V</var>
          </li>
        </ul>
        such that the vector space axioms hold for each <var>a, b</var> &in; <var>F</var> and for each <var>x, y, z</var> &in; <var>V</var>.
      </p>
      <p>
        <strong>Axiom (vector space axioms)</strong>
        <section>
          <table>
            <tr>
              <th></th>
              <th>vector addition</th>
              <th>scalar multiplication</th>
            </tr>
            <tr>
              <td>commutativity</td>
              <td>x+y = y+x</td>
              <td></td>
            </tr>
            <tr>
              <td>associativity</td>
              <td>(x+y)+z = x+(y+z)</td>
              <td>(ab)x = a(bx)</td>
            </tr>
            <tr>
              <td>existence of identity</td>
              <td>&exist;0 &in; V, x+0 = x</td>
              <td>1x = x</td>
            </tr>
            <tr>
              <td>existence of inverse</td>
              <td>&exist;(&minus;x) &in; V, x+(&minus;x) = 0</td>
              <td></td>
            </tr>
            <tr>
              <td>distributivity</td>
              <td>a(x+y) = ax+ay</td>
              <td>(a+b)x = ax+bx</td>
            </tr>
          </table>
        </section>
        The additive identity, denoted <var>0</var>, is called the zero vector.
      </p>
    </section>
    <section id="zero-in-vector-space" class="proposition">
      <p>
        Existence of vector additive identity of the vector space axioms tells:
        <ul>
          <li>any vector space must contain <var>0</var></li>
          <li>&empty; is not a vector space</li>
        </ul>
      </p>
    </section>
    <section id="cancellation-law-vector" class="proposition">
      <p>
        <strong>Theorem 1.1 (cancellation law for vector addition)</strong>:
        Let <var>V</var> be a vector space, and <var>x, y, z</var> &in; <var>V</var>, then
        <section>
          <table>
            <tr>
              <td>x+z = y+z</td>
              <td>&Implies;</td>
              <td>x = y</td>
            </tr>
          </table>
        </section>
        Proof:
        By the vector space axiom, we know that there exists an additive inverse, say <var>u</var>, of the vector <var>z</var> such that
        <section>
          <section>
            z+u = 0
          </section>
          Thus
          <section>
            <table>
              <tr>
                <td>x</td>
                <td>= x+0</td>
                <td><var>0</var> is the additive identity</td>
              </tr>
              <tr>
                <td></td>
                <td>= x+(z+u)</td>
                <td>since z+u = 0</td>
              </tr>
              <tr>
                <td></td>
                <td>= (x+z)+u</td>
                <td>by additive associativity</td>
              </tr>
              <tr>
                <td></td>
                <td>= (y+z)+u</td>
                <td>since x+z = y+z</td>
              </tr>
              <tr>
                <td></td>
                <td>= y+(z+u)</td>
                <td>by additive associativity</td>
              </tr>
              <tr>
                <td></td>
                <td>= y+0</td>
                <td>since z+u = 0</td>
              </tr>
              <tr>
                <td></td>
                <td>= y</td>
                <td><var>0</var> is the additive identity &marker;</td>
              </tr>
            </table>
          </section>
        </section>
      </p>
    </section>
    <section id="unique-zero-vector" class="proposition">
      <p>
        <strong>Corollary (1: unique zero vector)</strong>:
        The zero vector is unique.
      </p>
      <p>
        Proof:
        Let <var>V</var> be a vector space, and <var>x &in; V</var>. Suppose that <var>0&prime; &in; V</var> is an zero vector, too. Then,
        <section>
          <section>
            <table>
              <tr>
                <td>0&prime; + x</td>
                <td>= x</td>
                <td>since 0&prime; is an zero vector</td>
              </tr>
              <tr>
                <td></td>
                <td>= 0 + x</td>
                <td>since 0 is also an zero vector</td>
              </tr>
              <tr>
                <td>0&prime;</td>
                <td>= 0</td>
                <td>
                  the cancellation law
                  &marker;
                </td>
              </tr>
            </table>
          </section>
        </section>
      </p>
    </section>
    <section id="unique-additive-inverse" class="proposition">
      <p>
        <strong>Corollary (2: unique additive inverse)</strong>:
        The additive inverse vector is unique.
      </p>
      <p>
        Proof:
        Suppose that for a vector <var>x</var>, there exist two distinct additive inverses. Since the zero vector is unique,
        <section>
          <section>
            0 = x+(&minus;x) = x+(&minus;x&prime;)
          </section>
          Thus &minus;x = &minus;x&prime; by cancellation law for vector addition, it follows that the additive inverse is unique.
        </section>
      </p>
    </section>
    <section id="theorem-scalar-multiplications" class="proposition">
      <p>
        <strong>Theorem 1.2 (scalar multiplications in vector spaces)</strong>:
        In any vector space <var>V</var> over a field <var>F</var>, the following statements are true:  for each <var>a</var> &in; <var>F</var> and for each <var>x</var> &in; <var>V</var>,
        <section>
          <table>
            <tr>
              <td>[1]</td>
              <td>0x = 0</td>
            </tr>
            <tr>
              <td>[2]</td>
              <td>
                (&minus;a)x
                = &minus;(ax)
              </td>
            </tr>
            <tr>
              <td>[3]</td>
              <td>
                a(&minus;x)
                = &minus;(ax)
              </td>
            </tr>
            <tr>
              <td>[4]</td>
              <td>a0 = 0</td>
            </tr>
          </table>
        </section>
        Proof:
        <section>
          <p>
            [1]: By the vector space axioms and the cancellation law for vector addition,
            <section>
              <table>
                <tr>
                  <td>0x+0x</td>
                  <td>= (0+0)x</td>
                  <td>by scalar multiplicative distributivity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0x</td>
                  <td>the field additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0x+0</td>
                  <td>the vector additive identity</td>
                </tr>
                <tr>
                  <td colspan="2" class="center">
                    0x = 0
                  </td>
                  <td>by the cancellation law for vector addition</td>
                </tr>
              </table>
            </section>
          </p>
          <p>
            [2]: By the field axioms, we know that there exists an additive inverse for some scalar <var>a</var>, denoted &minus;<var>a</var>,
            <section>
              <table>
                <tr>
                  <td>ax+(&minus;a)x</td>
                  <td>= {a+(&minus;a)}x</td>
                  <td>by scalar multiplicative distributivity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0x</td>
                  <td>
                    the field additive inverse
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0</td>
                  <td>by [1]</td>
                </tr>
              </table>
            </section>
            But by the vector space axioms, we know that there exists an additive inverse for some vector <var>ax</var>, denoted &minus;(ax) such that
            <section>
              ax+{&minus;(ax)} = 0
            </section>
            Because the additive inverse is unique, (&minus;a)x = &minus;(ax).
          </p>
          <p>
            [3]: By the field axioms, the vector space axioms and [2],
            <section>
              <table>
                <tr>
                  <td>a(&minus;x)</td>
                  <td>= a{(&minus;1)x}</td>
                  <td>
                    &because; (&minus;1)x = &minus;x by [2]
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td>= {a(&minus;1)}x</td>
                  <td>by scalar multiplicative associativity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= (&minus;a)x</td>
                  <td>by field multiplication</td>
                </tr>
              </table>
            </section>
          </p>
          <p>
            [4]: By the vector space axioms and the cancellation low for vector addition,
            <section>
              <table>
                <tr>
                  <td>a0+a0</td>
                  <td>= a(0+0)</td>
                  <td>by vector additive associativity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a0</td>
                  <td>the vector additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a0+0</td>
                  <td>the vector additive identity</td>
                </tr>
                <tr>
                  <td colspan="2" class="center">a0 = 0</td>
                  <td>by the cancellation low for vector addition</td>
                </tr>
              </table>
            </section>
          </p>
        </section>
      </p>
    </section>
    <section id="zero-vector-space" class="proposition">
      <p>
        <strong>Proposition (exercise 1.2.11: zero vector space)</strong>:
        {0} consists of a single zero vector. {0} is a vector space over a field <var>F</var> (called the zero vector space).
      </p>
      <p>
        Proof: For any scalar <var>c</var> in <var>F</var>,
        <section>
          <section>
            <table>
              <tr>
                <td>0+0 = 0 &in; {0}</td>
                <td>closed under vector addition</td>
              </tr>
              <tr>
                <td>c0 = 0 &in; {0}</td>
                <td>closed under scalar multiplication</td>
              </tr>
            </table>
          </section>
          And it is immediate that the vector space axioms hold for {0}. &marker;
        </section>
      </p>
    </section>
  </section>
  <section id="subspaces">
    <h2>Subspaces</h2>
    <section id="define-subspace" class="definition">
      <p>
        <strong>Definition (subspace 部分空間)</strong>:
        A subset of a vector space <var>V</var> over a field <var>F</var> is called a subspace of <var>V</var> if the subset is a vector space over <var>F</var> with the operations of addition and scalar multiplication defined on <var>V</var>.
      </p>
    </section>
    <section id="subspace-conditions" class="proposition">
      <p>
        <strong>Theorem 1.3 (subspace conditions)</strong>:
        Let <var>W</var> be a subset of a vector space <var>V</var>, then        <var>W</var> is a subspace of <var>V</var> if and only if the three conditions hold for the operations defined in <var>V</var>. For some <var>x</var>, <var>y</var> &in; <var>W</var> and some <var>c</var> &in; <var>F</var>,
        <section>
          <table>
            <tr>
              <td></td>
              <td>any subspace must</td>
            </tr>
            <tr>
              <td>0 &in; W</td>
              <td>
                contain the zero vector
              </td>
            </tr>
            <tr>
              <td>x+y &in; W</td>
              <td>
                be closed under vector addition
              </td>
            </tr>
            <tr>
              <td>cx &in; W</td>
              <td>
                be closed under scalar multiplication
              </td>
            </tr>
          </table>
        </section>
      </p>
      <p>
        Proof:
        If <var>W</var> is a subspace of <var>V</var>, then
        <ul>
          <li>
            Let 0&prime; be an additive identity in <var>W</var> such that <var>x</var>+0&prime; = <var>x</var> for each <var>x</var> &in; <var>W</var>. But also <var>x</var>+0 = <var>x</var> in the vector space <var>V</var>, thus 0&prime; = 0 by the cancellation law for vector addition. Hence 0 &in; <var>W</var>.
          </li>
          <li>
            By definition of the subspace, <var>W</var> is a vector space with addition and scalar multiplication defined on <var>V</var>. Hence <var>W</var> is closed under addition and scalar multiplication.
          </li>
        </ul>
        Conversely, if the three conditions hold, then the vectors in <var>W</var> need to follow the definition of the vector space.
        <section>
          <table>
            <tr>
              <th></th>
              <th>vector addition</th>
              <th>scalar multiplication</th>
            </tr>
            <tr>
              <td>closed operations</td>
              <td>x+y &in; W</td>
              <td>cx &in; W</td>
            </tr>
            <tr>
              <td>commutativity</td>
              <td>x+y = y+x</td>
              <td></td>
            </tr>
            <tr>
              <td>associativity</td>
              <td>(x+y)+z = x+(y+z)</td>
              <td>(ab)x = a(bx)</td>
            </tr>
            <tr>
              <td>existence of identity</td>
              <td>&exist;0 &in; W, x+0 = x</td>
              <td>1x = x</td>
            </tr>
            <tr>
              <td>existence of inverse</td>
              <td>&exist;(&minus;x) &in; W, x+(&minus;x) = 0</td>
              <td></td>
            </tr>
            <tr>
              <td>distributivity</td>
              <td>a(x+y) = ax+ay</td>
              <td>(a+b)x=ax+bx</td>
            </tr>
          </table>
        </section>
        The properties except for
        <section>
          <table>
            <tr>
              <td>closed under vector addition</td>
              <td>x+y &in; W</td>
            </tr>
            <tr>
              <td>closed under scalar multiplication</td>
              <td>cx &in; W</td>
            </tr>
            <tr>
              <td>existence of the additive identity</td>
              <td>&exist;0 &in; W, x+0 = x</td>
            </tr>
            <tr>
              <td>existence of the additive inverse</td>
              <td>&exist;(&minus;x) &in; W, x+(&minus;x) = 0</td>
            </tr>
          </table>
        </section>
        hold for all vectors in the vector space. Since a subspace is a vector space, these properties automatically hold for the vectors in any subspace. But the hypothesis covers the first three properties, it suffices to prove that the additive inverse of each vector in <var>W</var> lies in <var>W</var>. If <var>x</var> &in; <var>W</var>, then
        <section>
          <table>
            <tr>
              <td>(&minus;1)x &in; W</td>
              <td>
                by the third condition <var>cx</var> &in; <var>W</var>
              </td>
            </tr>
            <tr>
              <td>(&minus;1)x = &minus;x</td>
              <td>
                by Theorem (scalar multiplications in vector spaces)
              </td>
            </tr>
          </table>
        </section>
        Thus &minus;x &in; <var>W</var>, therefore <var>W</var> is a subspace of <var>V</var>. &marker;
      </p>
    </section>
    <section id="vector-space-is-subspace" class="proposition">
      <p>
        <strong>Proposition (vector space is subspace)</strong>:
        <var>V</var> is a subspace of <var>V</var>, because
        <ul>
          <li>0 &in; V</li>
          <li>
            <var>V</var> is a vector space with the operations defined on <var>V</var>.
          </li>
        </ul>
      </p>
    </section>
    <section id="zero-subspace" class="proposition">
      <p>
        <strong>Proposition (zero subspace)</strong>:
        {0} is a subspace of any vector space, because
        <ul>
          <li>
            {0} is a subset of any vector space, since any vector space contains <var>0</var>,
          </li>
          <li>
            we know from Proposition (zero vector space) that {0} is a vector space.
          </li>
        </ul>
    </section>
    <section id="intersection-as-subspace" class="proposition">
      <p>
        <strong>Theorem 1.4 (intersection as subspace)</strong>:
        Any intersection of subspaces of a vector space <var>V</var> is a subspace of <var>V</var>.
      </p>
      <p>
        Proof: Let <var>C</var> be a collection of any subspaces of <var>V</var>, and let <var>W</var> denote the intersection of subspaces in <var>C</var>, then the subspace conditions hold for <var>W</var>.
      </p>
      <p>
        0 &in; W:
        <section>
          <p>
            Since every subspace contains the zero vector, each subspace in <var>C</var> contains 0. Hence 0 &in; <var>W</var>.
          </p>
        </section>
        x+y &in; W:
        <section>
          <p>
            Let <var>x</var>, <var>y</var> &in; <var>W</var>, then <var>x</var> and <var>y</var> are contained in each subspace in <var>C</var>. Because each subspace in <var>C</var> is closed under vector addition, it follows that <var>x</var>+<var>y</var> is contained in each subspace in <var>C</var>. Hence <var>x</var>+<var>y</var> &in; <var>W</var>.
          </p>
        </section>
        <var>ax</var> &in; <var>W</var>:
        <section>
          <p>
            Let <var>x</var> &in; <var>W</var>, then <var>x</var> is contained in each subspace in <var>C</var>. Because each subspace in <var>C</var> is closed under scalar multiplication, it follows that for any scalar <var>a</var> in <var>F</var>, <var>ax</var> is contained in each subspace in <var>C</var>. Hence <var>ax</var> &in; <var>W</var>. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="union-as-subspace" class="proposition">
      <p>
        <strong>Proposition (exercise 1.3.19: union as subspace)</strong>:
        Let W<sub>1</sub> and W<sub>2</sub> be subspaces of a vector space <var>V</var>, then (W<sub>1</sub> &Union; W<sub>2</sub>) is a subspace of <var>V</var>
        <section>
          <table>
            <tr>
              <td>if and only if</td>
              <td>
                (W<sub>1</sub> &subseteq; W<sub>2</sub>) &or;
                (W<sub>2</sub> &subseteq; W<sub>1</sub>)
              </td>
            </tr>
          </table>
        </section>
        Proof:
      </p>
      <p>
        If (W<sub>1</sub> &Union; W<sub>2</sub>) is a subspace of <var>V</var>, the subspace conditions hold for (W<sub>1</sub> &Union; W<sub>2</sub>), that is:
        <section>
          <p>
            0 &in; (W<sub>1</sub> &Union; W<sub>2</sub>):
            <section>
              <p>
                Because each subspace contains the zero vector, W<sub>1</sub> contains the zero vector, and so does W<sub>2</sub>. Hence 0 &in; (W<sub>1</sub> &Union; W<sub>2</sub>).
              </p>
            </section>
          </p>
          <p>
            ax &in; (W<sub>1</sub> &Union; W<sub>2</sub>):
            <section>
              <p>
                Let <var>x</var> &in; W<sub>1</sub> (or W<sub>2</sub>) and <var>a</var> &in; <var>F</var>. Because W<sub>1</sub> (or W<sub>2</sub>) is a subspace, <var>ax</var> &in; W<sub>1</sub> (or W<sub>2</sub>). Hence <var>ax</var> &in; (W<sub>1</sub> &Union; W<sub>2</sub>).
              </p>
            </section>
          </p>
          <p>
            x+y &in; (W<sub>1</sub> &Union; W<sub>2</sub>):
            <section>
              <p>
                Recall that for two statements <var>P</var> and <var>Q</var>, (P &Implies; Q) = &not;(P &and; &not;Q). Therefore, we can prove (P &Implies; Q) by showing a contradiction for (P &and; &not;Q). Let
                <section>
                  <table>
                    <tr>
                      <td>P</td>
                      <td>=
                        x+y &in; (W<sub>1</sub> &Union; W<sub>2</sub>)
                      </td>
                    </tr>
                    <tr>
                      <td>Q</td>
                      <td>=
                        (W<sub>1</sub> &subseteq; W<sub>2</sub>) &or;
                        (W<sub>2</sub> &subseteq; W<sub>1</sub>)
                      </td>
                    </tr>
                    <tr>
                      <td>&not;Q</td>
                      <td>=
                        (
                          &Exists;v<sub>1</sub> &in; W<sub>1</sub>,
                          v<sub>1</sub> &notin; W<sub>2</sub>
                        ) &and; (
                          &Exists;v<sub>2</sub> &in; W<sub>2</sub>,
                          v<sub>2</sub> &notin; W<sub>1</sub>
                        )
                      </td>
                    </tr>
                  </table>
                </section>
                With &not;Q, we can find some <var>x</var> and <var>y</var> such that
                <section>
                  <table>
                    <tr>
                      <td>x &in; W<sub>1</sub>&setminus;W<sub>2</sub></td>
                      <td>y &in; W<sub>2</sub>&setminus;W<sub>1</sub></td>
                    </tr>
                  </table>
                </section>
                On the other hand, <var>P</var> tells x+y &in; W<sub>1</sub> or W<sub>2</sub>, say W<sub>1</sub>. Then since x &in; W<sub>1</sub>&setminus;W<sub>2</sub>,
                <section>
                  (x+y) &minus; x = y &in; W<sub>1</sub>
                </section>
                which is a contradiction to <var>y</var> &in; W<sub>2</sub>&setminus;W<sub>1</sub>.
              </p>
            </section>
          </p>
        </section>
      </p>
      <p>
          If W<sub>1</sub> &subseteq; W<sub>2</sub> or W<sub>2</sub> &subseteq; W<sub>1</sub>, then (W<sub>1</sub> &Union; W<sub>2</sub>) is W<sub>1</sub> or W<sub>2</sub>, which are subspaces. &marker;
      </p>
    </section>
    <section id="define-sum-of-subspaces" class="definition">
      <p>
        <strong>Definition (sum of subsets)</strong>:
        The sum of non-empty subsets <var>S<sub>1</sub></var> and <var>S<sub>2</sub></var> of a vector space <var>V</var>, is a set such that
        <section>
          S<sub>1</sub>+S<sub>2</sub> =
          { x+y : x &in; S<sub>1</sub>, y &in; S<sub>2</sub> }
        </section>
      </p>
    </section>
    <section id="elements-of-subset-sum" class="description">
      <p>
        <strong>Description (elements of a sum of subsets)</strong>: Let
        x<sub>1</sub>, x<sub>2</sub>, &hellip;, x<sub>n</sub> &in; S<sub>1</sub> and
        y<sub>1</sub>, y<sub>2</sub>, &hellip;, y<sub>m</sub> &in; S<sub>2</sub>,
        then the possible elements that S<sub>1</sub>+S<sub>2</sub> contains are
        <section>
          <table>
            <tr>
              <td class="center">x<sub>1</sub>+y<sub>1</sub></td>
              <td class="center">x<sub>2</sub>+y<sub>1</sub></td>
              <td class="center">&ctdot;</td>
              <td class="center">x<sub>n</sub>+y<sub>1</sub></td>
            </tr>
            <tr>
              <td class="center">x<sub>1</sub>+y<sub>2</sub></td>
              <td class="center">x<sub>2</sub>+y<sub>2</sub></td>
              <td class="center">&ctdot;</td>
              <td class="center">x<sub>n</sub>+y<sub>2</sub></td>
            </tr>
            <tr>
              <td class="center">&vellip;</td>
              <td class="center">&vellip;</td>
              <td class="center">&vellip;</td>
              <td class="center">&vellip;</td>
            </tr>
            <tr>
              <td class="center">x<sub>1</sub>+y<sub>m</sub></td>
              <td class="center">x<sub>2</sub>+y<sub>m</sub></td>
              <td class="center">&ctdot;</td>
              <td class="center">x<sub>n</sub>+y<sub>m</sub></td>
            </tr>
          </table>
        </section>
        In fact, the number of elements in S<sub>1</sub>+S<sub>2</sub> is equal to or less than n&times;m. For example, let
        <section>
          <table>
            <tr>
              <td>S<sub>1</sub> = {1, 3, 5}</td>
              <td>S<sub>2</sub> = {0, 2, 4}</td>
            </tr>
          </table>
        </section>
        then the possible elements in S<sub>1</sub>+S<sub>2</sub> are
        <section>
          <table>
            <tr class="right"><td>1</td><td>3</td><td>5</td></tr>
            <tr class="right"><td>3</td><td>5</td><td>7</td></tr>
            <tr class="right"><td>5</td><td>7</td><td>9</td></tr>
          </table>
        </section>
        Thus S<sub>1</sub>+S<sub>2</sub> = {1, 3, 5, 7, 9}.
      </p>
    </section>
    <section id="subsets-in-sum" class="proposition">
      <p>
        <strong>Proposition (exercise 1.3.23: subspaces in sum)</strong>:
        Let W<sub>1</sub> and W<sub>2</sub> be subspaces of a vector space <var>V</var>. Then,
        <ol>
          <li>
            W<sub>1</sub>+W<sub>2</sub> is a subspace of <var>V</var> that contains both W<sub>1</sub> and W<sub>2</sub>.
          </li>
          <li>
            Any subspace of <var>V</var> that contains both W<sub>1</sub> and W<sub>2</sub> must also contain W<sub>1</sub>+W<sub>2</sub>.
          </li>
        </ol>
        Proof-1:
        <section>
          <p>0 &in; (W<sub>1</sub>+W<sub>2</sub>):</p>
          <section>
            <p>
              Since every subspace contains the zero vector, 0 &in; W<sub>1</sub> and 0 &in; W<sub>2</sub>. By definition of sum, 0+0 &in; (W<sub>1</sub>+W<sub>2</sub>), it follows that 0 &in; (W<sub>1</sub>+W<sub>2</sub>).
            </p>
          </section>
          <p>z<sub>1</sub>+z<sub>2</sub> &in; (W<sub>1</sub>+W<sub>2</sub>):</p>
          <section>
            <p>
              Let x<sub>1</sub>, x<sub>2</sub> &in; W<sub>1</sub>, and y<sub>1</sub>, y<sub>2</sub> &in; W<sub>2</sub>. Then x<sub>1</sub>+x<sub>2</sub> &in; W<sub>1</sub>, and y<sub>1</sub>+y<sub>2</sub> &in; W<sub>2</sub>, because W<sub>1</sub> and W<sub>2</sub> are subspaces. It follows that by definition of sum of subsets
              <section>
                <table>
                  <tr>
                    <td>
                      (x<sub>1</sub>+x<sub>2</sub>) + (y<sub>1</sub>+y<sub>2</sub>)
                      &in; (W<sub>1</sub>+W<sub>2</sub>)
                    </td>
                  </tr>
                </table>
              </section>
              But we can find two elements z<sub>1</sub>, z<sub>2</sub> &in; (W<sub>1</sub>+W<sub>2</sub>) such that z<sub>1</sub> = x<sub>1</sub>+y<sub>1</sub> and z<sub>2</sub> = x<sub>2</sub>+y<sub>2</sub>. Then
              <section>
                <table>
                  <tr>
                    <td>
                      z<sub>1</sub>+z<sub>2</sub>
                    </td>
                    <td>=
                      (x<sub>1</sub>+y<sub>1</sub>) + (x<sub>2</sub>+y<sub>2</sub>)
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td></td>
                    <td>=
                      (x<sub>1</sub>+x<sub>2</sub>) + (y<sub>1</sub>+y<sub>2</sub>)
                    </td>
                    <td>
                      &because;
                      x<sub>1</sub>, x<sub>2</sub>, y<sub>1</sub>, y<sub>2</sub>
                      &in; V
                    </td>
                  </tr>
                </table>
              </section>
              Hence z<sub>1</sub>+z<sub>2</sub> &in; (W<sub>1</sub>+W<sub>2</sub>).
            </p>
          </section>
          <p>az &in; (W<sub>1</sub>+W<sub>2</sub>):</p>
          <section>
            <p>
              Let <var>x</var> &in; W<sub>1</sub>, <var>y</var> &in; W<sub>2</sub>, and <var>a</var> be a scalar. Then <var>ax</var> &in; W<sub>1</sub> and <var>ay</var> &in; W<sub>2</sub>, because W<sub>1</sub> and W<sub>2</sub> are subspaces. It follows that by definition of of sum of subsets
              <section>
                ax+ay &in; (W<sub>1</sub>+W<sub>2</sub>)
              </section>
              But we can find an element <var>z</var> &in; (W<sub>1</sub>+W<sub>2</sub>) such that <var>z</var> = x+y. Then
              <section>
                <table>
                  <tr>
                    <td>az</td>
                    <td>= a(x+y)</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td></td>
                    <td>= ax+ay</td>
                    <td>&because; x, y &in; V, and <var>a</var> is a scalar</td>
                  </tr>
                </table>
              </section>
              Hence <var>az</var> &in; (W<sub>1</sub>+W<sub>2</sub>).
            </p>
          </section>
          (W<sub>1</sub>+W<sub>2</sub>) contains both W<sub>1</sub> and W<sub>2</sub>:
          <section>
            <p>
              Since W<sub>1</sub> and W<sub>2</sub> contain 0, for any element <var>x</var> &in; W<sub>1</sub> and any element <var>y</var> &in; W<sub>2</sub>,
              <section>
                <table>
                  <tr>
                    <td>
                      x = x+0 &in; (W<sub>1</sub>+W<sub>2</sub>)
                    </td>
                    <td>
                      y = 0+y &in; (W<sub>1</sub>+W<sub>2</sub>)
                    </td>
                  </tr>
                </table>
              </section>
              Thus (W<sub>1</sub>+W<sub>2</sub>) contains both W<sub>1</sub> and W<sub>2</sub>. &marker;
            </p>
          </section>
        </section>
        Proof-2:
        <section>
          <p>
            Let <var>W</var> be a subspace of <var>V</var> that contains both W<sub>1</sub> and W<sub>2</sub>, then since <var>W</var> is closed under vector addition, <var>W</var> also contains x+y for any element <var>x</var> in W<sub>1</sub> and any element <var>y</var> in W<sub>2</sub>. It follows that <var>W</var> contains W<sub>1</sub>+W<sub>2</sub>. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="define-direct-sum" class="definition">
      <p>
        <strong>Definition (direct sum [直和])</strong>:
        The direct sum of <var>W<sub>1</sub></var> and <var>W<sub>2</sub></var>
        <section>
          <var>V</var>
          = <var>W<sub>1</sub></var>&CirclePlus;<var>W<sub>2</sub></var>
        </section>
        is a vector space <var>V</var> if <var>W<sub>1</sub></var> and <var>W<sub>2</sub></var> are subspaces of <var>V</var> such that
        <section>
          <table>
            <tr>
              <td>
                W<sub>1</sub> &cap; W<sub>2</sub> = {0}
              </td>
              <td>and</td>
              <td>
                W<sub>1</sub>+W<sub>2</sub> = V
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="unique-vector-in-direct-sum" class="proposition">
      <p>
        <strong>Proposition (exercise 1.3.30: unique vector in direct sum)</strong>:
        Let W<sub>1</sub> and W<sub>2</sub> be subspaces of a vector space <var>V</var>, then for <var>x</var> &in; W<sub>1</sub> and <var>y</var> &in; W<sub>2</sub>
        <section>
          <table>
            <tr>
              <td>
                V = W<sum>1</sum>&CirclePlus;W<sub>2</sub>
              </td>
              <td>&iff;</td>
              <td>
                each vector in V can be uniquely written as x+y
              </td>
            </tr>
          </table>
        </section>
        Proof:
        Let x<sub>1</sub> &in; W<sub>1</sub> and y<sub>1</sub> &in; W<sub>2</sub>.
        <section>
          <p>
            If V = W<sum>1</sum>&CirclePlus;W<sub>2</sub>, then
            <ul>
              <li>
                If x+y = x<sub>1</sub>+y<sub>1</sub> but x&ne;x<sub>1</sub> and y&ne;y<sub>1</sub>, then we have x+(&minus;x<sub>1</sub>) = y<sub>1</sub>+(&minus;y). This is, y<sub>1</sub>+(&minus;y) &in; W<sub>1</sub> since x+(&minus;x<sub>1</sub>) &in; W<sub>1</sub>, a contradiction to (W<sub>1</sub> &cap; W<sub>2</sub>) = {0}.
              </li>
              <li>
                If x+y=x<sub>1</sub>+y, then x=x<sub>1</sub> &mdash; obviously when y=0 , or by Cancellation Law when y&ne;0. Similarly, x+y = x+y<sub>1</sub> &Implies; y=y<sub>1</sub>.
              </li>
              <li>
                Thus, x+y = x<sub>1</sub>+y<sub>1</sub> if and only if x=x<sub>1</sub> and y=y<sub>1</sub> &mdash; each vector in <var>V</var> can be uniquely written as x+y.
              </li>
            </ul>
          </p>
          <p>
            If x+y = x<sub>1</sub>+y<sub>1</sub> if and only if x=x<sub>1</sub> and y=y<sub>1</sub>, but let a non-zero vector <var>z</var> &in; (W<sub>1</sub> &cap; W<sub>2</sub>). Then we have two vectors in W<sub>1</sub>+W<sub>2</sub>, z+0 and 0+z. Since z+0 = 0+z = z, each of the two vectors can be written by the other, a contradiction. Thus, (W<sub>1</sub> &cap; W<sub>2</sub>) = {0}. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="define-coset" class="definition">
      <p>
        <strong>Definition (exercise 1.3.31: coset)</strong>:
        Let <var>W</var> be a subspace of a vector space <var>V</var> over a field <var>F</var>. For any <var>v</var> &in; <var>V</var>, the coset [剰余類] of <var>W</var> containing <var>v</var> is defined as:
        <section>
          {v}+W = { v+w : w &in; W }
        </section>
        It is customary to denote this coset by v+W rather then {v}+W.
      </p>
    </section>
    <section id="coset-as-subspace" class="proposition">
      <p>
        <strong>Proposition (exercise 1.3.31.a: coset as subspace)</strong>:
        <section>
          <p>
            v+W is a subspace of <var>V</var> if and only if v &in; W
          </p>
        </section>
      </p>
      <p>
        Proof:
        <section>
          <p>v+W is a subspace of V &Implies; v &in; W:</p>
          <section>
            <p>
              If v &notin; W, then &minus;v &notin; W. It follows that any element in { v+w : w &in; W } can not be the zero vector, a contradiction to that v+W is a subspace of V. Hence v &in; W.
            </p>
          </section>
          <p>v &in; W &Implies; v+W is a subspace of V:</p>
          <section>
            <p>
              If v &in; W, then &minus;v &in; W. But if v+W is not a subspace of V, then 0 = v+(&minus;v) &notin; v+W, following that &minus;v &notin; W, a contradiction. Therefore v+W is a subspace.
            </p>
          </section>
        </section>
      </p>
    </section>
    <section id="coset-operations" class="definition">
      <p>
        <strong>Definition (coset operations)</strong>:
        Addition and scalar multiplication can be defined in the collection S = {v+W : v &in; V} of all cosets of W as follows, for v<sub>1</sub>, v<sub>2</sub> &in; V and for a &in; F:
        <section>
          <table>
            <tr>
              <td>
                (v<sub>1</sub>+W) + (v<sub>2</sub>+W)
                = (v<sub>1</sub>+v<sub>2</sub>) + W
              </td>
              <td>and</td>
              <td>
                a(v+W) = av+W
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="unqie-coset" class="proposition">
      <p>
        <strong>Proposition (exercise 1.3.31.b: unique coset)</strong>:
        With addition and scalar multiplication defined by Definition (coset operations), for v<sub>1</sub>, v<sub>2</sub> &in; V
        <section>
          <table>
            <tr>
              <td>v<sub>1</sub>+W = v<sub>2</sub>+W</td>
              <td>&iff;</td>
              <td>v<sub>1</sub>&minus;v<sub>2</sub> &in; W</td>
            </tr>
          </table>
        </section>
        Proof:
        <section>
          <p>
            v<sub>1</sub>+W = v<sub>2</sub>+W
            &Implies;
            v<sub>1</sub>&minus;v<sub>2</sub> &in; W:
          </p>
          <section>
            <p>
              If v<sub>1</sub>+W = v<sub>2</sub>+W, then v<sub>1</sub>+w = v<sub>2</sub>+w for any w &in; W, that is, v<sub>1</sub> = v<sub>2</sub> by Theorem 1.1 (cancellation law for vector addition).
            </p>
          </section>
          <p>
            v<sub>1</sub>&minus;v<sub>2</sub> &in; W
            &Implies;
            v<sub>1</sub>+W = v<sub>2</sub>+W:
          </p>
          <section>
            <p>
              If v<sub>1</sub>&minus;v<sub>2</sub> &in; W, then v<sub>2</sub>+w = v<sub>2</sub>+(v<sub>1</sub>&minus;v<sub>2</sub>) = v<sub>1</sub>+0.
            </p>
          </section>
        </section>
      </p>
    </section>
    <section id="well-defined-coset-operations" class="proposition">
      <p>
        <strong>Proposition (exercise 1.3.31.c: well-defined coset operations)</strong>:
        Addition and scalar multiplication defined by Definition (coset operations) are well defined, that is, if
        <section>
          <table>
            <tr>
              <td>
                v<sub>1</sub>+W = v&prime;<sub>1</sub>+W
              </td>
              <td>and</td>
              <td>
                v<sub>2</sub>+W = v&prime;<sub>2</sub>+W
              </td>
            </tr>
          </table>
        </section>
        then
        <section>
          <table>
            <tr>
              <td colspan="2">
                (v<sub>1</sub>+W) + (v<sub>2</sub>+W) =
                (v&prime;<sub>1</sub>+W) + (v&prime;<sub>2</sub>+W)
              </td>
            </tr>
            <tr>
              <td>
                a(v<sub>1</sub>+W) = a(v&prime;<sub>1</sub>+W)
              </td>
              <td>&ctdot; for all a &in; F</td>
            </tr>
          </table>
        </section>
        Proof:
        <section>
          <p>Oh, yeah.</p>
        </section>
      </p>
    </section>
  </section>
  <section id="linear-combinations">
    <h2>Linear Combinations</h2>
    <section id="define-linear-combination" class="definition">
      <p>
        <strong>Definition (linear combination [線形結合])</strong>:
        Let <var>S</var> be a set of vectors
        {v<sub>1</sub>, v<sub>2</sub>, &hellip;, v<sub>n</sub>} in a vector space <var>V</var>.
        Any vector of the form
        <section>
          v
          = a<sub>1</sub>v<sub>1</sub>
          + a<sub>2</sub>v<sub>2</sub>
          + &ctdot;
          + a<sub>n</sub>v<sub>n</sub>
        </section>
        for some scalars a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>n</sub>, is called a linear combination of <var>S</var>.
      </p>
    </section>
    <section id="zero-vector-linear" class="proposition">
      <p>
        <strong>Proposition (zero vector is a linear combination)</strong>:
        The zero vector is a linear combination of any nonempty subset of <var>V</var>.
      </p>
      <p>
        Proof:
        Observe that for each <var>v</var> &in; <var>V</var>, 0<var>v</var> = 0
        &marker;
      </p>
    </section>
    <section id="linear-combination-in-subspace" class="proposition">
      <p>
        <strong>Proposition (exercise 1.3.20: linear combination in subspace)</strong>:
        If <var>W</var> is a subspace of a vector space <var>V</var> and w<sub>1</sub>, w<sub>2</sub>, &hellip;, w<sub>n</sub> are in <var>W</var>, then for any scalars a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>n</sub>
        <section>
          a<sub>1</sub>w<sub>1</sub> + a<sub>2</sub>w<sub>2</sub> +
          &hellip; + a<sub>n</sub>w<sub>n</sub> &in; W
        </section>
        Proof:
        For every <var>i</var> &in; &naturals;,
        <section>
          <section>
            a<sub>i</sub>w<sub>i</sub> &in; <var>W</var>
          </section>
          because any subspace is closed under scalar multiplication. So it suffices to prove that
          <section>
            &sum; <sub>(i=1,2,&hellip;,n)</sub>
            a<sub>i</sub>w<sub>i</sub> &in; W
          </section>
          In order to prove it by induction, break the claim
          <section>
            <table>
              <tr>
                <td>Base claim:</td>
                <td>
                  a<sub>1</sub>w<sub>1</sub> &in; W
                </td>
              </tr>
              <tr>
                <td>Inductive claim:</td>
                <td>
                  if &sum; <sub>(i=1,2,&hellip;,n)</sub>
                  a<sub>i</sub>w<sub>i</sub> &in; W
                </td>
              </tr>
              <tr>
                <td></td>
                <td>
                  then
                  &sum; <sub>(i=1,2,&hellip;,n,n+1)</sub>
                  a<sub>i</sub>w<sub>i</sub> &in; W
                </td>
              </tr>
              <tr>
                <td></td>
                <td>
                  = {
                    (&sum; <sub>(i=1,2,&hellip;,n)</sub>
                    a<sub>i</sub>w<sub>i</sub>)
                    +
                    (a<sub>n+1</sub>w<sub>n+1</sub>)
                  }
                  &in; W
                </td>
              </tr>
            </table>
          </section>
          The base claim is already proven. The inductive claim holds because
          <ul>
            <li>
              (&sum; <sub>(i=1,2,&hellip;,n)</sub> a<sub>i</sub>w<sub>i</sub>) is a vector of <var>W</var> by the inductive hypothesis,
            </li>
            <li>
              (a<sub>n+1</sub>w<sub>n+1</sub>) is already proven to be a vector of <var>W</var>, and
            </li>
            <li>
              the inductive conclusion holds because any subspace is closed under addition. &marker;
            </li>
          </ul>
        </section>
      </p>
    </section>
    <section id="span" class="definition">
      <p>
        <strong>Definition (span [線形包])</strong>:
        The span of a subset <var>S</var> of a vector space is the set consisting of all linear combinations of vectors v<sub>1</sub>, v<sub>2</sub>, &hellip;, v<sub>n</sub> &in; <var>S</var>, denoted
        <section>
          <table>
            <tr>
              <td>span(S)</td>
              <td>or</td>
              <td>span({v<sub>1</sub>, v<sub>2</sub>, &hellip;, v<sub>n</sub>})</td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="zero-span" class="description">
      <p>
        <strong>Zero span</strong>:
        For convenience, we define
        <section>
          span(&empty;) = {0} &mdash; the zero vector space
        </section>
      </p>
    </section>
    <section id="spanning-set" class="description">
      <p>
        <strong>Spanning sets</strong>:
        If span(S) = <var>V</var>, then
        <section>
          <var>S</var> is called a spanning set for <var>V</var>.
        </section>
      </p>
    </section>
    <section id="span-contains-spanning-set" class="proposition">
      <p>
        <strong>Lemma (1.5.1: span contains spanning set)</strong>:
        Let <var>S</var> be a subset of a vector space <var>V</var>, then
        <section>
          span(S) is a subspace of <var>V</var> such that <var>S</var> &subseteq; span(S).
        </section>
      </p>
      <p>
        Proof:
        <section>
          <p>
            If <var>S</var> = &empty;, then span(S) = {0} by definition, which is a subspace of any vector space by Proposition (zero subspace). It is immediate that &empty; &in; span(S) = {0}.
          </p>
          <p>
            If <var>S</var> = {0}, then it is immediate that span(S) = {0}, which is a subspace of any vector space by Proposition (zero subspace). <var>S</var> &subseteq; span(S) holds because {0} = span(S).
          </p>
          <p>
            If <var>S</var> contains two or more vectors, we prove that subspace conditions hold for span(S). Let <var>x</var>, <var>y</var> &in; span(S).
            <section>
              <p>
                0 &in; span(S) holds.
                <section>
                  0x is a linear combination that is, 0x &in; span(S). Since 0x = 0, 0 &in; span(S).
                </section>
              </p>
              <p>
                <var>x</var>+<var>y</var> &in; span(S) and <var>cx</var> &in; span(S) for any scalar c hold.
                <section>
                  Since <var>S</var> &ne; &empty;, there exist vectors u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>m</sub>, v<sub>1</sub>, v<sub>2</sub>, &hellip;, v<sub>n</sub> &in; <var>S</var> such that
                  <section>
                    <table>
                      <tr>
                        <td>
                          x = a<sub>1</sub>u<sub>1</sub>
                            + a<sub>2</sub>u<sub>2</sub> + &ctdot;
                            + a<sub>m</sub>u<sub>m</sub>
                        </td>
                        <td>and</td>
                        <td>
                          y = b<sub>1</sub>v<sub>1</sub>
                            + b<sub>2</sub>v<sub>2</sub> + &ctdot;
                            + b<sub>n</sub>v<sub>n</sub>
                        </td>
                      </tr>
                    </table>
                  </section>
                  for scalars a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>m</sub>, b<sub>1</sub>, b<sub>2</sub>, &hellip;, b<sub>n</sub>. Then
                  <section>
                    <table>
                      <tr>
                        <td>
                          x + y
                          = a<sub>1</sub>u<sub>1</sub>
                          + a<sub>2</sub>u<sub>2</sub>
                          + &ctdot; + a<sub>m</sub>u<sub>m</sub>
                          + b<sub>1</sub>v<sub>1</sub>
                          + b<sub>2</sub>v<sub>2</sub>
                          + &ctdot; + b<sub>n</sub>v<sub>n</sub>
                        </td>
                      </tr>
                      <tr>
                        <td>
                          cx
                          = (ca<sub>1</sub>)u<sub>1</sub>
                          + (ca<sub>2</sub>)u<sub>2</sub> + &ctdot;
                          + (ca<sub>m</sub>)u<sub>m</sub>
                        </td>
                      </tr>
                    </table>
                  </section>
                  are linear combinations of the vectors in <var>S</var>, that is x+y &in; span(S) and <var>cx</var> &in; span(S). Thus span(S) is closed under vector addition and scalar multiplication.
                </section>
              </p>
            </section>
            When <var>S</var> contains two or more vectors, u = 1u &in; span(S) for any <var>u</var> &in; <var>S</var>, it follows S &subseteq; span(S). &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="spanning-set-subspace-contains-span" class="proposition">
      <p>
        <strong>Lemma (1.5.2: spanning set contains span)</strong>:
        Let <var>S</var> be a subset of a vector space <var>V</var>, and let <var>W</var> be a subspace of <var>V</var>, then
        <section>
          If <var>S</var> &subseteq; <var>W</var>, then
          span(S) &subseteq; <var>W</var>.
        </section>
        Proof:
        <section>
          <p>
            If <var>S</var> = &empty;, then it is immediate that &empty; &subset; <var>W</var> and that span(&empty;) = {0} &subseteq; <var>W</var> since any subspace contains the zero vector by Theorem (subspace conditions).
          </p>
          <p>
            If <var>S</var> = {0}, then it is immediate similarly that {0} &subseteq; <var>W</var> and span({0}) = {0} &subseteq; <var>W</var>.
          </p>
          <p>
            If <var>S</var> contains two or more vectors, then let <var>W</var> denote any subspace of <var>V</var> such that <var>S</var> &subseteq; <var>W</var>. If <var>w</var> &in; span(S), then <var>w</var> has the form
            <section>
              w = c<sub>1</sub>w<sub>1</sub> + c<sub>2</sub>w<sub>2</sub> + &hellip; + c<sub>k</sub>w<sub>k</sub>
            </section>
            for some vectors w<sub>1</sub>, w<sub>2</sub>, &hellip;, w<sub>k</sub> and some scalars c<sub>1</sub>, c<sub>2</sub>, &hellip;, c<sub>k</sub> &in; <var>F</var>.
            Since <var>S</var> &subseteq; <var>W</var>, we have
            <section>
              w<sub>1</sub>, w<sub>2</sub>, &hellip;, w<sub>k</sub> &in; W
            </section>
            Therefore, <var>w</var> &in; <var>W</var> by Lemma (linear combination in subspace), it follows that span(S) &subseteq; <var>W</var>. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="generate-definition" class="definition">
      <p>
        <strong>Definition (generate)</strong>:
        Let <var>V</var> be a vector space and <var>S</var> a subset of <var>V</var>, then
        <section>
          if span(S) = V, we say the vectors of S generate (or span) V.
        </section>
      </p>
    </section>
    <section id="generate-example3" class="description">
      <p>
        <strong>Example (vectors generate a vector space)</strong>:
        The vectors (1,1,0), (1,0,1), and (0,1,1) generate &reals;<sup>3</sup> since an arbitrary vector (a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>) in &reals;<sup>3</sup> is a linear combination of the three given vectors; in fact, the scalars <var>r</var>, <var>s</var> and <var>t</var> for which
        <section>
          <table>
            <tr>
              <td colspan="3">
                r(1,1,0) + s(1,0,1) + t(0,1,1)
                = (a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>)
              </td>
            </tr>
            <tr>
              <td colspan="3">are</td>
            </tr>
            <tr>
              <td>r = &frac12; (a<sub>1</sub> + a<sub>2</sub> &minus; a<sub>3</sub>),</td>
              <td>s = &frac12; (a<sub>1</sub> &minus; a<sub>2</sub> + a<sub>3</sub>),</td>
              <td>t = &frac12; (&minus;a<sub>1</sub> + a<sub>2</sub> + a<sub>3</sub>)</td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="spanning-set-spans-itself" class="proposition">
      <p>
        <strong>
          Proposition (exercise 1.4.12: spanning set spans itself)
        </strong>:
        A subset <var>W</var> of a vector space <var>V</var> is a subspace of <var>V</var> if and only if
        <section>
          span(W) = W
        </section>
        Proof:
        If a subset <var>W</var> of a vector space <var>V</var> is a subspace of <var>V</var>, then span(W) = <var>W</var>, because
        <section>
          <section>
            <table>
              <tr>
                <td>W &subseteq; span(W)</td>
                <td>by Lemma (span contains spanning set)</td>
              </tr>
              <tr>
                <td>span(W) &subseteq; W</td>
                <td>by Lemma (spanning set contains span)</td>
              </tr>
            </table>
          </section>
          Conversely, we know from Lemma (span is subspace) that span(W) is a subspace of <var>V</var> (since W &subseteq; V), hence <var>W</var> is too.
          &marker;
        </section>
      </p>
    </section>
    <section id="subset-and-span" class="proposition">
      <p>
        <strong>Proposition (exercise 1.4.13: subset and span)</strong>:
        Let S<sub>1</sub> and S<sub>2</sub> be subsets of a vector space <var>V</var>, then
        <section>
          if S<sub>1</sub> &subseteq; S<sub>2</sub>,
          then span(S<sub>1</sub>) &subseteq; span(S<sub>2</sub>)
        </section>
        Proof:
        Let v<sub>1</sub>, v<sub>2</sub>, &hellip;, v<sub>n</sub> &in; S<sub>1</sub>, then some linear combination <var>v</var> in span(S<sub>1</sub>)
        <section>
          <p>
            <section>
              v = a<sub>1</sub>v<sub>1</sub> + a<sub>2</sub>v<sub>2</sub> + &ctdot; + a<sub>n</sub>v<sub>n</sub>
            </section>
            is also a linear combination of S<sub>2</sub> because v<sub>1</sub>, v<sub>2</sub>, &hellip;, v<sub>n</sub> &in; S<sub>2</sub>.
            &marker;
          </p>
        </section>
      </p>
    </section>
  </section>
  <section id="linear-dependence">
    <h2>Linear Dependence and Linear Independence</h2>
    <section id="intro-linear-dependence" class="description">
      <p>
        Let <var>W</var> be a subspace of a vector space <var>V</var>. It is desirable to find a small finite subset <var>S</var> of <var>W</var> that generates <var>W</var>. (&hellip;) The search for this subset is related to the question of whether or not some vector in <var>S</var> is a linear combination of the other vectors in <var>S</var>.
      </p>
      <p>
        Rather than asking whether some vector in <var>S</var> is a linear combination of the other vectors in <var>S</var>, it is more efficient to ask whether the zero vector can be expressed as a linear combination of the vectors in <var>S</var> with coefficients that are not all zero.
      </p>
    </section>
    <section id="define-linearly-dependent" class="definition">
      <p>
        <strong>Definition (linearly dependent [線形従属])</strong>:
        A subset <var>S</var> of a vector space <var>V</var> is called linearly dependent if there exist a finite number of distinct vectors u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>n</sub> &in; <var>S</var> and scalars a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>n</sub>, not all zero, such that
        <section>
          a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
        </section>
      </p>
    </section>
    <section id="trivial-representation" class="description">
      <p>
        <strong>Trivial representation</strong>:
        For any vectors u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>n</sub>, we have
        <section>
          a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
        </section>
        if a<sub>1</sub> = a<sub>2</sub> = &ctdot; = a<sub>n</sub> = 0. We call this the trivial representation of 0 as a linear combination of u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>n</sub>.
      </p>
    </section>
    <section id="zero-vector-linearly-dependent" class="proposition">
      <p>
        <strong>Proposition (zero vector linearly dependent)</strong>:
        Any set that contains the zero vector is linearly dependent.
      </p>
      <p>
        Proof: It is immediate that a0 = 0 for any scalar <var>a</var>. Because it is a nontrivial representation of the zero vector as the sole linear combination of vectors in the set, the set is linearly dependent.
      </p>
    </section>
    <section id="linearly-dependent-sets" class="proposition">
      <p>
        <strong>Proposition (linearly dependent sets)</strong>:
        A set is linearly dependent if only if
        <section>
          at least one of the vectors in the set is expressible as a linear combination of the other vectors in the set.
        </section>
        Proof:
        Let <var>S</var> be a set and {u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>n</sub>} &in; <var>S</var>.
        <section>
          <p>
            If <var>S</var> is linearly dependent, then there exist a nontrivial representation of <var>0</var> as a linear combination of vectors in <var>S</var> for some scalars a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>n</sub>, not all zero, such that
            <section>
              a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
            </section>
            Let a<sub>1</sub> be a nonzero scalar, then a vector u<sub>1</sub> &in; <var>S</var>
            <section>
              u<sub>1</sub> = &minus;1 &frasl; a<sub>1</sub> (
                a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
              )
            </section>
            can be expressed as a linear combination of the other vectors in <var>S</var>.
          </p>
          <p>
            Conversely, if at least one of the vectors in <var>S</var>, say u<sub>1</sub>, is expressible as a linear combination of the other vectors u<sub>2</sub>, &hellip;, u<sub>n</sub> for some scalars a<sub>2</sub>, &hellip;, a<sub>n</sub> such that
            <section>
              u<sub>1</sub> =
              a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
            </section>
            then from the expression, we have a representation of <var>0</var> as a linear combination of vectors in <var>S</var> such that
            <section>
              0 = (&minus;1)u<sub>1</sub> +
              a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
            </section>
            Because the coefficient of u<sub>1</sub> is nonzero, <var>S</var> is linear dependent. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="linearly-independent" class="definition">
      <p>
        <strong>Definition (linearly independent [線形独立])</strong>:
        A subset <var>S</var> of a vector space that is not linearly dependent is called linearly independent, that is
        <section>
          <p>
            the only representations of the zero vector as linear combinations of its vectors are trivial representations.
          </p>
        </section>
        For convenience, we define that
        <section>
          the empty set &empty; is linearly independent.
        </section>
      </p>
    </section>
    <section id="single-vector-linearly-independent" class="proposition">
      <p>
        <strong>Proposition (single vector linearly independent)</strong>:
        Consider a set containing only one nonzero vector <var>v</var>. Then {v} is linearly independent.
      </p>
      <p>
        Proof: Since the sole vector <var>v</var> in the set is not zero vector, <var>av</var> = 0 for some scalar <var>a</var> &mdash; the only representation of the zero vector as linear combination of the vector in the set &mdash; holds only when <var>a</var> = 0. Hence the set is linearly independent. &marker;
      </p>
    </section>
    <section id="superset-dependent" class="proposition">
      <p>
        <strong>Theorem (1.6: superset linearly dependent)</strong>:
        Let S<sub>1</sub> and S<sub>2</sub> be subsets of a vector space <var>V</var> such that S<sub>1</sub> &subseteq; S<sub>2</sub> &subseteq; V, then
        <section>
          <table>
            <tr>
              <td>S<sub>1</sub> is linearly dependent</td>
              <td>&Implies;</td>
              <td>S<sub>2</sub> is linearly dependent</td>
            </tr>
          </table>
        </section>
        Proof:
        <section>
          Since S<sub>1</sub> is linearly dependent, there exist a finite number of distinct vectors u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>n</sub> &in; S<sub>1</sub> and scalars a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>n</sub>, not all zero, such that
          <section>
            a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
          </section>
          u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>n</sub> &in; S<sub>2</sub>, hence S<sub>2</sub> is also linear dependent.
        </section>
      </p>
    </section>
    <section id="subset-independent" class="proposition">
      <p>
        <strong>Corollary (subset linearly independent)</strong>:
        Let S<sub>1</sub> and S<sub>2</sub> be subsets of a vector space <var>V</var> such that S<sub>1</sub> &subseteq; S<sub>2</sub> &subseteq; V, then
        <section>
          <table>
            <tr>
              <td>S<sub>2</sub> is linearly independent</td>
              <td>&Implies;</td>
              <td>S<sub>1</sub> is linearly independent</td>
            </tr>
          </table>
        </section>
        Proof: It holds because
        <ul>
          <li>
            a subset that is not linearly dependent is linearly independent, and
          </li>
          <li>
            the second implication is the contrapositive of first implication.
            &marker;
          </li>
        </ul>
      </p>
    </section>
    <section id="the-smallest-generating-set" class="description">
      <p>
        <strong>The smallest generating set</strong>:
        The issue of whether <var>S</var> is the smallest generating set for its span is related to the question of whether <var>S</var> is linearly dependent.
      </p>
      <p>
        If <var>S</var> is linearly dependent set containing two or more vectors, then some vector <var>v</var> &in; <var>S</var> can be written as a linear combination of the other vectors in <var>S</var>. In other words,
        <section>
          <table>
            <tr>
              <td>
                if <var>S</var> is linearly dependent set, then
              </td>
            </tr>
            <tr>
              <td>
                the subset obtained by removing some <var>v</var> &in; <var>S</var> has the same span as <var>S</var>.
              </td>
            </tr>
          </table>
        </section>
        It follows that
        <section>
          <table>
            <tr>
              <td>
                if no proper subset of <var>S</var> generates the span of <var>S</var>, then
              </td>
            </tr>
            <tr>
              <td>
                <var>S</var> must be linearly independent.
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="linearly-dependent-span" class="proposition">
      <p>
        <strong>Theorem (1.7: linearly dependent span)</strong>:
        Let <var>S</var> be a linearly independent subset of a vector space <var>V</var> and
        <section>
          <table>
            <tr>
              <td>u &notin; S</td>
              <td>and</td>
              <td>u &in; V</td>
            </tr>
          </table>
        </section>
        then <var>S</var> &cup; {u} is linearly dependent if and only if
        <section>
          u &in; span(S)
        </section>
        Proof:
        <section>
          <p>
            If <var>S</var> &cup; {u} is linearly dependent, then there are vectors u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>n</sub> &in; <var>S</var> such that au + a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0 for some nonzero scalars a, a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>n</sub>, so
            <section>
              u
              = &minus;1 &frasl; a
                ( a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> )
            </section>
            is a linear combination of vectors in <var>S</var>, it follows that <var>u</var> &in; span(S).
          </p>
          <p>
            Conversely, if u &in; span(S), then there exist vectors u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>m</sub> &in; <var>S</var> and scalars a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>m</sub>
            <section>
              <table>
                <tr>
                  <td>such that</td>
                  <td>
                    u
                    = a<sub>1</sub>u<sub>1</sub>
                    + a<sub>2</sub>u<sub>2</sub> + &ctdot;
                    + a<sub>m</sub>u<sub>m</sub>
                  </td>
                </tr>
                <tr>
                  <td>if follows</td>
                  <td>
                    0
                    = a<sub>1</sub>u<sub>1</sub>
                    + a<sub>2</sub>u<sub>2</sub> + &ctdot;
                    + a<sub>m</sub>u<sub>m</sub> + (&minus;1)u
                  </td>
                </tr>
              </table>
            </section>
            Note that u &ne; u<sub>i</sub> (i = 1, 2, &hellip;, m), because u &notin; <var>S</var>. And also note that the coefficient of <var>u</var> in this linear combination is &minus;1 (nonzero). Thus {u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>m</sub>, u} is linearly dependent, it follows that <var>S</var> &cup; {u} is also linearly dependent by Theorem (linearly dependent span).
            &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="scalar-multiple-linearly-dependent" class="proposition">
      <p>
        <strong>
          Proposition (exercise 1.5.9: scalar multiple linearly dependent)
        </strong>:
        Let <var>u</var> and <var>v</var> be distinct vectors in a vector space <var>V</var>. Then {u, v} is linearly dependent if and only if
        <section>
          <var>u</var> and <var>v</var> is a multiple of the other
        </section>
      </p>
      <p>
        Proof:
        <section>
          <p>
            If {u, v} is linearly dependent, there exists a nontrivial representation of <var>0</var> as a linear combination for some nonzero scalars <var>a</var> and <var>b</var> such that
            <section>
              <table>
                <tr>
                  <td>au + bv = 0</td>
                  <td>it follows</td>
                </tr>
                <tr>
                  <td>u = (&minus;b &frasl; a) v</td>
                  <td>because a &ne; 0</td>
                </tr>
              </table>
            </section>
            Thus, <var>u</var> and <var>v</var> is a multiple of the other.
            Conversely, if <var>u</var> and <var>v</var> is a multiple of the other, then
            <section>
              <table>
                <tr>
                  <td>u = av</td>
                  <td>it follows</td>
                </tr>
                <tr>
                  <td>0 = (&minus;1)u + av</td>
                  <td>which is nontrivial</td>
                </tr>
              </table>
            </section>
            Thus, {u, v} is linearly dependent. &marker;
          </p>
        </section>
      </p>
    </section>
  </section>
  <section id="bases-and-dimension">
    <h2>Bases and Dimension</h2>
    <section id="define-basis" class="definition">
      <p>
        <strong>Definition (basis [基底])</strong>:
        A subset &Beta; of a vector space <var>V</var> is called a basis for <var>V</var> if
        <ul>
          <li>&Beta; is linearly independent</li>
          <li>
            &Beta; generates <var>V</var>
            &ctdot; i.e., span(&Beta;) = <var>V</var>
          </li>
        </ul>
        We also say that &Beta; forms a basis for <var>V</var>.
      </p>
    </section>
    <section id="basis-is-building-block" class="description">
      <p>
        With a linearly independent generating set &mdash;a basis &mdash; for a vector space, every vector in the vector space can be expressed in one and only one way as a linear combination of the vectors in the basis. This makes linearly independent generating sets the building blocks of vector spaces.
      </p>
    </section>
    <section id="basis-of-zero-vector-space" class="proposition">
      <p>
        <strong>Proposition (example 1.6.1: basis of zero vector space)</strong>:
        The empty set &empty; is a basis for the zero vector space &mdash; {0}.
      </p>
      <p>
        Proof:
        Recall that span(&empty;) = {0} by definition of the zero span.
        <ul>
          <li>
            &empty; is linearly independent by definition of linear independence.
          </li>
          <li>
            span(&empty;) = {0} by definition.
          </li>
        </ul>
      </p>
    </section>
    <section id="basis-unique-linear-combination" class="proposition">
      <p>
        <strong>Theorem (1.8: basis unique linear combination)</strong>:
        Let u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>n</sub> be distinct vectors in a vector space <var>V</var>, then, &Beta; = { u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>n</sub> } is a basis for <var>V</var> if and only if
        <section>
          each vector <var>u</var> &in; <var>V</var> can be uniquely expressed as a linear combination of vectors of &Beta;.
        </section>
      </p>
      <p>
        Proof:
        Let <var>V</var> be a vector space generated by &Beta; &mdash; that is, span(&Beta;) = <var>V</var>.
        <section>
          <p>
            Let &Beta; be a basis for <var>V</var>. If <var>u</var> &in; <var>V</var>, then <var>u</var> &in; span(&Beta;) because span(&Beta;) = <var>V</var>. Thus <var>u</var> is a linear combination of the vector of &Beta;. Suppose that
            <section>
              <table>
                <tr>
                  <td>u</td>
                  <td>
                    = a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td>
                    = b<sub>1</sub>u<sub>1</sub> + b<sub>2</sub>u<sub>2</sub> + &ctdot; + b<sub>n</sub>u<sub>n</sub>
                  </td>
                </tr>
              </table>
            </section>
            are two such representations of <var>u</var>. Subtracting the second equation from the first gives
            <section>
              0 = (a<sub>1</sub> &minus; b<sub>1</sub>) u<sub>1</sub> + (a<sub>2</sub> &minus; b<sub>2</sub>) u<sub>2</sub> + &ctdot; + (a<sub>n</sub> &minus; b<sub>n</sub>) u<sub>n</sub>
            </section>
            Since &Beta; is linear independent by definition of the basis, it follows that
            <section>
              (a<sub>1</sub> &minus; b<sub>1</sub>) = (a<sub>2</sub> &minus; b<sub>2</sub>) = &ctdot; = (a<sub>n</sub> &minus; b<sub>n</sub>) = 0
            </section>
            Hence
              a<sub>1</sub> = b<sub>1</sub>, a<sub>2</sub> = b<sub>2</sub>, &hellip;, a<sub>n</sub> = b<sub>n</sub>,
            and so <var>u</var> &in; <var>V</var> can be uniquely expressed as a linear combination of vectors of &Beta;.
          </p>
          <p>
            Conversely let <var>u</var> &in; <var>V</var> is uniquely expressible as a linear combination of vectors of &Beta;. In order to prove by contradiction that &Beta; is a basis for <var>V</var>, suppose that &Beta; is not a basis of <var>V</var>, that is, &Beta; is linearly dependent.
            <ul>
              <li>
                We know from Theorem (sets and linearly dependent) that since &Beta; &subseteq; span(&Beta;) = <var>V</var>, the set <var>V</var> is also linearly dependent.
              </li>
              <li>
                Hence from Theorem (linearly dependent sets), at least one of the vectors in <var>V</var>, say <var>u</var>, is expressible as a linear combination of the other vectors in <var>V</var> &mdash; a contradiction to the hypothesis of this implication.
              </li>
            </ul>
            Hence &Beta; is linearly independent and a basis of <var>V</var>. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="finite-spanning-set" class="proposition">
      <p>
        <strong>Theorem (1.9: finite spanning set)</strong>:
        If a vector space <var>V</var> is generated by a finite set <var>S</var>, then some subset of <var>S</var> is a basis for <var>V</var>. Hence <var>V</var> has a finite basis.
      </p>
      <p>
        Proof:
        <section>
          <p>
            If <var>S</var> = &empty;, then <var>S</var>  is a basis of <var>V</var> = {0}, because
            <ul>
              <li>
                &empty; is linearly independent by definition
              </li>
              <li>
                span(&empty;) = {0} = <var>V</var>
              </li>
            </ul>
          </p>
          <p>
            If <var>S</var> = {0}, then &empty;, a subset of <var>S</var>, is a basis of <var>V</var> = {0}, because
            <ul>
              <li>
                &empty; is linearly independent
              </li>
              <li>
                span({0}) = {0} = <var>V</var>
              </li>
            </ul>
          </p>
          <p>
            If <var>S</var> contains a single nonzero vector, say <var>S</var> = {u}, then <var>V</var> = span{u}, and {u} is a basis for <var>V</var> = span{u}, because
            <ul>
              <li>
                {u} &subseteq; {u} = <var>S</var>
              </li>
              <li>
                {u} is linearly independent by Corollary (single vector linearly independent)
              </li>
            </ul>
          </p>
          <p>
            If <var>S</var> contains <var>k</var> vectors such that { u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>k</sub> } is linearly independent, 
          </p>
        </section>
      </p>
    </section>
  </section>
  <section id="reference">
    <h2>References</h2>
    <ul>
      <li>
        Friedberg, S.H., et al. (2018). <i>Linear Algebra, 5th Edition</i>. Pearson.
      </li>
    </ul>
  </section>
</article>
</body>
</html>
