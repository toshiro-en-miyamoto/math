<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>Linear Algebra, Friedberg(2018)</title>
  <style>
    body {
      margin-left: 1.2rem;
    }
    th, td {
      padding-right: 1.2rem;
      vertical-align: text-bottom;
    }
    th {
      text-align: left;
    }
    section, article {
      margin-top: 1rem;
      margin-left: 2rem;
      margin-bottom: 1rem;
    }
    article {
      margin-right: 2rem;
    }
    .definition, .proposition, .description {
      padding-right: 1rem;
      padding-left: 1rem;
    }
    .definition {
      padding-top: 0.2rem;
      padding-bottom: 0.2rem;
      background-color: lightgrey;
    }
    .center {
      text-align: center;
    }
    .right {
      text-align: right;
    }
  </style>
</head>
<body>
<article>
<h1>Linear Algebra [線形代数]</h1>
  <section id="set">
    <h2>Sets (Appendix A)</h2>
    <section id="empty-set" class="definition">
      <p>
        <strong>Definition (empty set)</strong>:
        The empty set, denoted by &empty;, is the set containing no elements. The empty set is a subset of every set.
      </p>
    </section>
  </section>
  <section id="fields">
    <h2>Fields (Appendix C)</h2>
    <section id="definition-field" class="definition">
      <p>
        <strong>Definition (field [体])</strong>:
        A field <var>F</var> is a set on which two operations (addition and multiplication) are defined so that for each pair of elements <var>a, b</var> &in; <var>F</var>,
        <ul>
          <li>
            there is a unique element
            <var>a</var>+<var>b</var> &in; <var>F</var>
          </li>
          <li>
            there is a unique element
            <var>a</var>&middot;<var>b</var> &in; <var>F</var>
          </li>
        </ul>
        such that the field axioms hold for all elements <var>a, b, c</var> &in; <var>F</var>.
      </p>
      <p>
        <strong>Axiom (Field axioms [体の公理])</strong>:
        <section id="field-axioms">
          <table>
            <tr>
              <th></th>
              <th></th>
              <th>addition</th>
              <th>multiplication</th>
            </tr>
            <tr>
              <td>交換則</td>
              <td>commutativity</td>
              <td>a+b = b+a</td>
              <td>a&middot;b = b&middot;a</td>
            </tr>
            <tr>
              <td>結合則</td>
              <td>associativity</td>
              <td>(a+b)+c = a+(b+c)</td>
              <td>(a&middot;b)&middot;c = a&middot;(b&middot;c)</td>
            </tr>
            <tr>
              <td>単位元</td>
              <td>existence of identity</td>
              <td>0+a = a</td>
              <td>1&middot;a = a</td>
            </tr>
            <tr>
              <td>逆元</td>
              <td>existence of inverse</td>
              <td>a+(&minus;a) = 0</td>
              <td>a&middot;a<sup>-1</sup> = 1 (if a&NotEqual;0)</td>
            </tr>
            <tr>
              <td>分配則</td>
              <td>distributivity of &middot; over +</td>
              <td colspan="2" class="center">
                a&middot;(b+c) = a&middot;b+a&middot;c
              </td>
            </tr>
          </table>
        </section>
        The zero ring can be avoided by including the zero-one law (0 &ne; 1).
      </p>
    </section>
    <section id="cancellation-laws" class="proposition">
      <p>
        <strong>Theorem (cancellation laws [簡約律])</strong>:
        Let <var>F</var> be a field. For all elements <var>a, b, c</var> &in; <var>F</var> and all <var>d</var>&ne;0 &in; <var>F</var>,
        <section>
          <table>
            <tr>
              <td>1.</td>
              <td>a+b = c+b</td>
              <td>&Implies;</td>
              <td>a = c</td>
            </tr>
            <tr>
              <td>2.</td>
              <td>a&middot;d = c&middot;d</td>
              <td>&Implies;</td>
              <td>a = c</td>
            </tr>
          </table>
        </section>
        Proof:
        <ol>
          <li>
            There exists an element <var>d</var> &in; <var>F</var> such that <var>b</var>+<var>d</var> = <var>0</var>.
            <section>
              <table>
                <tr>
                  <td>
                    (a+b)+d = (c+b)+d
                  </td>
                  <td>by adding <var>d</var> to both sides</td>
                </tr>
                <tr>
                  <td>
                    a+{b+d} = c+{b+d}
                  </td>
                  <td>associativity of addition</td>
                </tr>
                <tr>
                  <td>a+0 = c+0</td>
                  <td>&because; b+d = 0</td>
                </tr>
                <tr>
                  <td>a = c</td>
                  <td>existence of additive identity</td>
                </tr>
              </table>
            </section>
          </li>
          <li>
            If <var>d</var> &NotEqual; <var>0</var>, there exists an element <var>e</var> such that <var>d</var>&middot;<var>e</var> = <var>1</var>.
            <section>
              <table>
                <tr>
                  <td>
                    (a&middot;d)&middot;e
                    =
                    (c&middot;d)&middot;e
                  </td>
                  <td>by multiplying both sides by <var>e</td>
                </tr>
                <tr>
                  <td>
                    a&middot;(d&middot;e)
                    =
                    c&middot;(d&middot;e)
                  </td>
                  <td>associativity of multiplication</td>
                </tr>
                <tr>
                  <td>a&middot;1 = c&middot;1</td>
                  <td>&because; d&middot;e = 1</td>
                </tr>
                <tr>
                  <td>a = c</td>
                  <td>
                    existence of multiplicative identity
                    &marker;
                  </td>
                </tr>
              </table>
            </section>
          </li>
        </ol>
      </p>
    </section>
    <section id="unique-identity-inverse" class="proposition">
      <p>
        <strong>Corollary (unique identities and inverses)</strong>:
        Let <var>F</var> be a field and <var>a</var> &in; <var>F</var>, then
        <ul>
          <li>Additive identity (0) is unique.</li>
          <li>Additive inverse (<var>&minus;a</var>) is unique.</li>
          <li>Multiplicative identity (1) is unique.</li>
          <li>
            Multiplicative inverse (<var>a<sup>-1</sup></var> if <var>a</var>  &NotEqual; 0) is unique.
          </li>
        </ul>
        Proof:
        <ul>
          <li>
            Additive identity (0): Suppose that 0&prime; &in; <var>F</var> is an additive identity.
            <section>
              <table>
                <tr>
                  <td>0&prime; + a</td>
                  <td>= a</td>
                  <td>&because; 0&prime; is an additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0 + a</td>
                  <td>&because; 0 is an additive identity</td>
                </tr>
                <tr>
                  <td>0&prime;</td>
                  <td>= 0</td>
                  <td>additive cancellation law</td>
              </table>
            </section>
          </li>
          <li>
            Other three proofs are similar. &marker;
          </li>
        </ul>
      </p>
    </section>
    <section id="substraction-division" class="definition">
      <p>
        <strong>Definition (Subtraction and Division)</strong>: Subtraction (&minus;) and division (&divide; or &frasl;) can be defined in terms of addition and multiplication by using inverses. Let <var>F</var> be a field. For all <var>a</var> &in; <var>F</var> and all <var>b</var> &NotEqual; 0 &in; <var>F</var>,
        <section>
          <table>
            <tr>
              <td>a&minus;b = a+(&minus;b)</td>
            </tr>
            <tr>
              <td>
                a&divide;b
                = <sup>a</sup>&frasl;<sub>b</sub>
                = a&middot;b<sup>-1</sup>
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="multiplications-in-fields" class="proposition">
      <p>
        <strong>Corollary (multiplications in fields)</strong>:
        Let <var>F</var> be a field. For all <var>a, b</var> &in; <var>F</var>,
        <ol>
          <li>a&middot;0 = 0</li>
          <li>
            (&minus;a)&middot;b = a&middot;(&minus;b) = &minus;(a&middot;b)
          </li>
          <li>(&minus;a)&middot;(&minus;b) = a&middot;b</li>
        </ol>
        Proof:
        <ol>
          <li>
            a&middot;0 = 0
            <section>
              <table>
                <tr>
                  <td>a&middot;0+a&middot;0</td>
                  <td>= a&middot;(0+0)</td>
                  <td>distributivity of &middot; over +</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a&middot;0</td>
                  <td>&because; 0+0 = 0</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a&middot;0+0</td>
                  <td>existence of additive identity</td>
                </tr>
                <tr>
                  <td>a&middot;0</td>
                  <td>= 0</td>
                  <td>additive cancellation law</td>
                </tr>
              </table>
            </section>
          </li>
          <li>
            (&minus;a)&middot;b = &minus;(a&middot;b)
            <section>
              <table>
                <tr>
                  <td>a&middot;b+(&minus;a)&middot;b</td>
                  <td>= {a+(&minus;a)}&middot;b</td>
                  <td>distributivity of &middot; over +</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0&middot;b</td>
                  <td>existence of additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0</td>
                  <td>corollary 1</td>
                </tr>
              </table>
            </section>
          </li>
          <li>
            (&minus;a)&middot;(&minus;b) = a&middot;b
            <section>
              <table>
                <tr>
                  <td>(&minus;a)&middot;(&minus;b)</td>
                  <td>= &minus;{a&middot;(&minus;b)}</td>
                  <td>by applying (2)</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= &minus;{&minus;(a&middot;b)}</td>
                  <td>by applying (2)</td>
                </tr>
                <tr>
                  <td></td>
                  <td>
                    = a&middot;b
                    &marker;
                  </td>
                </tr>
              </table>
            </section>
          </li>
        </ol>
      </p>
    </section>
    <section id="zero-ring" class="description">
      <p>
        <strong>Zero ring</strong>:
        The zero ring, denoted {0} or simply <var>0</var>, consists of the one-element set {0} with the operations + and &middot; defined such that 0+0 = 0 and 0&middot;0 = 0. [Wikipedia: Zero ring]. If additive identity <var>0</var> equals multiplicative identity <var>1</var>, then the ring <var>R</var> has only a single element <var>0</var> = <var>1</var>, i.e.,
        <section>
          0 = 1 &Implies;
          for all <var>r</var> &in; <var>R</var>, <var>r</var> = 0
        </section>
        Proof: for all <var>r</var> &in; <var>R</var>,
        <section>
          <table>
            <tr>
              <td>r</td>
              <td>= 1r</td>
              <td>&because; 1 is multiplicative identity</td>
            </tr>
            <tr>
              <td></td>
              <td>= 0r</td>
              <td>&because; 0 = 1</td>
            </tr>
            <tr>
              <td></td>
              <td>= 0</td>
              <td>
                corollary (multiplications in fields) #1
                &marker;
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
  </section>
  <section id="vector-spaces">
    <h2>Vector Spaces</h2>
    <section id="define-vector-space" class="definition">
      <p>
        <strong>Definition (vector space [ベクトル空間])</strong>:
        A vector space <var>V</var> over a field <var>F</var> consists of a set on which two operations (vector addition and scalar multiplication [スカラー倍]) are defined so that for each scalar <var>a</var> &in; <var>F</var> and for each vector <var>x, y</var> &in; <var>V</var>:
        <ul>
          <li>
            there is a unique element <var>x+y</var> &in; <var>V</var>
          </li>
          <li>
            there is a unique element <var>ax</var> &in; <var>V</var>
          </li>
        </ul>
        such that the vector space axioms hold for each <var>a, b</var> &in; <var>F</var> and for each <var>x, y, z</var> &in; <var>V</var>.
      </p>
      <p>
        <strong>Axiom (vector space axioms)</strong>
        <section>
          <table>
            <tr>
              <th></th>
              <th>vector addition</th>
              <th>scalar multiplication</th>
            </tr>
            <tr>
              <td>commutativity</td>
              <td>x+y = y+x</td>
              <td></td>
            </tr>
            <tr>
              <td>associativity</td>
              <td>(x+y)+z = x+(y+z)</td>
              <td>(ab)x = a(bx)</td>
            </tr>
            <tr>
              <td>existence of identity</td>
              <td>x+0 = x</td>
              <td>1x = x</td>
            </tr>
            <tr>
              <td>existence of inverse</td>
              <td>x+(&minus;x) = 0</td>
              <td></td>
            </tr>
            <tr>
              <td>distributivity</td>
              <td>a(x+y) = ax+ay</td>
              <td>(a+b)x=ax+bx</td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section class="description">
      <p>
        We call:
        <section>
          <table>
            <tr>
              <th>scalars</th>
              <td>the elements of the field</td>
            </tr>
            <tr>
              <th>vectors</th>
              <td>the elements of the vector space</td>
            </tr>
            <tr>
              <th>zero vector</th>
              <td>
                <var>0</var>, the additive identity
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="cancellation-law-vector" class="proposition">
      <p>
        <strong>Theorem 1.1 (cancellation law for vector addition)</strong>:
        Let <var>V</var> be a vector space, and <var>x, y, z</var> &in; <var>V</var>, then
        <section>
          <table>
            <tr>
              <td>x+z = y+z</td>
              <td>&Implies;</td>
              <td>x = y</td>
            </tr>
          </table>
        </section>
        Proof:
        By the vector space axiom, we know that there exists an additive inverse, say <var>u</var>, of the vector <var>z</var> such that
        <section>
          z+u = 0
        </section>
        Thus
        <section>
          <table>
            <tr>
              <td>x</td>
              <td>= x+0</td>
              <td><var>0</var> is the additive identity</td>
            </tr>
            <tr>
              <td></td>
              <td>= x+(z+u)</td>
              <td>the additive inverse <var>u</var> of <var>z</var></td>
            </tr>
            <tr>
              <td></td>
              <td>= (x+z)+u</td>
              <td>by additive associativity</td>
            </tr>
            <tr>
              <td></td>
              <td>= (y+z)+u</td>
              <td>by the hypothesis</td>
            </tr>
            <tr>
              <td></td>
              <td>= y+(z+u)</td>
              <td>by additive associativity</td>
            </tr>
            <tr>
              <td></td>
              <td>= y+0</td>
              <td>the additive inverse <var>u</var> of <var>z</var></td>
            </tr>
            <tr>
              <td></td>
              <td>= y</td>
              <td><var>0</var> is the additive identity &marker;</td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="unique-zero-vector" class="proposition">
      <p>
        <strong>Corollary 1 (unique zero vector)</strong>:
        The zero vector is unique.
      </p>
      <p>
        Proof: Let <var>V</var> be a vector space, and <var>x &in; V</var>. Suppose that <var>0&prime; &in; V</var> is an zero vector, too. Then,
        <section>
          <table>
            <tr>
              <td>0&prime; + x</td>
              <td>= x</td>
              <td>&because; 0&prime; is an zero vector</td>
            </tr>
            <tr>
              <td></td>
              <td>= 0 + x</td>
              <td>&because; 0 is an zero vector</td>
            </tr>
            <tr>
              <td>0&prime;</td>
              <td>= 0</td>
              <td>
                the cancellation law
                &marker;
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="unique-additive-inverse" class="proposition">
      <p>
        <strong>Corollary 2 (unique additive inverse)</strong>:
        The additive inverse vector is unique.
      </p>
      <p>
        Proof: Suppose that for a vector <var>x</var>, there exist two distinct additive inverses. Since the zero vector is unique,
        <section>
          0 = x+(&minus;x) = x+(&minus;x&prime;)
        </section>
        Thus &minus;x = &minus;x&prime; by cancellation law for vector addition, it follows that the additive inverse is unique.
      </p>
    </section>
    <section id="theorem-scalar-multiplications" class="proposition">
      <p>
        <strong>Theorem 1.2 (scalar multiplications in vector spaces)</strong>:
        In any vector space <var>V</var> over a field <var>F</var>, the following statements are true:  for each <var>a</var> &in; <var>F</var> and for each <var>x</var> &in; <var>V</var>,
        <section>
          <table>
            <tr>
              <td>[1]</td>
              <td>0x = 0</td>
            </tr>
            <tr>
              <td>[2]</td>
              <td>
                (&minus;a)x
                = &minus;(ax)
              </td>
            </tr>
            <tr>
              <td>[3]</td>
              <td>
                a(&minus;x)
                = &minus;(ax)
              </td>
            </tr>
            <tr>
              <td>[4]</td>
              <td>a0 = 0</td>
            </tr>
          </table>
        </section>
        Proof:
        <section>
          <p>
            [1]: By the vector space axioms and the cancellation law for vector addition,
            <section>
              <table>
                <tr>
                  <td>0x+0x</td>
                  <td>= (0+0)x</td>
                  <td>by scalar multiplicative distributivity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0x</td>
                  <td>the field additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0x+0</td>
                  <td>the vector additive identity</td>
                </tr>
                <tr>
                  <td colspan="2" class="center">
                    0x = 0
                  </td>
                  <td>by the cancellation law for vector addition</td>
                </tr>
              </table>
            </section>
          </p>
          <p>
            [2]: By the field axioms, we know that there exists an additive inverse for some scalar <var>a</var>, denoted &minus;<var>a</var>,
            <section>
              <table>
                <tr>
                  <td>ax+(&minus;a)x</td>
                  <td>= {a+(&minus;a)}x</td>
                  <td>by scalar multiplicative distributivity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0x</td>
                  <td>
                    the field additive inverse
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0</td>
                  <td>by [1]</td>
                </tr>
              </table>
            </section>
            But by the vector space axioms, we know that there exists an additive inverse for some vector <var>ax</var>, denoted &minus;(ax) such that
            <section>
              ax+{&minus;(ax)} = 0
            </section>
            Because the additive inverse is unique, (&minus;a)x = &minus;(ax).
          </p>
          <p>
            [3]: By the field axioms, the vector space axioms and [2],
            <section>
              <table>
                <tr>
                  <td>a(&minus;x)</td>
                  <td>= a{(&minus;1)x}</td>
                  <td>
                    &because; (&minus;1)x = &minus;x by [2]
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td>= {a(&minus;1)}x</td>
                  <td>by scalar multiplicative associativity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= (&minus;a)x</td>
                  <td>by field multiplication</td>
                </tr>
              </table>
            </section>
          </p>
          <p>
            [4]: By the vector space axioms and the cancellation low for vector addition,
            <section>
              <table>
                <tr>
                  <td>a0+a0</td>
                  <td>= a(0+0)</td>
                  <td>by vector additive associativity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a0</td>
                  <td>the vector additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a0+0</td>
                  <td>the vector additive identity</td>
                </tr>
                <tr>
                  <td colspan="2" class="center">a0 = 0</td>
                  <td>by the cancellation low for vector addition</td>
                </tr>
              </table>
            </section>
          </p>
        </section>
      </p>
    </section>
    <section id="tuple" class="description">
      <p>
        <strong>Notation (n-tuple)</strong>:
        Let <var>a<sub>1</sub>, a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var> be elements of a field <var>F</var>. We call an object of the following form an n-tuple with elements from <var>F</var>:
        <section>
          <table>
            <tr>
              <td>
                (a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>n</sub>)
              </td>
            </tr>
          </table>
        </section>
        where the following statement holds:
        for any <var>i</var>, where 1 &le; <var>i</var> &le; <var>n</var>
        <section>
          <table>
            <tr>
              <td>
                (a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>n</sub>)
                &equals;
                (b<sub>1</sub>, b<sub>2</sub>, &hellip;, b<sub>n</sub>)
              </td>
              <td>&iff;</td>
              <td>
                a<sub>i</sub> = b<sub>i</sub>
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="set-of-tuple" class="description">
      <p>
        <strong>Notation (a set of n-tuples)</strong>:
        A set of all n-tuples with entries from a filed <var>F</var> is denoted <var>F<sup>n</sup></var>, e.g.,
        <section>
          <table>
            <tr>
              <td>
                Let <var>u</var> be an n-tuple
                (a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>n</sub>),
              </td>
              <td>then</td>
              <td>
                u &in; F<sup>n</sup>
              </td>
            </tr>
            <tr>
              <td>
                Let <var>v</var> be an n-tuple
                (b<sub>1</sub>, b<sub>2</sub>, &hellip;, b<sub>n</sub>),
              </td>
              <td>then</td>
              <td>
                v &in; F<sup>n</sup>
              </td>
            </tr>
          </table>
        </section>
        <var>F<sup>n</sup></var> is a vector space over <var>F</var> with the operations of coordinative addition and scalar multiplication; that is, if <var>c</var> &in; <var>F</var>, then
        <section>
          <table>
            <tr>
              <td>
                u+v =
                (a<sub>1</sub>+b<sub>1</sub>, a<sub>2</sub>+b<sub>2</sub>,
                &hellip;, a<sub>n</sub>+b<sub>n</sub>)
              </td>
            </tr>
            <tr>
              <td>
                cu = (ca<sub>1</sub>, ca<sub>2</sub>, &hellip;, ca<sub>n</sub>)
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="column-vector" class="description">
      <p>
        <strong>Notation (column vector)</strong>:
        Vectors in <var>F<sup>n</sup></var> may be written as column vectors:
        <section>
          <math>
            <mrow>
              <mo fence="true">&lpar;</mo>
              <mtable>
                <mtr><mtd>
                  <msub><mi>a</mi><mn>1</mn></msub>
                </mtd></mtr>
                <mtr><mtd>
                  <msub><mi>a</mi><mn>2</mn></msub>
                </mtd></mtr>
                <mtr><mtd>
                  <mo>&vellip;</mo>
                </mtd></mtr>
                <mtr><mtd>
                  <msub><mi>a</mi><mi>n</mi></msub>
                </mtd></mtr>
              </mtable>
              <mo fence="true">&rpar;</mo>
            </mrow>
          </math>
        </section>
        rather than as row vectors
        <math displaystyle="true">
          <mrow>
            <mo fence="true">&lpar;</mo>
            <msub><mi>a</mi><mn>1</mn></msub><mo>,</mo>
            <msub><mi>a</mi><mn>2</mn></msub><mo>,</mo>
            <mi>&hellip;</mi><mo>,</mo>
            <msub><mi>a</mi><mi>n</mi></msub>
            <mo fence="true">&rpar;</mo>
          </mrow>
        </math>
      </p>
    </section>
    <section id="matrix" class="description">
      <p>
        <strong>Notation (matrix)</strong>:
        An <var>m&times;n</var> matrix with entries from a field <var>F</var> is
        <section>
          <math displaystyle="true">
            <mrow>
              <mo fence="true">&lpar;</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <msub><mi>a</mi><mrow><mn>1</mn><mn>1</mn></mrow></msub>
                  </mtd>
                  <mtd>
                    <msub><mi>a</mi><mrow><mn>1</mn><mn>2</mn></mrow></msub>
                  </mtd>
                  <mtd><mi>&ctdot;</mi></mtd>
                  <mtd>
                    <msub><mi>a</mi><mrow><mn>1</mn><mi>n</mi></mrow></msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub><mi>a</mi><mrow><mn>2</mn><mn>1</mn></mrow></msub>
                  </mtd>
                  <mtd>
                    <msub><mi>a</mi><mrow><mn>2</mn><mn>2</mn></mrow></msub>
                  </mtd>
                  <mtd><mi>&ctdot;</mi></mtd>
                  <mtd>
                    <msub><mi>a</mi><mrow><mn>2</mn><mi>n</mi></mrow></msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd><mi>&vellip;</mi></mtd>
                  <mtd><mi>&vellip;</mi></mtd>
                  <mtd></mtd>
                  <mtd><mi>&vellip;</mi></mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub><mi>a</mi><mrow><mi>m</mi><mn>1</mn></mrow></msub>
                  </mtd>
                  <mtd>
                    <msub><mi>a</mi><mrow><mi>m</mi><mn>2</mn></mrow></msub>
                  </mtd>
                  <mtd><mi>&ctdot;</mi></mtd>
                  <mtd>
                    <msub><mi>a</mi><mrow><mi>m</mi><mi>n</mi></mrow></msub>
                  </mtd>
                </mtr>
              </mtable>
              <mo fence="true">&rpar;</mo>
            </mrow>
          </math>          
        </section>
        where each entry
        <var>a<sub>ij</sub></var> (1 &le; i &le; m, 1 &le; j &le; n)
        is an element of <var>F</var>.
        <ul>
          <li>
            diagonal entries:
            <var>a<sub>ij</sub> with <var>i</var> = <var>j</var>
          </li>
          <li>
            <b>i</b>th row:
            <var>a<sub>i1</sub>, a<sub>i2</sub>, &hellip;, a<sub>in</sub></var>
          </li>
          <li>
            <b>j</b>th column:
            <var>a<sub>1j</sub>, a<sub>2j</sub>, &hellip;, a<sub>mj</sub></var>
          </li>
          <li>
            zero matrix: denoted 0, all of its entries are 0
          </li>
          <li>
            square matrix [正方行列]:
            <var>n&times;n</var> matrices for any <var>n</var> &ge; 0
          </li>
        </ul>
      </p>
      <p>
        We denote
        <ul>
          <li>matrices by capital letters, e.g. <var>A</var>, and</li>
          <li>the entry of a matrix <var>A</var> that lies in row <var>i</var> and column <var>j</var> by <var>A<sub>ij</sub></var>.</li>
        </ul>
      </p>
    </section>
    <section id="set-of-matrices" class="description">
      <p>
        <strong>Notation (a set of matrices)</strong>:
        The set of all <var>m&times;n</var> matrices with each entries from a field <var>F</var> is denoted M<sub>m&times;n</sub>(F).
      </p>
      <p>
        M<sub>m&times;n</sub>(F) is a vector space over <var>F</var> with the following operations of matrix addition and scalar multiplication; for
        <var>A, B</var> &in; M<sub>m&times;n</sub>(F) and <var>c</var> &in; <var>F</var>, and 1 &le; <var>i</var> &le; <var>m</var> and 1 &le; <var>j</var> &le; <var>n</var>,
        <section>
        </section>
      </p>
      <section>
        <table>
          <tr>
            <td>
              (A+B)<sub>ij</sub> = A<sub>ij</sub>+B<sub>ij</sub>
            </td>
          </tr>
          <tr>
            <td>
              (cA)<sub>ij</sub> = cA<sub>ij</sub>
            </td>
          </tr>
        </table>
      </section>
      <p>
        The definitions of matrix addition and scalar multiplication in M<sub>m&times;n</sub>(F) are natural extensions of the corresponding operations in <var>F<sup>n</sup></var> and <var>F<sup>m</sup></var>. Let <var>A, B</var> &in; M<sub>m&times;n</sub>(F) and <var>c</var> &in; <var>F</var>, then
        <ul>
          <li>
            <var>A + B</var> is the matrix M<sub>m&times;n</sub>(F) whose <b>i</b>th row vector is the sum of the <b>i</b>th row vectors of <var>A</var> and <var>B</var>, and
          </li>
          <li>
            <var>c&InvisibleTimes;A</var> is the matrix M<sub>m&times;n</sub>(F) whose <b>i</b>th row vector is <var>c</var> times the <b>i</b>th row vector of <var>A</var>.
          </li>
        </ul>
      </p>
    </section>
    <section id="polynomial" class="description">
      <p>
        <strong>Polynomial [多項式]</strong>:
        A polynomial with coefficients [係数] from a field <var>F</var> is an expression [式] of the form
        <section>
          f(x)
          = a<sub>n</sub>&InvisibleTimes;x<sup>n</sup>
          + a<sub>n-1</sub>&InvisibleTimes;x<sup>n-1</sup>
          + &ctdot; + a<sub>1</sub>&InvisibleTimes;x + a<sub>0</sub>
        </section>
      </p>
    </section>
    <section id="polynomail-degree" class="description">
      <p>
        <strong>Degree [次数]</strong>:
        The degree of a polynomial is defined to the largest exponent of <var>x</var> with a non-zero coefficient.
      </p>
      <p>
        If <var>a<sub>i</sub></var> = 0 for 0 &le; <var>i</var> &le; <var>n</var>, that is f(x) = 0, then
        <ul>
          <li>
            f(x) is called the zero polynomial
          </li>
          <li>
            its degree is defined to be &minus;1
          </li>
        </ul>
      </p>
    </section>
    <section id="equal-polynomials" class="description">
      <p>
        <strong>Equality of polynomials</strong>:
        Two polynomials,
        <section>
          <table>
            <tr>
              <td>
                f(x)
                = a<sub>n</sub>&InvisibleTimes;x<sup>n</sup>
                + a<sub>n-1</sub>&InvisibleTimes;x<sup>n-1</sup>
                + &ctdot; + a<sub>1</sub>&InvisibleTimes;x + a<sub>0</sub>
              </td>
            </tr>
            <tr>
              <td>
                g(x)
                = b<sub>m</sub>&InvisibleTimes;x<sup>m</sup>
                + b<sub>m-1</sub>&InvisibleTimes;x<sup>m-1</sup>
                + &ctdot; + b<sub>1</sub>&InvisibleTimes;x + b<sub>0</sub>
              </td>
            </tr>
          </table>
        </section>
        are called equal if <var>m</var> = <var>n</var> and
        <var>a<sub>i</sub></var> = <var>b<sub>i</sub></var>
        for <var>i</var> = 0, 1, &hellip;, <var>n</var>.
      </p>
    </section>
    <section id="set-of-polynomials" class="description">
      <p>
        <strong>Notation (a set of polynomials)</strong>:
        A set of all polynomials with coefficients from a field <var>F</var>, denoted P(F), is a vector space with the two operations &mdash; suppose <var>m</var> &le; <var>n</var>, and <var>b<sub>m+1</sub></var> = <var>b<sub>m+2</sub></var> = &ctdot; = <var>b<sub>n</sub></var> = 0, then
        <section>
          <table>
            <tr>
              <td>
                f(x)+g(x)
                = (a<sub>n</sub>+b<sub>n</sub>)&InvisibleTimes;x<sup>n</sup>
                + (a<sub>n-1</sub>+b<sub>n-1</sub>)&InvisibleTimes;x<sup>n-1</sup>
                + &ctdot;
                + (a<sub>1</sub>+b<sub>1</sub>)&InvisibleTimes;x
                + (a<sub>0</sub>+b<sub>0</sub>)
              </td>
            </tr>
            <tr>
              <td>
                c&InvisibleTimes;f(x)
                = c&InvisibleTimes;a<sub>n</sub>&InvisibleTimes;x<sup>n</sup>
                + c&InvisibleTimes;a<sub>n-1</sub>&InvisibleTimes;x<sup>n-1</sup>
                + &ctdot;
                + c&InvisibleTimes;a<sub>1</sub>&InvisibleTimes;x
                + c&InvisibleTimes;a<sub>0</sub>
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="sequence" class="description">
      <p>
        <strong>Notation (sequence)</strong>:
        A sequence in a field <var>F</var> is a function X:&naturals;&rightarrow;F. The sequence <var>X</var>, denoted (x<sub>n</sub>), can be written informally as an infinite list of elements from <var>F</var> as
        <section>
          X = (x<sub>n</sub>)
            = (x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>,&hellip;)
        </section>
        where x<sub>n</sub> = X(n). For example, the Fibonacci sequence <var>X</var> is defined as
        <section>
          <table>
            <tr>
              <td colspan="2">
                X(1) = x<sub>1</sub> = 1
              </td>
            </tr>
            <tr>
              <td colspan="2">
                X(2) = x<sub>2</sub> = 1
              </td>
            </tr>
            <tr>
              <td>
                X(n) = X(n-1) + X(n-2)
              </td>
              <td>
                where <var>n</var> &ge; 3
              </td>
            </tr>
          </table>
        </section>
        or informally (1, 1, 2, 3, 5, 8, 13,&hellip;).
      </p>
      <p>
        Let <var>V</var> consist of all the sequences (a<sub>n</sub>) in <var>F</var>. <var>V</var> is a vector space with the following operations, for (a<sub>n</sub>), (b<sub>n</sub>) &in; <var>V</var> and <var>t</var> &in; <var>F</var>
        <section>
          <table>
            <tr>
              <td>
                (a<sub>n</sub>)+(b<sub>n</sub>)
                = (a<sub>n</sub>+b<sub>n</sub>)
              </td>
            </tr>
            <tr>
              <td>
                t&InvisibleTimes;(a<sub>n</sub>)
                = (t&InvisibleTimes;a<sub>n</sub>)
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="zero-vector-space" class="description">
      <p>
        <strong>Zero vector space</strong>:
        Let <var>V</var> = {0} &mdash; the zero vector space &mdash; consist of a single vector <var>0</var>. <var>V</var> is a vector space over a field <var>F</var>, for each scalar <var>c</var> &in; <var>F</var>:
        <section>
          <table>
            <tr><td>0 + 0 = 0</td></tr>
            <tr><td>c0 = 0</td></tr>
          </table>
        </section>
      </p>
    </section>
  </section>
  <section id="subspaces">
    <h2>Subspaces</h2>
    <section id="define-subspace" class="definition">
      <p>
        <strong>Definition (subspace 部分空間)</strong>:
        A subset of a vector space <var>V</var> over a field <var>F</var> is called a <strong>subspace</strong> of <var>V</var>
        &mdash;
        if the subset is a vector space over <var>F</var> with the operations of addition and scalar multiplication defined on <var>V</var>.
      </p>
    </section>
    <section id="vector-space-is-subspace" class="proposition">
      <p>
        <strong>Proposition (vector space is subspace)</strong>:
        <var>V</var> is a subspace of <var>V</var>, because
        <ul>
          <li>V &subseteq; V</li>
          <li>
            <var>V</var> is a vector space with the operations defined on <var>V</var>.
          </li>
        </ul>
      </p>
    </section>
    <section id="zero-subspace" class="proposition">
      <p>
        <strong>Proposition (zero subspace)</strong>:
        {0} is a subspace of any vector space, because
        <ul>
          <li>
            {0} is a subset of any vector space, since any vector space contains <var>0</var>,
          </li>
          <li>
            vector addition and scalar multiplication on <var>0</var> are defined in any vector space.
          </li>
        </ul>
    </section>
    <section id="subspace-conditions" class="proposition">
      <p>
        <strong>Theorem 1.3 (subspace conditions)</strong>:
        Let <var>W</var> be a subset of a vector space <var>V</var>, then
        <section>
          <var>W</var> is a subspace of <var>V</var>
        </section>
        if and only if the following three conditions hold, for any scalar <var>c</var>
        <section>
          <table>
            <tr>
              <td>0 &in; W</td>
              <td>
                <var>W</var> has a zero vector
              </td>
            </tr>
            <tr>
              <td>x+y &in; W</td>
              <td>
                <var>W</var> is closed under addition
              </td>
            </tr>
            <tr>
              <td>cx &in; W</td>
              <td>
                <var>W</var> is closed under scalar multiplication
              </td>
            </tr>
          </table>
        </section>
      </p>
      <p>
        Proof:
        <section>
          <p>
            If <var>W</var> is a subspace of <var>V</var>, then
            <ul>
              <li>
                <var>W</var> is a vector space with addition and scalar multiplication defined on <var>V</var> by definition of the subspace, hence <var>W</var> is closed under addition and scalar multiplication.
              </li>
              <li>
                <var>W</var> is a vector space containing an additive identity, say 0&prime;, such that <var>x</var>+0&prime; = <var>x</var> for each <var>x</var> &in; <var>W</var>. But also <var>x</var> + 0 = <var>x</var>, and thus 0&prime; = 0 by the cancellation law for vector addition. Hence 0 &in; <var>W</var>.
              </li>
            </ul>
          </p>
        </section>
        <section>
          <p>
            Conversely, recall the vector space axioms.
            <section>
              <table>
                <tr>
                  <th></th>
                  <th>vector addition</th>
                  <th>scalar multiplication</th>
                </tr>
                <tr>
                  <td>commutativity</td>
                  <td>x+y = y+x</td>
                  <td></td>
                </tr>
                <tr>
                  <td>associativity</td>
                  <td>(x+y)+z = x+(y+z)</td>
                  <td>(ab)x = a(bx)</td>
                </tr>
                <tr>
                  <td>existence of identity</td>
                  <td>x+0 = x</td>
                  <td>1x = x</td>
                </tr>
                <tr>
                  <td>existence of inverse</td>
                  <td>x+(&minus;x) = 0</td>
                  <td></td>
                </tr>
                <tr>
                  <td>distributivity</td>
                  <td>a(x+y) = ax+ay</td>
                  <td>(a+b)x=ax+bx</td>
                </tr>
              </table>
            </section>
            These properties except for
            <section>
              <table>
                <tr>
                  <td>x+0 = x</td>
                  <td>the additive identity</td>
                </tr>
                <tr>
                  <td>x+(&minus;x) = 0</td>
                  <td>the additive inverse</td>
                </tr>
              </table>
            </section>
            hold for all vectors in the vector space. Therefore, these properties automatically hold for the vectors in any subset. Thus a subset <var>W</var> of a vector space <var>V</var> is a subspace of <var>V</var> if and only if the following four properties hold:
            <section>
              <table>
                <tr>
                  <td>0 &in; W</td>
                  <td>
                    <var>W</var> has a zero vector
                  </td>
                </tr>
                <tr>
                  <td>x+y &in; W</td>
                  <td>
                    <var>W</var> is closed under addition
                  </td>
                </tr>
                <tr>
                  <td>cx &in; W</td>
                  <td>
                    <var>W</var> is closed under scalar multiplication
                  </td>
                </tr>
                <tr>
                  <td>x+(&minus;x) = 0</td>
                  <td>
                    for each <var>x</var> &in; <var>W</var>, &minus;<var>x</var> &in; <var>W</var>
                  </td>
                </tr>
              </table>
            </section>
            Since the hypothesis of this implication covers the first three properties, it suffices to prove that the additive inverse of each vector in <var>W</var> lies in <var>W</var>. But if <var>x</var> &in; <var>W</var>, then
            <section>
              <table>
                <tr>
                  <td>(&minus;1)x &in; W</td>
                  <td>
                    by the third condition <var>cx</var> &in; <var>W</var>
                  </td>
                </tr>
                <tr>
                  <td>(&minus;1)x = &minus;x</td>
                  <td>
                    by theorem (scalar multiplications in vector spaces)
                  </td>
                </tr>
              </table>
            </section>
            Thus &minus;x &in; <var>W</var>.
            Hence <var>W</var> is a subspace of <var>V</var>. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="additive-inverse-subspace" class="proposition">
      <p>
        <strong>Corollary (additive inverse)</strong>:
        Let <var>W</var> be a subspace of <var>V</var>, then there exists <var>&minus;x</var> &in; <var>W</var> for each <var>x</var> &in; <var>W</var>.
      </p>
      <p>
        Proof: For each <var>x</var> &in; <var>W</var>,
        <section>
          <table>
            <tr>
              <td>
                &minus;x
              </td>
              <td>
                = (&minus;1)&InvisibleTimes;x
              </td>
              <td>&because; vector multiplication theorem</td>
            </tr>
            <tr>
              <td></td>
              <td>
                &in; W
              </td>
              <td>&because; subspace condition 3 &marker;</td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="transpose" class="description">
      <p>
        <strong>Notation (transpose [転置行列])</strong>:
        The transpose <var>A<sup>t</sup></var> of an <var>m&times;n</var> matrix <var>A</var> is the <var>n&times;m</var> matrix obtained from <var>A</var> by interchanging the rows with the columns; that is,
        <section>
          (A<sup>t</sup>)<sub>ij</sub> = A<sub>ji</sub>
        </section>
        For example,
        <section>
          <math>
            <mrow>
              <mrow>
                <msup>
                  <mrow>
                    <mo fence="true">&lpar;</mo>
                    <mtable>
                      <mtr>
                        <mtd><mn>1</mn></mtd>
                        <mtd><mn>&minus;2</mn></mtd>
                        <mtd><mn>3</mn></mtd>
                      </mtr>
                      <mtr>
                        <mtd><mn>0</mn></mtd>
                        <mtd><mn>5</mn></mtd>
                        <mtd><mn>&minus;1</mn></mtd>
                      </mtr>
                    </mtable>
                    <mo fence="true">&rpar;</mo>
                  </mrow>
                  <mi>t</mi>
                </msup>
              </mrow>
              <mo>&equals;</mo>
              <mrow>
                <mo fence="true">&lpar;</mo>
                <mtable>
                  <mtr>
                    <mtd><mn>1</mn></mtd>
                    <mtd><mn>0</mn></mtd>
                  </mtr>
                  <mtr>
                    <mtd><mn>&minus;2</mn></mtd>
                    <mtd><mn>5</mn></mtd>
                  </mtr>
                  <mtr>
                    <mtd><mn>3</mn></mtd>
                    <mtd><mn>&minus;1</mn></mtd>
                  </mtr>
                </mtable>
                <mo fence="true">&rpar;</mo>
              </mrow>
            </mrow>
          </math>
        </section>
      </p>
    </section>
    <section id="distributivity-of-transpose" class="proposition">
      <p>
        <strong>Lemma (distributivity of transposes)</strong>:
        Let <var>A, B</var> &in; M<sub>m&times;n</sub>(F) and <var>a, b</var> &in; <var>F</var>, then
        <section>
          (a&InvisibleTimes;A + b&InvisibleTimes;B)<sup>t</sup>
          = a&InvisibleTimes;A<sup>t</sup> + b&InvisibleTimes;B<sup>t</sup>
        </section>
        Proof:
        <section>
          <table>
            <tr>
              <td>
                ((aA + bB)<sup>t</sup>)<sub>ij</sub>
              </td>
              <td>
                = (aA + bB)<sub>ji</sub>
              </td>
              <td>
                &because;
                (A<sup>t</sup>)<sub>ij</sub> = A<sub>ji</sub>
              </td>
            </tr>
            <tr>
              <td></td>
              <td>
                = (aA)<sub>ji</sub> + (bB)<sub>ji</sub>
              </td>
              <td>
                &because;
                (A+B)<sub>ij</sub> = A<sub>ij</sub> + B<sub>ij</sub>
              </td>
            </tr>
            <tr>
              <td></td>
              <td>
                = ((aA)<sup>t</sup>)<sub>ij</sub>
                + ((bB)<sup>t</sup>)<sub>ij</sub>
              </td>
              <td>
                &because;
                (A<sup>t</sup>)<sub>ij</sub> = A<sub>ji</sub>
              </td>
            </tr>
            <tr>
              <td></td>
              <td>
                = a(A<sup>t</sup>)<sub>ij</sub> + b(B<sup>t</sup>)<sub>ij</sub>
              </td>
              <td>
                &because;
                (cA)<sub>ij</sub> = cA<sub>ij</sub>
                &marker;
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="symmetric-matrix" class="description">
      <p>
        <strong>Symmetric matrix [対称行列]</strong>:
        A symmetric matrix is a matrix such that
        <section>
          A<sup>t</sup> = A
        </section>
        For example,
        <section>
          <math>
            <mrow>
              <mrow>
                <msup>
                  <mrow>
                    <mo fence="true">&lpar;</mo>
                    <mtable>
                      <mtr>
                        <mtd><mn>1</mn></mtd>
                        <mtd><mn>2</mn></mtd>
                      </mtr>
                      <mtr>
                        <mtd><mn>2</mn></mtd>
                        <mtd><mn>3</mn></mtd>
                      </mtr>
                    </mtable>
                    <mo fence="true">&rpar;</mo>
                  </mrow>
                  <mrow><mi>t</mi></mrow>
                </msup>
              </mrow>
              <mo>&equals;</mo>
              <mrow>
                <mo fence="true">&lpar;</mo>
                <mtable>
                  <mtr>
                    <mtd><mn>1</mn></mtd>
                    <mtd><mn>2</mn></mtd>
                  </mtr>
                  <mtr>
                    <mtd><mn>2</mn></mtd>
                    <mtd><mn>3</mn></mtd>
                  </mtr>
                </mtable>
                <mo fence="true">&rpar;</mo>
              </mrow>
            </mrow>
          </math>
        </section>
        The set <var>W</var> of all symmetric matrices in M<sub>n&times;n</sub>(F) is a subspace of M<sub>n&times;n</sub>(F), because the subspace conditions hold for <var>W</var>. Let <var>A, B</var> &in; <var>W</var> and <var>a</var> &in; <var>F</var>,
        <section>
          <table>
            <tr>
              <td>0 &in; W</td>
              <td>
                &because; 0<sup>t</sup> = 0
              </td>
            </tr>
            <tr>
              <td>A+B &in; W</td>
              <td>
                &because;
                (A+B)<sup>t</sup>
                = A<sup>t</sup>+B<sup>t</sup>
                = A+B
              </td>
            </tr>
            <tr>
              <td>aA &in; W</td>
              <td>
                &because;
                (aA)<sup>t</sup>
                = aA<sup>t</sup>
                = aA
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="upper-triangular" class="description">
      <p>
        <strong>Upper triangular [上三角行列]</strong>:
        An <var>m&times;n</var> matrix <var>A</var> is called upper triangular if <var>A<sub>ij</sub></var> = 0 whenever <var>i</var> &gt; <var>j</var>.
        <section>
          <math>
            <mrow>
              <mrow><mi>A</mi></mrow>
              <mo>&equals;</mo>
              <mrow>
                <mo fence="true">&lpar;</mo>
                <mtable>
                  <mtr>
                    <mtd><mn>1</mn></mtd>
                    <mtd><mn>2</mn></mtd>
                    <mtd><mn>3</mn></mtd>
                    <mtd><mn>4</mn></mtd>
                  </mtr>
                  <mtr>
                    <mtd><mn>0</mn></mtd>
                    <mtd><mn>5</mn></mtd>
                    <mtd><mn>6</mn></mtd>
                    <mtd><mn>7</mn></mtd>
                  </mtr>
                  <mtr>
                    <mtd><mn>0</mn></mtd>
                    <mtd><mn>0</mn></mtd>
                    <mtd><mn>8</mn></mtd>
                    <mtd><mn>9</mn></mtd>
                  </mtr>
                </mtable>
                <mo fence="true">&rpar;</mo>
              </mrow>
            </mrow>
          </math>
        </section>
      </p>
    </section>
    <section id="diagnal-matrix" class="description">
      <strong>Diagonal matrix [対角行列]</strong>:
      A square matrix <var>A</var> is called a diagonal matrix if <var>A<sub>ij</sub></var> = 0 whenever <var>i</var> &ne; <var>j</var>.
      <section>
        <math>
          <mrow>
            <mrow><mi>A</mi></mrow>
            <mo>&equals;</mo>
            <mrow>
              <mo fence="true">&lpar;</mo>
              <mtable>
                <mtr>
                  <mtd><mn>3</mn></mtd>
                  <mtd><mn>0</mn></mtd>
                  <mtd><mn>0</mn></mtd>
                </mtr>
                <mtr>
                  <mtd><mn>0</mn></mtd>
                  <mtd><mn>&minus;2</mn></mtd>
                  <mtd><mn>0</mn></mtd>
                </mtr>
                <mtr>
                  <mtd><mn>0</mn></mtd>
                  <mtd><mn>0</mn></mtd>
                  <mtd><mn>8</mn></mtd>
                </mtr>
              </mtable>
              <mo fence="true">&rpar;</mo>
            </mrow>
          </mrow>
        </math>
      </section>
    </section>
    <section id="diagnonal-subspace" class="description">
      <p>
        The set of diagonal matrices <var>W</var> is a subspace of M<sub>n&times;n</sub>(F), because the subspace conditions hold for <var>W</var>. Let <var>A, B</var> be diagonal matrices and let <var>c</var> &in; <var>F</var>,
        <section>
          <table>
            <tr>
              <td>0 &in; W</td>
              <td>
                &because; the zero matrix is diagonal
              </td>
            </tr>
            <tr>
              <td>A+B &in; W</td>
              <td>
                &because; whenever <var>i</var> &ne; <var>j</var>,
                (A+B)<sub>ij</sub>
                = A<sub>ij</sub> + B<sub>ij</sub>
                = 0 + 0 = 0
              </td>
            </tr>
            <tr>
              <td>cA &in; W</td>
              <td>
                &because; whenever <var>i</var> &ne; <var>j</var>,
                (cA)<sub>ij</sub> = cA<sub>ij</sub> = c0 = 0
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="polynominal-set-degree" class="description">
      <p>
        <strong>
          Notation (a set of all polynomials with degree <var>&le; n</var>)
        </strong>:
        Let <var>n</var> &ge; 0, and let P<sub>n</sub>(F) consist of all polynomials in P(F) having degree &le; <var>n</var>. Then P<sub>n</sub>(F) is a subspace of P(F), because the subspace conditions hold for P<sub>n</sub>(F). Let <var>p<sub>1</sub>, p<sub>2</sub></var> &in; P<sub>n</sub>(F), and let <var>c</var> &in; <var>F</var>,
        <section>
          <table>
            <tr>
              <td>zero &in; P<sub>n</sub>(F)</td>
              <td>
                &because;
                the zero polynomial has degree -1 &lt; <var>n</var>
              </td>
            </tr>
            <tr>
              <td>
                p<sub>1</sub>+p<sub>2</sub> &in; P<sub>n</sub>(F)
              </td>
              <td>
                &because; p<sub>1</sub>+p<sub>2</sub>
                is another polynomial of degree &le; <var>n</var>
              </td>
            </tr>
            <tr>
              <td>
                cp<sub>1</sub> &in; P<sub>n</sub>(F)
              </td>
              <td>
                &because; cp<sub>1</sub>
                is another polynomial of degree &le; <var>n</var>
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="sum-of-subspaces" class="definition">
      <p>
        <strong>Definition (sum of subsets)</strong>:
        The sum of non-empty subsets <var>S<sub>1</sub></var> and <var>S<sub>2</sub></var> of a vector space <var>V</var>, notated <var>S<sub>1</sub></var> + <var>S<sub>2</sub></var>, is the set
        <section>
          {
            <var>x</var> + <var>y</var> :
            <var>x</var> &in; <var>S<sub>1</sub></var>,
            <var>y</var> &in; <var>S<sub>2</sub></var>
          }
        </section>
      </p>
      <p>
        <strong>Definition (direct sum [直和])</strong>:
        The direct sum of <var>W<sub>1</sub></var> and <var>W<sub>2</sub></var>
        <section>
          <var>V</var>
          = <var>W<sub>1</sub></var> &CirclePlus; <var>W<sub>2</sub></var>
        </section>
        is a vector space <var>V</var> if <var>W<sub>1</sub></var> and <var>W<sub>2</sub></var> are subspaces of <var>V</var> such that
        <section>
          <table>
            <tr>
              <td>
                <var>W<sub>1</sub></var> &cap; <var>W<sub>2</sub></var>
                = &empty;
              </td>
            </tr>
            <tr>
              <td>
                <var>W<sub>1</sub></var> &plus; <var>W<sub>2</sub></var>
                = <var>V</var>
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="skew-matix" class="description">
      <p>
        <strong>Skew-symmetric matrix [交代行列]</strong>:
        A square matrix <var>M</var> is called skew-symmetric if
        <section>
          <var>M<sup>t</sup></var> = <var>&minus;M</var>
        </section>
        For example,
        <section>
          <table>
            <tr>
              <td>
                <math>
                  <mrow>
                    <mi>M</mi>
                    <mo>&equals;</mo>
                    <mo fence="true">&lpar;</mo>
                    <mtable>
                      <mtr>
                        <mtd>0</mtd>
                        <mtd>1</mtd>
                        <mtd>&minus;2</mtd>
                      </mtr>
                      <mtr>
                        <mtd>&minus;1</mtd>
                        <mtd>0</mtd>
                        <mtd>3</mtd>
                      </mtr>
                      <mtr>
                        <mtd>2</mtd>
                        <mtd>&minus;3</mtd>
                        <mtd>0</mtd>
                      </mtr>
                    </mtable>
                    <mo fence="true">&rpar;</mo>
                  </mrow>
                </math>
              </td>
              <td>
                <math>
                  <mrow>
                    <msup><mi>M</mi><mi>t</mi></msup>
                    <mo>&equals;</mo>
                    <mo fence="true">&lpar;</mo>
                    <mtable>
                      <mtr>
                        <mtd>0</mtd>
                        <mtd>&minus;1</mtd>
                        <mtd>2</mtd>
                      </mtr>
                      <mtr>
                        <mtd>1</mtd>
                        <mtd>0</mtd>
                        <mtd>&minus;3</mtd>
                      </mtr>
                      <mtr>
                        <mtd>&minus;2</mtd>
                        <mtd>3</mtd>
                        <mtd>0</mtd>
                      </mtr>
                    </mtable>
                    <mo fence="true">&rpar;</mo>
                  </mrow>
                </math>
              </td>
              <td>
                <math>
                  <mrow>
                    <mo>&minus;</mo>
                    <msup><mi>M</mi><mi>t</mi></msup>
                    <mo>&equals;</mo>
                    <mo fence="true">&lpar;</mo>
                    <mtable>
                      <mtr>
                        <mtd>0</mtd>
                        <mtd>1</mtd>
                        <mtd>&minus;2</mtd>
                      </mtr>
                      <mtr>
                        <mtd>&minus;1</mtd>
                        <mtd>0</mtd>
                        <mtd>3</mtd>
                      </mtr>
                      <mtr>
                        <mtd>2</mtd>
                        <mtd>&minus;3</mtd>
                        <mtd>0</mtd>
                      </mtr>
                    </mtable>
                    <mo fence="true">&rpar;</mo>
                  </mrow>
                </math>
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
  </section>
  <section id="linear-combination-equation">
    <h2>Linear Combinations</h2>
    <section id="linear-combination" class="definition">
      <p>
        <strong>Definition (linear combination [線形結合])</strong>:
        Let <var>S</var> be a set of vectors
        {<var>v<sub>1</sub></var>, <var>v<sub>2</sub></var>, &hellip;, <var>v<sub>n</sub></var>} in a vector space <var>V</var>.
        Any vector of the form
        <section>
          <var>v</var>
          = <var>a<sub>1</sub>v<sub>1</sub></var>
          + <var>a<sub>2</sub>v<sub>2</sub></var>
          + &ctdot;
          + <var>a<sub>n</sub>v<sub>n</sub></var>
        </section>
        for some scalars
        <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>,
        is called a linear combination of <var>S</var>.
      </p>
    </section>
    <section id="zero-vector-linear" class="proposition">
      <p>
        <strong>Corollary (zero vector is a linear combination)</strong>:
        The zero vector is a linear combination of any nonempty subset of <var>V</var>.
      </p>
      <p>
        Proof:
        Observe that for each <var>v</var> &in; <var>V</var>, 0<var>v</var> = 0
        &marker;
      </p>
    </section>
    <section id="linear-combination-example2" class="description">
      <p>
        <strong>Example (linear combinations)</strong>:
        Consider linear combinations of two polynomials
        <section>
          <table>
            <tr>
              <td>
                x<sup>3</sup>&minus;2x<sup>2</sup>&minus;5x&minus;3
              </td>
            </tr>
            <tr>
              <td>
                3x<sup>3</sup>&minus;5x<sup>2</sup>&minus;4x&minus;9
              </td>
            </tr>
          </table>
        </section>
        Case 1:
        2x<sup>3</sup>&minus;2x<sup>2</sup>&plus;12x&minus;6
        is a linear combination of the polynomials in P<sub>3</sub>(&reals;)
        <section>
          <p>
            <section>
              <table>
                <tr>
                  <td>
                    2x<sup>3</sup>&minus;2x<sup>2</sup>&plus;12x&minus;6
                  </td>
                  <td>
                    = a(x<sup>3</sup>&minus;2x<sup>2</sup>&minus;5x&minus;3)
                    + b(3x<sup>3</sup>&minus;5x<sup>2</sup>&minus;4x&minus;9)
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td>
                    = (a&plus;3b)x<sup>3</sup>
                    + (&minus;2a&minus;5b)x<sup>2</sup>
                    + (&minus;5a&minus;4b)x + (&minus;3a&minus;9b)
                  </td>
                </tr>
              </table>
            </section>
            Thus we are led to the following system of linear equations, adding appropriate multiples of the first equation to the others in order to eliminate <var>a</var>, and adding the appropriate multiples of the second equation to the others yields <var>a</var> and <var>b</var>:
            <section>
              <table>
                <tr class="right">
                  <td>a+3b =</td><td>2</td>
                  <td rowspan="4" style="vertical-align: middle;">
                    &LongRightArrow;
                  </td>
                  <td>a+3b =</td><td>2</td>
                  <td rowspan="4" style="vertical-align: middle;">
                    &LongRightArrow;
                  </td>
                  <td>a =</td><td>&minus;4</td>
                </tr>
                <tr class="right">
                  <td>&minus;2a&minus;5b =</td><td>&minus;2</td>
                  <td>b =</td><td>2</td>
                  <td>b =</td><td>2</td>
                </tr>
                <tr class="right">
                  <td>&minus;5a&minus;4b =</td><td>12</td>
                  <td>11b =</td><td>22</td>
                  <td>0 =</td><td>0</td>
                </tr>
                <tr class="right">
                  <td>&minus;3a&minus;9b =</td><td>&minus;6</td>
                  <td>0b =</td><td>0</td>
                  <td>0 =</td><td>0</td>
                </tr>
              </table>
            </section>
            Hence 2x<sup>3</sup>&minus;2x<sup>2</sup>&plus;12x&minus;6
            is a linear combinations of the two polynomials:
            <section>
              <table>
                <tr>
                  <td>
                    2x<sup>3</sup>&minus;2x<sup>2</sup>&plus;12x&minus;6
                  </td>
                  <td>
                    = &minus;4(x<sup>3</sup>&minus;2x<sup>2</sup>&minus;5x&minus;3)
                    + 2(3x<sup>3</sup>&minus;5x<sup>2</sup>&minus;4x&minus;9)
                  </td>
                </tr>
              </table>
            </section>
          </p>
        </section>
        Case 2:
        3x<sup>3</sup>&minus;2x<sup>2</sup>&plus;7x&plus;8
        is not a linear combination of the polynomials
        <section>
          <p>
            <section>
              <table>
                <tr>
                  <td>
                    3x<sup>3</sup>&minus;2x<sup>2</sup>&plus;7x&plus;8
                  </td>
                  <td>
                    = a(x<sup>3</sup>&minus;2x<sup>2</sup>&minus;5x&minus;3)
                    + b(3x<sup>3</sup>&minus;5x<sup>2</sup>&minus;4x&minus;9)
                  </td>
                </tr>
              </table>
            </section>
            Using the preceding technique, we obtain a system of linear equations, and eliminating <var>a</var> as before yields
            <section>
              <table>
                <tr class="right">
                  <td>a+3b =</td><td>3</td>
                  <td rowspan="4" style="vertical-align: middle;">
                    &LongRightArrow;
                  </td>
                  <td>a+3b =</td><td>3</td>
                </tr>
                <tr class="right">
                  <td>&minus;2a&minus;5b =</td><td>&minus;2</td>
                  <td>b =</td><td>4</td>
                </tr>
                <tr class="right">
                  <td>&minus;5a&minus;4b =</td><td>7</td>
                  <td>11b =</td><td>22</td>
                </tr>
                <tr class="right">
                  <td>&minus;3a&minus;9b =</td><td>8</td>
                  <td>0 =</td><td>17</td>
                </tr>
              </table>
            </section>
            But the presence of the inconsistent equation <var>0</var> &equals; <var>17</var> indicates that the system of linear equations has no solution.
            Hence 3x<sup>3</sup>&minus;2x<sup>2</sup>&plus;7x&plus;8
            is not a linear combination of the two polynomials.
          </p>
        </section>
      </p>
    </section>
    <section id="span" class="definition">
      <p>
        <strong>Definition (span [線形包])</strong>:
        Let <var>S</var> be a set of vectors {v<sub>1</sub>, v<sub>2</sub>, &hellip;, v<sub>n</sub>} in a vector space <var>V</var>. The span of <var>S</var>, denoted span(S) or span{ v<sub>1</sub>, v<sub>2</sub>, &hellip;, v<sub>n</sub> }, is the set consisting of all linear combinations of the vectors in <var>S</var>. For some scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var> &in; <var>F</var>,
        <section>
          span(
            v<sub>1</sub>, v<sub>2</sub>, &hellip;, v<sub>n</sub>
          ) = {
            a<sub>1</sub>v<sub>1</sub>
            + a<sub>2</sub>v<sub>2</sub>
            + &ctdot;
            + a<sub>n</sub>v<sub>n</sub>
          }
        </section>
      </p>
    </section>
    <section id="zero-span" class="description">
      <p>
        <strong>Zero span</strong>:
        For convenience, we define span(&empty;) = {0} &mdash; the zero vector space.
      </p>
    </section>
    <section id="plane-as-span" class="description">
      <p>
        <strong>Example (xy-plane as a span)</strong>:
        The span of the set{(1,0,0), (0,1,0)} in &reals;<sup>3</sup> consists of all vectors in &reals;<sup>3</sup> that have the form
        <section>
          a (1,0,0) + b (0,1,0) = (a,b,0)
        </section>
        for some scalars <var>a</var> and <var>b</var>.
        Thus the span of the set{(1,0,0), (0,1,0)} contains all the points in the xy-plane.
      </p>
    </section>
    <section id="linear-combination-in-subspace" class="proposition">
      <p>
        <strong>Lemma (linear combination in subspace)</strong>:
        If <var>W</var> is a subspace of a vector space <var>V</var> and <var>w<sub>1</sub></var>, <var>w<sub>2</sub></var>, &hellip;, <var>w<sub>n</sub></var> are in <var>W</var>, then for any scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>
        <section>
          a<sub>1</sub>w<sub>1</sub> + a<sub>2</sub>w<sub>2</sub> + &hellip; + a<sub>n</sub>w<sub>n</sub> &in; W
        </section>
        Proof:
        For every <var>i</var> &in; &naturals;, a<sub>i</sub>w<sub>i</sub> &in; <var>W</var> because any subspace is closed under scalar multiplication. In order to prove by induction, break the claim
        <section>
          <table>
            <tr>
              <td>Base claim:</td>
              <td>
                a<sub>1</sub>w<sub>1</sub> &in; W
              </td>
            </tr>
            <tr>
              <td>Inductive claim:</td>
              <td>
                &sum; <sub>(i=1,2,&hellip;,n)</sub>
                a<sub>i</sub>w<sub>i</sub> &in; W
              </td>
            </tr>
            <tr>
              <td></td>
              <td>
                &Implies;
                &sum; <sub>(i=1,2,&hellip;,n,n+1)</sub>
                a<sub>i</sub>w<sub>i</sub> &in; W
              </td>
            </tr>
            <tr>
              <td></td>
              <td>
                = {
                  (&sum; <sub>(i=1,2,&hellip;,n)</sub>
                  a<sub>i</sub>w<sub>i</sub>)
                  +
                  (a<sub>n+1</sub>w<sub>n+1</sub>)
                }
                &in; W
              </td>
            </tr>
          </table>
        </section>
        The base claim is already proven. The inductive claim holds because
        <ul>
          <li>
            (&sum; <sub>(i=1,2,&hellip;,n)</sub> a<sub>i</sub>w<sub>i</sub>) is a vector of <var>W</var> by the inductive hypothesis,
          </li>
          <li>
            (a<sub>n+1</sub>w<sub>n+1</sub>) is already proven to be a vector of <var>W</var>, and
          </li>
          <li>
            the inductive conclusion holds because any subspace is closed under addition. &marker;
          </li>
        </ul>
      </p>
    </section>
    <section id="span-in-subspace" class="proposition">
      <p>
        <strong>Theorem (span in subspace)</strong>:
        If <var>W</var> is a subspace of vector space <var>V</var> and <var>S</var> is a subset of <var>V</var>, then
        <section>
          <table>
            <tr>
              <td>
                <var>S</var> &subseteq; <var>W</var> &Implies; span(S) &subseteq; <var>W</var>
              </td>
            </tr>
            <tr>
              <td>
                If <var>W</var> contains <var>S</var>, then <var>W</var> contains span(S)
              </td>
            </tr>
          </table>
        </section>
        Proof:
        <section>
          <p>
            If <var>S</var> = &empty;, span(S = &empty;) = {0}, the zero vector space. The zero vector must be contained in any subspace, it follows span(S) &subseteq; <var>W</var>.
          </p>
          <p>
            If <var>S</var> &ne; &empty;, then a vector <var>w</var> &in; span(S) has the form
            <section>
              w = c<sub>1</sub>w<sub>1</sub> + c<sub>2</sub>w<sub>2</sub> + &hellip; + c<sub>k</sub>w<sub>k</sub>
            </section>
            for some vectors <var>w<sub>1</sub></var>, <var>w<sub>2</sub></var>, &hellip;, <var>w<sub>k</sub></var> &in; <var>S</var> and some scalars <var>c<sub>1</sub></var>, <var>c<sub>2</sub></var>, &hellip;, <var>c<sub>k</sub></var> &in; <var>F</var>.
            Since <var>S</var> &subseteq; <var>W</var>, we have
            <section>
              w<sub>1</sub>, w<sub>2</sub>, &hellip;, w<sub>k</sub> &in; W
            </section>
            Therefore, <var>w</var> &in; <var>W</var> by the lemma (linear combination in subspace). Because <var>w</var>, an arbitrary vector in span(S), belongs to <var>W</var>, it follows that span(S) &subseteq; <var>W</var>.
            &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="span-is-subspace" class="proposition">
      <p>
        <strong>Theorem (span is subspace)</strong>:
        If <var>S</var> is a subset of a vector space <var>V</var>, then
        <section>
          span(S) is a subspace of <var>V</var>, and
          <var>S</var> &subseteq; span(S)
        </section>
        Proof:
        <section>
          <p>
            Prove that span(S) is a subspace of <var>V</var>.
            <section>
              <p>
                If <var>S</var> = &empty;, span(S = &empty;) = {0} &mdash; the zero vector space &mdash; is a subspace of any vector space by the corollary (zero subspace).
              </p>
              <p>
                0 &in; span(S &ne; &empty;):
                <section>
                  <p>
                    Corollary (zero vector is a linear combination) show that the zero vector is a linear combination of any nonempty subset of <var>V</var>.
                  </p>
                </section>
              </p>
              <p>
                span(S &ne; &empty;) is closed under both addition and scalar multiplication:
                <section>
                  <p>
                    Let <var>u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>m</sub></var>, <var>v<sub>1</sub>, v<sub>2</sub>, &hellip;, v<sub>n</sub></var> &in; <var>S</var>, then there exist <var>x, y</var> &in; span(S) such that
                    <section>
                      <table>
                        <tr>
                          <td>
                            x = a<sub>1</sub>u<sub>1</sub>
                              + a<sub>2</sub>u<sub>2</sub>
                              + &hellip;
                              + a<sub>m</sub>u<sub>m</sub>
                          </td>
                        </tr>
                        <tr>
                          <td>
                            y = b<sub>1</sub>v<sub>1</sub>
                              + b<sub>2</sub>v<sub>2</sub>
                              + &hellip;
                              + b<sub>n</sub>v<sub>n</sub>
                          </td>
                        </tr>
                      </table>
                    </section>
                    for scalars <var>a<sub>1</sub>, a<sub>2</sub>, &hellip;, a<sub>m</sub></var>, <var>b<sub>1</sub>, b<sub>2</sub>, &hellip;, b<sub>n</sub></var>.
                    Then
                    <section>
                      x + y
                      = a<sub>1</sub>u<sub>1</sub>
                      + a<sub>2</sub>u<sub>2</sub>
                      + &hellip;
                      + a<sub>m</sub>u<sub>m</sub>
                      + b<sub>1</sub>v<sub>1</sub>
                      + b<sub>2</sub>v<sub>2</sub>
                      + &hellip;
                      + b<sub>n</sub>v<sub>n</sub>
                    </section>
                    is a linear combination in <var>S</var>, that is <var>x</var>+<var>y</var> &in; span(S), which follows that span(S) is closed under addition. And for some scalar <var>c</var>,
                    <section>
                      cx
                      = (ca<sub>1</sub>)u<sub>1</sub>
                      + (ca<sub>2</sub>)u<sub>2</sub>
                      + &hellip;
                      + (ca<sub>m</sub>)u<sub>m</sub>
                    </section>
                    is a linear combination in <var>S</var>, that is <var>cx</var> &in; span(S), which follows that span(S) is closed under scalar multiplication.
                  </p>
                </section>
              </p>
            </section>
          </p>
          <p>
            Prove that <var>S</var> &in; span(S).
            <section>
              <p>
                If <var>S</var> = &empty;, span(S) is a set by definition, and <var>S</var> = &empty; is a subset of every set.
                If <var>S</var> &ne; &empty;, for any <var>v</var> &in; S,
                <section>
                  v = 1v &in; span(S)
                  &marker;
                </section>
              </p>
            </section>
          </p>
        </section>
      </p>
    </section>
    <section id="generate-definition" class="definition">
      <p>
        <strong>Definition (generate)</strong>:
        Let <var>V</var> be a vector space and <var>S</var> a subset of <var>V</var>, then
        <section>
          if span(S) = V, we say the vectors of S generate (or span) V.
        </section>
      </p>
    </section>
    <section id="generate-example3" class="description">
      <p>
        <strong>Example (vectors generate a vector space)</strong>:
        The vectors (1,1,0), (1,0,1), and (0,1,1) generate &reals;<sup>3</sup> since an arbitrary vector (a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>) in &reals;<sup>3</sup> is a linear combination of the three given vectors; in fact, the scalars <var>r</var>, <var>s</var> and <var>t</var> for which
        <section>
          <table>
            <tr>
              <td colspan="3">
                r(1,1,0) + s(1,0,1) + t(0,1,1)
                = (a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>)
              </td>
            </tr>
            <tr>
              <td colspan="3">are</td>
            </tr>
            <tr>
              <td>r = &frac12; (a<sub>1</sub> + a<sub>2</sub> &minus; a<sub>3</sub>),</td>
              <td>s = &frac12; (a<sub>1</sub> &minus; a<sub>2</sub> + a<sub>3</sub>),</td>
              <td>t = &frac12; (&minus;a<sub>1</sub> + a<sub>2</sub> + a<sub>3</sub>)</td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="subspace-and-span" class="proposition">
      <p>
        <strong>Corollary (subspace and span)</strong>:
        <section>
          <p>
            a subset <var>W</var> of a vector space <var>V</var> is a subspace of <var>V</var> &iff; span(W) = <var>W</var>
          </p>
        </section>
        Proof:
        <section>
          <p>
            If a subset <var>W</var> of a vector space <var>V</var> is a subspace of <var>V</var>, then
            <section>
              <table>
                <tr><td>span(W) &subseteq; <var>W</var> by theorem (span in subspace), and</td></tr>
                <tr><td><var>W</var> &subseteq; span(W) by theorem (span is subspace),</td></tr>
                <tr><td>therefore span(W) = <var>W</var>.</td></tr>
              </table>
            </section>
            Now we will show a contradiction if span(W) = <var>W</var> but a subset <var>W</var> of a vector space <var>V</var> is NOT a subspace of <var>V</var>. If <var>W</var> &ne; &empty;, then a vector <var>w</var> &in; span(W) has the form
            <section>
              w = a<sub>1</sub>w<sub>1</sub> + a<sub>2</sub>w<sub>2</sub> + &ctdot; + a<sub>n</sub>w<sub>n</sub>
            </section>
            for some vectors <var>w<sub>1</sub></var>, <var>w<sub>2</sub></var>, &hellip;, <var>w<sub>n</sub></var> &in; <var>W</var> and some scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>. Since span(W) = <var>W</var>,
            <section>
              for all <var>w</var>, <var>w</var> &in; <var>W</var>
            </section>
            but the subset <var>W</var> is NOT closed under addition and scalar multiplication meaning
            <section>
              there exists <var>w</var> such that <var>w</var> &notin; <var>W</var>
            </section>
            which is a contradiction. Thus span(W) = <var>W</var> &Implies; a subset <var>W</var> of a vector space <var>V</var> is a subspace of <var>V</var>. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="subset-and-span" class="proposition">
      <p>
        <strong>Corollary (subset and span)</strong>:
        Let <var>S<sub>1</sub></var> and <var>S<sub>2</sub></var> be subsets of a vector space <var>V</var>, then
        <section>
          <var>S<sub>1</sub></var> &subseteq; <var>S<sub>2</sub></var> &Implies;
          span(S<sub>1</sub>) &subseteq; span(S<sub>2</sub>)
        </section>
        Proof:
        <section>
          <p>
            Let <var>v<sub>1</sub></var>, <var>v<sub>2</sub></var>, &hellip;, <var>v<sub>n</sub></var> &in; <var>S<sub>1</sub></var>, then some linear combination in span(S<sub>1</sub>)
            <section>
              v = a<sub>1</sub>v<sub>1</sub> + a<sub>2</sub>v<sub>2</sub> + &ctdot; + a<sub>n</sub>v<sub>n</sub>
            </section>
            is also a linear combination of <var>S<sub>2</sub></var> because <var>v<sub>1</sub></var>, <var>v<sub>2</sub></var>, &hellip;, <var>v<sub>n</sub></var> &in; <var>S<sub>2</sub></var> as well, thus
            <section>
              span(S<sub>1</sub>) &subset; span(S<sub>2</sub>)
            </section>
            If <var>S<sub>1</sub></var> = <var>S<sub>2</sub></var>, then span(S<sub>1</sub>) = span(S<sub>2</sub>) is immediate by the definition of span. &marker;
          </p>
        </section>
      </p>
    </section>
  </section>
  <section id="linear-dependence">
    <h2>Linear Dependence and Linear Independence</h2>
    <section id="intro-linear-dependence" class="description">
      <p>
        Let <var>W</var> be a subspace of a vector space <var>V</var>. It is desirable to find a small finite subset <var>S</var> of <var>W</var> that generates <var>W</var>. (&hellip;) The search for this subset is related to the question of whether or not some vector in <var>S</var> is a linear combination of the other vectors in <var>S</var>.
      </p>
      <p>
        Rather than asking whether some vector in <var>S</var> is a linear combination of the other vectors in <var>S</var>, it is more efficient to ask whether the zero vector can be expressed as a linear combination of the vectors in <var>S</var> with coefficients that are not all zero.
      </p>
    </section>
    <section id="define-linearly-dependent" class="definition">
      <p>
        <strong>Definition (linearly dependent [線形従属])</strong>:
        A subset <var>S</var> of a vector space <var>V</var> is called linearly dependent if there exist a finite number of distinct vectors <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> &in; <var>S</var> and scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>, not all zero, such that
        <section>
          a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
        </section>
        In other words, for a set to be linearly dependent,
        <section>
          there must exist a nontrivial representation of <var>0</var> (the zero vector) as a linear combination of vectors in the set.
        </section>
      </p>
    </section>
    <section id="trivial-representation" class="description">
      <p>
        <strong>Term (trivial representation)</strong>:
        For any vectors <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var>, we have
        <section>
          a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
        </section>
        if <var>a<sub>1</sub></var> = <var>a<sub>2</sub></var> = &ctdot; = <var>a<sub>n</sub></var> = 0. We call this the trivial representation of <var>0</var> as a linear combination of  <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var>.
      </p>
    </section>
    <section id="zero-vector-linearly-dependent" class="proposition">
      <p>
        <strong>Corollary (zero vector linearly dependent)</strong>:
        Any set that contains the zero vector is linearly dependent.
      </p>
      <p>
        Proof: It is immediate that <var>a0</var> = <var>0</var> for any scalar <var>a</var>. Because it is a nontrivial representation of the zero vector as the sole linear combination of vectors in the set, the set is linearly dependent.
      </p>
    </section>
    <section id="linearly-dependent-sets" class="proposition">
      <p>
        <strong>Theorem (linearly dependent sets)</strong>:
        A set is linearly dependent if only if
        <section>
          at least one of the vectors in the set is expressible as a linear combination of the other vectors in the set.
        </section>
        Proof:
        Let <var>S</var> be a set and <var>W</var> = {u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>n</sub>} &in; <var>S</var>.
        <section>
          <p>
            If <var>S</var> is linearly dependent, then there exist a nontrivial representation of <var>0</var> as a linear combination of vectors in <var>W</var> for some scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>, not all zero, such that
            <section>
              a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
            </section>
            Let <var>a<sub>1</sub></var> be a nonzero scalar, then <var>u<sub>1</sub></var> can be expressed as a linear combination of the other vectors in <var>S</var>:
            <section>
              u<sub>1</sub> = &minus;1 &frasl; a<sub>1</sub> (
                a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
              )
            </section>
            If at least one of the vectors in <var>S</var>, say <var>u<sub>1</sub></var>, is expressible as a linear combination of the other vectors <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</var> for some scalars <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var> such that
            <section>
              u<sub>1</sub> =
              a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
            </section>
            then from the expression, we have a representation of <var>0</var> as a linear combination of vectors in <var>W</var> &subseteq; <var>S</var> such that
            <section>
              0 = (&minus;1)u<sub>1</sub> +
              a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
            </section>
            Because the coefficient of <var>u<sub>1</sub></var> is nonzero, the representation is nontrivial. Thus, <var>S</var> is linear dependent. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="linearly-independent" class="definition">
      <p>
        <strong>Definition (linearly independent [線形独立])</strong>:
        A subset <var>S</var> of a vector space that is not linearly dependent is called linearly independent, that is
        <section>
          <p>
            the only representations of the zero vector as linear combinations of its vectors are trivial representations.
          </p>
        </section>
        For convenience, we define that
        <section>
          the empty set &empty; is linearly independent.
        </section>
      </p>
    </section>
    <section id="single-vector-linearly-independent" class="proposition">
      <p>
        <strong>Corollary (single vector linearly independent)</strong>:
        Consider a set containing only one nonzero vector <var>v</var>. Then {v} is linearly independent.
      </p>
      <p>
        Proof: Since the sole vector <var>v</var> in the set is not zero vector, <var>av</var> = 0 for some scalar <var>a</var> &mdash; the only representation of the zero vector as linear combination of the vector in the set &mdash; holds only when <var>a</var> = 0. Hence the set is linearly independent. &marker;
      </p>
    </section>
    <section id="superset-dependent" class="proposition">
      <p>
        <strong>Theorem (sets and linearly dependent)</strong>:
        Let <var>S<sub>1</sub></var> and <var>S<sub>2</sub></var> be subsets of a vector space <var>V</var> such that S<sub>1</sub> &subseteq; S<sub>2</sub> &subseteq; V, then
        <section>
          <table>
            <tr>
              <td>S<sub>1</sub> is linearly dependent</td>
              <td>&Implies;</td>
              <td>S<sub>2</sub> is linearly dependent</td>
            </tr>
            <tr>
              <td>S<sub>2</sub> is linearly independent</td>
              <td>&Implies;</td>
              <td>S<sub>1</sub> is linearly independent</td>
            </tr>
          </table>
        </section>
        Proof:
        <section>
          <p>
            Since <var>S<sub>1</sub></var> is linearly dependent, there exist a finite number of distinct vectors <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> &in; <var>S<sub>1</sub></var> and scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>, not all zero, such that
            <section>
              a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
            </section>
            Since S<sub>1</sub> &subseteq; S<sub>2</sub>, <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> are also elements of <var>S<sub>2</sub></var>, it follows <var>S<sub>2</sub></var> is linear dependent.
          </p>
          <p>
            The second implication holds because
            <ul>
              <li>
                a subset that is not linearly dependent is linearly independent, and
              </li>
              <li>
                the second implication is the contrapositive of first implication.
                &marker;
              </li>
            </ul>
          </p>
        </section>
      </p>
    </section>
    <section id="the-smallest-generating-set" class="description">
      <p>
        <strong>The smallest generating set</strong>:
        The issue of whether <var>S</var> is the smallest generating set for its span is related to the question of whether <var>S</var> is linearly dependent.
      </p>
      <p>
        Suppose that <var>S</var> is linearly dependent set containing two or more vectors. Then some vector <var>v</var> &in; <var>S</var> can be written as a linear combination of the other vectors in <var>S</var>, and
        <section>
          <table>
            <tr>
              <td>
                <var>S</var> is linearly dependent set, then
              </td>
            </tr>
            <tr>
              <td>
                the subset obtained by removing some <var>v</var> &in; <var>S</var> has the same span as <var>S</var>.
              </td>
            </tr>
          </table>
        </section>
        It follows that
        <section>
          <table>
            <tr>
              <td>
                if no proper subset of <var>S</var> generates the span of <var>S</var>, then
              </td>
            </tr>
            <tr>
              <td>
                <var>S</var> must be linearly independent.
              </td>
            </tr>
          </table>
        </section>
        Another way to view this statement is given in theorem (linearly dependent span).
      </p>
    </section>
    <section id="linearly-dependent-span" class="proposition">
      <p>
        <strong>Theorem (linearly dependent span)</strong>:
        Let <var>V</var> be a vector space and
        <section>
          <table>
            <tr>
              <td colspan="3">
                <var>S</var> &subseteq; <var>V</var>
              </td>
            </tr>
            <tr>
              <td>
                <var>u</var> &notin; <var>S</var>
              </td>
              <td>and</td>
              <td>
                <var>u</var> &in; <var>V</var>
              </td>
            </tr>
            <tr>
              <td colspan="3">
                <var>S</var> be linearly independent
              </td>
            </tr>
          </table>
        </section>
        then
        <section>
          <table>
            <tr>
              <td>
                <var>S</var> &cup; {u} is linearly dependent
              </td>
              <td>
                if and only if
              </td>
              <td>
                <var>u</var> &in; span(S)
              </td>
            </tr>
          </table>
        </section>
        Proof:
        <section>
          <p>
            If <var>S</var> &cup; {u} is linearly dependent, then there are vectors <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> &in; <var>S</var> such that <var>au</var> + <var>a<sub>1</sub>u<sub>1</sub></var> + <var>a<sub>2</sub>u<sub>2</sub></var> + &ctdot; + <var>a<sub>n</sub>u<sub>n</sub></var> = 0 for some nonzero scalars <var>a</var>, <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>, so
            <section>
              u
              = &minus;1 &frasl; a
                ( a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> )
            </section>
            This shows that <var>u</var> is a linear combination of <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> &in; <var>S</var>, so we have <var>u</var> &in; span(S).
          </p>
          <p>
            If <var>u</var> &in; span(S), then there exist vectors <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>m</sub></var> &in; <var>S</var> and scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>m</sub></var> such that <var>u</var> = <var>a<sub>1</sub>u<sub>1</sub></var> + <var>a<sub>2</sub>u<sub>2</sub></var> + &ctdot; <var>a<sub>m</sub>u<sub>m</sub></var>, so we have
            <section>
              a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; a<sub>m</sub>u<sub>m</sub> + (&minus;1)u = 0
            </section>
            Note that because <var>u</var> &notin; <var>S</var>, <var>u</var> &ne; <var>u<sub>i</sub></var> for <var>i</var> = 1, 2, &hellip;, <var>m</var>. Hence the coefficient of <var>u</var> in this linear combination is &minus;1 (i.e., nonzero), the set {u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>m</sub>, v} is linearly dependent.
            <section>
              <table>
                <tr>
                  <td>
                    because
                    {u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>m</sub>}
                    &subseteq; S
                    and u &notin; S
                  </td>
                </tr>
                <tr>
                  <td>
                    {u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>m</sub>, u}
                    &subseteq; S &cup; {u}
                  </td>
                </tr>
              </table>
            </section>
            Therefore <var>S</var> &cup; {u} is linearly dependent by theorem (sets and linearly dependent). &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="scalar-multiple-linearly-dependent" class="proposition">
      <p>
        <strong>Corollary (scalar multiple linearly dependent)</strong>:
        Let <var>u</var> and <var>v</var> be distinct vectors in a vector space <var>V</var>. Then {u, v} is linearly dependent if and only if <var>u</var> and <var>v</var> is a multiple of the other.
      </p>
      <p>
        Proof:
        <section>
          <p>
            If {u, v} is linearly dependent, a nontrivial representation of <var>0</var> as a linear combination for some nonzero scalars <var>a<sub>1</sub></var> and <var>a<sub>2</sub></var> such that
            <section>
              a<sub>1</sub>u + a<sub>2</sub>v = 0
            </section>
            Because the coefficients are not zero, we have
            <section>
              u = (&minus; a<sub>2</sub> &frasl; a<sub>1</sub>) v
            </section>
            Thus, <var>u</var> and <var>v</var> is a multiple of the other.
          </p>
          <p>
            If <var>u</var> and <var>v</var> is a multiple of the other, then we have
            <section>
              <var>u</var> = <var>av</var>
            </section>
            for some scalar <var>a</var>. This equation gives a nontrivial representation of <var>0</var> as a linear combination
            <section>
              0 = (&minus;1)u + av
            </section>
            Thus, {u, v} is linearly dependent. &marker;
          </p>
        </section>
      </p>
    </section>
  </section>
  <section id="bases-and-dimension">
    <h2>Bases and Dimension</h2>
    <section id="define-basis" class="definition">
      <p>
        <strong>Definition (basis [基底])</strong>:
        A basis <var>&Beta;</var> for a vector space <var>V</var> is a linearly independent subset of <var>V</var> that generates <var>V</var>. If <var>&Beta;</var> is a basis for <var>V</var>, we also say that the vectors of <var>&Beta;</var> form a basis for <var>V</var>.
      </p>
    </section>
    <section id="basis-of-zero-vector-space" class="proposition">
      <p>
        <strong>Corollary (basis of zero vector space)</strong>:
        The empty set &empty; is the basis for the zero vector space.
      </p>
      <p>
        Proof:
        Recall that the zero vector space is a set {0}. Its sole element is the zero vector.
        <ul>
          <li>
            The empty set &empty; is linearly independent by definition of linear independence.
          </li>
          <li>
            The empty set &empty; is a subset of {0}.
          <li>
            The empty set &empty; generates {0} &mdash; span(&empty;) = {0} by definition of the zero span.
          </li>
        </ul>
      </p>
    </section>
    <section id="basis-unique-linear-combination" class="proposition">
      <p>
        <strong>Theorem (basis unique linear combination)</strong>:
        Let <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> be distinct vectors in a vector space <var>V</var>, then, <var>&Beta;</var> = { <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> } is a basis for <var>V</var> if and only if
        <section>
          each vector <var>u</var> &in; <var>V</var> can be uniquely expressed as a linear combination of vectors of <var>&Beta;</var>.
        </section>
      </p>
      <p>
        Proof:
        Let <var>V</var> be a vector space generated by <var>&Beta;</var> &mdash; that is, span(&Beta;) = <var>V</var>.
        <section>
          <p>
            Let <var>&Beta;</var> be a basis for <var>V</var>. If <var>u</var> &in; <var>V</var>, then <var>u</var> &in; span(&Beta;) because span(&Beta;) = <var>V</var>. Thus <var>u</var> is a linear combination of the vector of <var>&Beta;</var>. Suppose that
            <section>
              <table>
                <tr>
                  <td>u</td>
                  <td>
                    = a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td>
                    = b<sub>1</sub>u<sub>1</sub> + b<sub>2</sub>u<sub>2</sub> + &ctdot; + b<sub>n</sub>u<sub>n</sub>
                  </td>
                </tr>
              </table>
            </section>
            are two such representations of <var>u</var>. Subtracting the second equation from the first gives
            <section>
              0 = (a<sub>1</sub> &minus; b<sub>1</sub>) u<sub>1</sub> + (a<sub>2</sub> &minus; b<sub>2</sub>) u<sub>2</sub> + &ctdot; + (a<sub>n</sub> &minus; b<sub>n</sub>) u<sub>n</sub>
            </section>
            Since <var>&Beta;</var> is linear independent by definition of the basis, it follows that
            <section>
              (a<sub>1</sub> &minus; b<sub>1</sub>) = (a<sub>2</sub> &minus; b<sub>2</sub>) = &ctdot; = (a<sub>n</sub> &minus; b<sub>n</sub>) = 0
            </section>
            Hence
              a<sub>1</sub> = b<sub>1</sub>, a<sub>2</sub> = b<sub>2</sub>, &hellip;, a<sub>n</sub> = b<sub>n</sub>,
            and so <var>u</var> &in; <var>V</var> can be uniquely expressed as a linear combination of vectors of <var>&Beta;</var>.
          </p>
          <p>
            Conversely let <var>u</var> &in; <var>V</var> is uniquely expressible as a linear combination of vectors of <var>&Beta;</var>. In order to prove by contradiction that <var>&Beta;</var> is a basis for <var>V</var>, suppose that <var>&Beta;</var> is not a basis of <var>V</var>, that is, <var>&Beta;</var> is linearly dependent.
            <ul>
              <li>
                We know from theorem (sets and linearly dependent) that since <var>&Beta;</var> &subseteq; span(&Beta;) = <var>V</var>, the set <var>V</var> is also linearly dependent.
              </li>
              <li>
                Hence from theorem (linearly dependent sets), at least one of the vectors in <var>V</var>, say <var>u</var>, is expressible as a linear combination of the other vectors in <var>V</var> &mdash; a contradiction to the hypothesis of this implication.
              </li>
            </ul>
            Hence <var>&Beta;</var> is linearly independent and a basis of <var>V</var>. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="finite-spanning-set" class="proposition">
      <p>
        <strong>Theorem (finite spanning set)</strong>:
        If a vector space <var>V</var> is generated by a finite set <var>S</var>, then some subset of <var>S</var> is a basis for <var>V</var>. Hence <var>V</var> has a finite basis.
      </p>
      <p>
        Proof:
        <section>
          <p>
            If <var>S</var> = &empty;, then <var>V</var> = span(&empty;) = {0}, and &empty; is a basis of <var>V</var> = {0}, because
            <ul>
              <li>
                &empty; &subseteq; &empty; = <var>S</var>
              </li>
              <li>
                &empty; is linearly independent
              </li>
            </ul>
          </p>
          <p>
            If <var>S</var> = {0}, then <var>V</var> = span{0} = {0}, and &empty; is a basis of <var>V</var> = {0}, because
            <ul>
              <li>
                &empty; &subset; {0} = <var>S</var>
              </li>
              <li>
                &empty; is linearly independent
              </li>
            </ul>
          </p>
          <p>
            If <var>S</var> contains a single nonzero vector, say <var>S</var> = {u}, then <var>V</var> = span{u}, and {u} is a basis for <var>V</var> = span{u}, because
            <ul>
              <li>
                {u} &subseteq; {u} = <var>S</var>
              </li>
              <li>
                {u} is linearly independent by corollary (single vector linearly independent)
              </li>
            </ul>
          </p>
          <p>
            If <var>S</var> contains <var>k</var> vectors such that { u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>k</sub> } is linearly independent, 
          </p>
        </section>
      </p>
    </section>
  </section>
  <section id="reference">
    <h2>References</h2>
    <ul>
      <li>
        Friedberg, S.H., et al. (2016). <i>Linear Algebra, 5th Edition</i>. Pearson.
      </li>
    </ul>
  </section>
</article>
</body>
</html>
