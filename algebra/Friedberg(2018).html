<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>Linear Algebra, Friedberg(2018)</title>
  <style>
    body {
      margin-left: 1.2rem;
    }
    th, td {
      padding-right: 1.2rem;
      vertical-align: text-bottom;
    }
    th {
      text-align: left;
    }
    section, article {
      margin-top: 1rem;
      margin-left: 2rem;
      margin-bottom: 1rem;
    }
    article {
      margin-right: 2rem;
    }
    .definition, .proposition, .description {
      padding-right: 1rem;
      padding-left: 1rem;
    }
    .definition {
      padding-top: 0.2rem;
      padding-bottom: 0.2rem;
      background-color: lightgrey;
    }
    .center {
      text-align: center;
    }
    .right {
      text-align: right;
    }
  </style>
</head>
<body>
<article>
<h1>Linear Algebra [線形代数]</h1>
  <section id="set">
    <h2>Sets (Appendix A)</h2>
    <section id="empty-set" class="definition">
      <p>
        <strong>Definition (empty set)</strong>:
        The empty set, denoted &empty;, is a set containing no elements. &empty; is a subset of every set.
      </p>
    </section>
  </section>
  <section id="fields">
    <h2>Fields (Appendix C)</h2>
    <section id="definition-field" class="definition">
      <p>
        <strong>Definition (field [体])</strong>:
        A field <var>F</var> is a set on which two operations (addition and multiplication) are defined so that for each pair of elements <var>a, b</var> &in; <var>F</var>,
        <ul>
          <li>
            there is a unique element
            <var>a</var>+<var>b</var> &in; <var>F</var>
          </li>
          <li>
            there is a unique element
            <var>a</var>&middot;<var>b</var> &in; <var>F</var>
          </li>
        </ul>
        such that the field axioms hold for all elements <var>a, b, c</var> &in; <var>F</var>.
      </p>
      <p>
        <strong>Axiom (Field axioms [体の公理])</strong>:
        <section id="field-axioms">
          <table>
            <tr>
              <th></th>
              <th></th>
              <th>addition</th>
              <th>multiplication</th>
            </tr>
            <tr>
              <td>交換則</td>
              <td>commutativity</td>
              <td>a+b = b+a</td>
              <td>a&middot;b = b&middot;a</td>
            </tr>
            <tr>
              <td>結合則</td>
              <td>associativity</td>
              <td>(a+b)+c = a+(b+c)</td>
              <td>(a&middot;b)&middot;c = a&middot;(b&middot;c)</td>
            </tr>
            <tr>
              <td>単位元</td>
              <td>existence of identity</td>
              <td>&exist;0 &in; F, 0+a = a</td>
              <td>&exist;1 &in; F, 1&middot;a = a</td>
            </tr>
            <tr>
              <td>逆元</td>
              <td>existence of inverse</td>
              <td>&exist;(&minus;a) &in; F, a+(&minus;a) = 0</td>
              <td>&exist;(a<sup>-1</sup>) &in; F, a&middot;a<sup>-1</sup> = 1 (if a&NotEqual;0)</td>
            </tr>
            <tr>
              <td>分配則</td>
              <td>distributivity of &middot; over +</td>
              <td colspan="2" class="center">
                a&middot;(b+c) = a&middot;b+a&middot;c
              </td>
            </tr>
          </table>
        </section>
        The zero ring can be avoided by including the zero-one law (0 &ne; 1).
      </p>
    </section>
    <section id="cancellation-laws" class="proposition">
      <p>
        <strong>Theorem (cancellation laws [簡約律])</strong>:
        Let <var>F</var> be a field. For all elements <var>a, b, c</var> &in; <var>F</var> and all <var>d</var>&ne;0 &in; <var>F</var>,
        <section>
          <table>
            <tr>
              <td>1.</td>
              <td>a+b = c+b</td>
              <td>&Implies;</td>
              <td>a = c</td>
            </tr>
            <tr>
              <td>2.</td>
              <td>a&middot;d = c&middot;d</td>
              <td>&Implies;</td>
              <td>a = c</td>
            </tr>
          </table>
        </section>
        Proof:
        <ol>
          <li>
            There exists an element <var>d</var> &in; <var>F</var> such that <var>b</var>+<var>d</var> = <var>0</var>.
            <section>
              <table>
                <tr>
                  <td>
                    (a+b)+d = (c+b)+d
                  </td>
                  <td>by adding <var>d</var> to both sides</td>
                </tr>
                <tr>
                  <td>
                    a+{b+d} = c+{b+d}
                  </td>
                  <td>associativity of addition</td>
                </tr>
                <tr>
                  <td>a+0 = c+0</td>
                  <td>&because; b+d = 0</td>
                </tr>
                <tr>
                  <td>a = c</td>
                  <td>existence of additive identity</td>
                </tr>
              </table>
            </section>
          </li>
          <li>
            If <var>d</var> &NotEqual; <var>0</var>, there exists an element <var>e</var> such that <var>d</var>&middot;<var>e</var> = <var>1</var>.
            <section>
              <table>
                <tr>
                  <td>
                    (a&middot;d)&middot;e
                    =
                    (c&middot;d)&middot;e
                  </td>
                  <td>by multiplying both sides by <var>e</td>
                </tr>
                <tr>
                  <td>
                    a&middot;(d&middot;e)
                    =
                    c&middot;(d&middot;e)
                  </td>
                  <td>associativity of multiplication</td>
                </tr>
                <tr>
                  <td>a&middot;1 = c&middot;1</td>
                  <td>&because; d&middot;e = 1</td>
                </tr>
                <tr>
                  <td>a = c</td>
                  <td>
                    existence of multiplicative identity
                    &marker;
                  </td>
                </tr>
              </table>
            </section>
          </li>
        </ol>
      </p>
    </section>
    <section id="unique-identity-inverse" class="proposition">
      <p>
        <strong>Corollary (unique identities and inverses)</strong>:
        Let <var>F</var> be a field and <var>a</var> &in; <var>F</var>, then
        <ul>
          <li>Additive identity (0) is unique.</li>
          <li>Additive inverse (<var>&minus;a</var>) is unique.</li>
          <li>Multiplicative identity (1) is unique.</li>
          <li>
            Multiplicative inverse (<var>a<sup>-1</sup></var> if <var>a</var>  &NotEqual; 0) is unique.
          </li>
        </ul>
        Proof:
        <ul>
          <li>
            Additive identity (0): Suppose that 0&prime; &in; <var>F</var> is an additive identity.
            <section>
              <table>
                <tr>
                  <td>0&prime; + a</td>
                  <td>= a</td>
                  <td>&because; 0&prime; is an additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0 + a</td>
                  <td>&because; 0 is an additive identity</td>
                </tr>
                <tr>
                  <td>0&prime;</td>
                  <td>= 0</td>
                  <td>additive cancellation law</td>
              </table>
            </section>
          </li>
          <li>
            Other three proofs are similar. &marker;
          </li>
        </ul>
      </p>
    </section>
    <section id="substraction-division" class="definition">
      <p>
        <strong>Definition (Subtraction and Division)</strong>: Subtraction (&minus;) and division (&divide; or &frasl;) can be defined in terms of addition and multiplication by using inverses. Let <var>F</var> be a field. For all <var>a</var> &in; <var>F</var> and all <var>b</var> &NotEqual; 0 &in; <var>F</var>,
        <section>
          <table>
            <tr>
              <td>a&minus;b = a+(&minus;b)</td>
            </tr>
            <tr>
              <td>
                a&divide;b
                = <sup>a</sup>&frasl;<sub>b</sub>
                = a&middot;b<sup>-1</sup>
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="multiplications-in-fields" class="proposition">
      <p>
        <strong>Corollary (multiplications in fields)</strong>:
        Let <var>F</var> be a field. For all <var>a, b</var> &in; <var>F</var>,
        <ol>
          <li>a&middot;0 = 0</li>
          <li>
            (&minus;a)&middot;b = a&middot;(&minus;b) = &minus;(a&middot;b)
          </li>
          <li>(&minus;a)&middot;(&minus;b) = a&middot;b</li>
        </ol>
        Proof:
        <ol>
          <li>
            a&middot;0 = 0
            <section>
              <table>
                <tr>
                  <td>a&middot;0+a&middot;0</td>
                  <td>= a&middot;(0+0)</td>
                  <td>distributivity of &middot; over +</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a&middot;0</td>
                  <td>&because; 0+0 = 0</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a&middot;0+0</td>
                  <td>existence of additive identity</td>
                </tr>
                <tr>
                  <td>a&middot;0</td>
                  <td>= 0</td>
                  <td>additive cancellation law</td>
                </tr>
              </table>
            </section>
          </li>
          <li>
            (&minus;a)&middot;b = &minus;(a&middot;b)
            <section>
              <table>
                <tr>
                  <td>a&middot;b+(&minus;a)&middot;b</td>
                  <td>= {a+(&minus;a)}&middot;b</td>
                  <td>distributivity of &middot; over +</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0&middot;b</td>
                  <td>existence of additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0</td>
                  <td>this corollary 1</td>
                </tr>
              </table>
            </section>
          </li>
          <li>
            (&minus;a)&middot;(&minus;b) = a&middot;b
            <section>
              <table>
                <tr>
                  <td>(&minus;a)&middot;(&minus;b)</td>
                  <td>= &minus;{a&middot;(&minus;b)}</td>
                  <td>by applying (2)</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= &minus;{&minus;(a&middot;b)}</td>
                  <td>by applying (2)</td>
                </tr>
                <tr>
                  <td></td>
                  <td>
                    = a&middot;b
                    &marker;
                  </td>
                </tr>
              </table>
            </section>
          </li>
        </ol>
      </p>
    </section>
    <section id="zero-ring" class="description">
      <p>
        <strong>Zero ring</strong>:
        In Ring theory, the zero ring is a set consisting of a single element <var>0</var> with the operations + and &middot; defined such that 0+0 = 0 and 0&middot;0 = 0. [Wikipedia: Zero ring].
      </p>
    </section>
    <section id="if-isentities-equal" class="proposition">
      <p>
        <strong>Proposition (zero ring characteristics)</strong>:
        If the additive identity <var>0</var> equals the multiplicative identity <var>1</var>, then the ring <var>R</var> has only a single element <var>0</var> = <var>1</var>.
      </p>
      <p>
        Proof: for all <var>r</var> &in; <var>R</var>,
        <section>
          <table>
            <tr>
              <td>r</td>
              <td>= 1r</td>
              <td>the multiplicative identity 1</td>
            </tr>
            <tr>
              <td></td>
              <td>= 0r</td>
              <td>the hypothesis</td>
            </tr>
            <tr>
              <td></td>
              <td>= 0</td>
              <td>
                Corollary (multiplications in fields) #1
                &marker;
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
  </section>
  <section id="vector-spaces">
    <h2>Vector Spaces</h2>
    <section id="define-vector-space" class="definition">
      <p>
        <strong>Definition (vector space [ベクトル空間])</strong>:
        A vector space <var>V</var> over a field <var>F</var> consists of a set on which two operations (vector addition and scalar multiplication [スカラー倍]) are defined so that
        <ul>
          <li>
            there is a unique element <var>x+y</var> &in; <var>V</var>
          </li>
          <li>
            there is a unique element <var>ax</var> &in; <var>V</var>
          </li>
        </ul>
        such that the vector space axioms hold for each <var>a, b</var> &in; <var>F</var> and for each <var>x, y, z</var> &in; <var>V</var>.
      </p>
      <p>
        <strong>Axiom (vector space axioms)</strong>
        <section>
          <table>
            <tr>
              <th></th>
              <th>vector addition</th>
              <th>scalar multiplication</th>
            </tr>
            <tr>
              <td>commutativity</td>
              <td>x+y = y+x</td>
              <td></td>
            </tr>
            <tr>
              <td>associativity</td>
              <td>(x+y)+z = x+(y+z)</td>
              <td>(ab)x = a(bx)</td>
            </tr>
            <tr>
              <td>existence of identity</td>
              <td>&exist;0 &in; V, x+0 = x</td>
              <td>1x = x</td>
            </tr>
            <tr>
              <td>existence of inverse</td>
              <td>&exist;(&minus;x) &in; V, x+(&minus;x) = 0</td>
              <td></td>
            </tr>
            <tr>
              <td>distributivity</td>
              <td>a(x+y) = ax+ay</td>
              <td>(a+b)x=ax+bx</td>
            </tr>
          </table>
        </section>
        The additive identity, denoted <var>0</var>, is called the zero vector.
      </p>
    </section>
    <section id="zero-in-vector-space" class="proposition">
      <p>
        Existence of vector additive identity of the vector space axioms tells:
        <ul>
          <li>any vector space must contain <var>0</var></li>
          <li>&empty; is not a vector space</li>
        </ul>
      </p>
    </section>
    <section id="cancellation-law-vector" class="proposition">
      <p>
        <strong>Theorem 1.1 (cancellation law for vector addition)</strong>:
        Let <var>V</var> be a vector space, and <var>x, y, z</var> &in; <var>V</var>, then
        <section>
          <table>
            <tr>
              <td>x+z = y+z</td>
              <td>&Implies;</td>
              <td>x = y</td>
            </tr>
          </table>
        </section>
        Proof:
        By the vector space axiom, we know that there exists an additive inverse, say <var>u</var>, of the vector <var>z</var> such that
        <section>
          <section>
            z+u = 0
          </section>
          Thus
          <section>
            <table>
              <tr>
                <td>x</td>
                <td>= x+0</td>
                <td><var>0</var> is the additive identity</td>
              </tr>
              <tr>
                <td></td>
                <td>= x+(z+u)</td>
                <td>the additive inverse <var>u</var> of <var>z</var></td>
              </tr>
              <tr>
                <td></td>
                <td>= (x+z)+u</td>
                <td>by additive associativity</td>
              </tr>
              <tr>
                <td></td>
                <td>= (y+z)+u</td>
                <td>by the hypothesis</td>
              </tr>
              <tr>
                <td></td>
                <td>= y+(z+u)</td>
                <td>by additive associativity</td>
              </tr>
              <tr>
                <td></td>
                <td>= y+0</td>
                <td>the additive inverse <var>u</var> of <var>z</var></td>
              </tr>
              <tr>
                <td></td>
                <td>= y</td>
                <td><var>0</var> is the additive identity &marker;</td>
              </tr>
            </table>
          </section>
        </section>
      </p>
    </section>
    <section id="unique-zero-vector" class="proposition">
      <p>
        <strong>Corollary (1: unique zero vector)</strong>:
        The zero vector is unique.
      </p>
      <p>
        Proof:
        Let <var>V</var> be a vector space, and <var>x &in; V</var>. Suppose that <var>0&prime; &in; V</var> is an zero vector, too. Then,
        <section>
          <section>
            <table>
              <tr>
                <td>0&prime; + x</td>
                <td>= x</td>
                <td>&because; 0&prime; is an zero vector</td>
              </tr>
              <tr>
                <td></td>
                <td>= 0 + x</td>
                <td>&because; 0 is an zero vector</td>
              </tr>
              <tr>
                <td>0&prime;</td>
                <td>= 0</td>
                <td>
                  the cancellation law
                  &marker;
                </td>
              </tr>
            </table>
          </section>
        </section>
      </p>
    </section>
    <section id="unique-additive-inverse" class="proposition">
      <p>
        <strong>Corollary (2: unique additive inverse)</strong>:
        The additive inverse vector is unique.
      </p>
      <p>
        Proof:
        Suppose that for a vector <var>x</var>, there exist two distinct additive inverses. Since the zero vector is unique,
        <section>
          <section>
            0 = x+(&minus;x) = x+(&minus;x&prime;)
          </section>
          Thus &minus;x = &minus;x&prime; by cancellation law for vector addition, it follows that the additive inverse is unique.
        </section>
      </p>
    </section>
    <section id="theorem-scalar-multiplications" class="proposition">
      <p>
        <strong>Theorem 1.2 (scalar multiplications in vector spaces)</strong>:
        In any vector space <var>V</var> over a field <var>F</var>, the following statements are true:  for each <var>a</var> &in; <var>F</var> and for each <var>x</var> &in; <var>V</var>,
        <section>
          <table>
            <tr>
              <td>[1]</td>
              <td>0x = 0</td>
            </tr>
            <tr>
              <td>[2]</td>
              <td>
                (&minus;a)x
                = &minus;(ax)
              </td>
            </tr>
            <tr>
              <td>[3]</td>
              <td>
                a(&minus;x)
                = &minus;(ax)
              </td>
            </tr>
            <tr>
              <td>[4]</td>
              <td>a0 = 0</td>
            </tr>
          </table>
        </section>
        Proof:
        <section>
          <p>
            [1]: By the vector space axioms and the cancellation law for vector addition,
            <section>
              <table>
                <tr>
                  <td>0x+0x</td>
                  <td>= (0+0)x</td>
                  <td>by scalar multiplicative distributivity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0x</td>
                  <td>the field additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0x+0</td>
                  <td>the vector additive identity</td>
                </tr>
                <tr>
                  <td colspan="2" class="center">
                    0x = 0
                  </td>
                  <td>by the cancellation law for vector addition</td>
                </tr>
              </table>
            </section>
          </p>
          <p>
            [2]: By the field axioms, we know that there exists an additive inverse for some scalar <var>a</var>, denoted &minus;<var>a</var>,
            <section>
              <table>
                <tr>
                  <td>ax+(&minus;a)x</td>
                  <td>= {a+(&minus;a)}x</td>
                  <td>by scalar multiplicative distributivity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0x</td>
                  <td>
                    the field additive inverse
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td>= 0</td>
                  <td>by [1]</td>
                </tr>
              </table>
            </section>
            But by the vector space axioms, we know that there exists an additive inverse for some vector <var>ax</var>, denoted &minus;(ax) such that
            <section>
              ax+{&minus;(ax)} = 0
            </section>
            Because the additive inverse is unique, (&minus;a)x = &minus;(ax).
          </p>
          <p>
            [3]: By the field axioms, the vector space axioms and [2],
            <section>
              <table>
                <tr>
                  <td>a(&minus;x)</td>
                  <td>= a{(&minus;1)x}</td>
                  <td>
                    &because; (&minus;1)x = &minus;x by [2]
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td>= {a(&minus;1)}x</td>
                  <td>by scalar multiplicative associativity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= (&minus;a)x</td>
                  <td>by field multiplication</td>
                </tr>
              </table>
            </section>
          </p>
          <p>
            [4]: By the vector space axioms and the cancellation low for vector addition,
            <section>
              <table>
                <tr>
                  <td>a0+a0</td>
                  <td>= a(0+0)</td>
                  <td>by vector additive associativity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a0</td>
                  <td>the vector additive identity</td>
                </tr>
                <tr>
                  <td></td>
                  <td>= a0+0</td>
                  <td>the vector additive identity</td>
                </tr>
                <tr>
                  <td colspan="2" class="center">a0 = 0</td>
                  <td>by the cancellation low for vector addition</td>
                </tr>
              </table>
            </section>
          </p>
        </section>
      </p>
    </section>
    <section id="zero-vector-space" class="proposition">
      <p>
        <strong>Proposition (exercise 1.2.11: zero vector space)</strong>:
        {0} consists of a single zero vector. {0} is a vector space over a field <var>F</var> (called the zero vector space).
      </p>
      <p>
        Proof: For any scalar <var>c</var> in <var>F</var>,
        <section>
          <section>
            <table>
              <tr>
                <td>0+0 = 0 &in; {0}</td>
                <td>closed under vector addition</td>
              </tr>
              <tr>
                <td>c0 = 0 &in; {0}</td>
                <td>closed under scalar multiplication</td>
              </tr>
            </table>
          </section>
          And it is immediate that the vector space axioms hold for {0}. &marker;
        </section>
      </p>
    </section>
  </section>
  <section id="subspaces">
    <h2>Subspaces</h2>
    <section id="define-subspace" class="definition">
      <p>
        <strong>Definition (subspace 部分空間)</strong>:
        A subset of a vector space <var>V</var> over a field <var>F</var> is called a subspace of <var>V</var> if the subset is a vector space over <var>F</var> with the operations of addition and scalar multiplication defined on <var>V</var>.
      </p>
    </section>
    <section id="subspace-conditions" class="proposition">
      <p>
        <strong>Theorem 1.3 (subspace conditions)</strong>:
        Let <var>W</var> be a subset of a vector space <var>V</var>, then        <var>W</var> is a subspace of <var>V</var> if and only if the three conditions hold for the operations defined in <var>V</var>. For some <var>x</var>, <var>y</var> &in; <var>W</var> and some <var>c</var> &in; <var>F</var>,
        <section>
          <table>
            <tr>
              <td colspan="2"></td>
              <td>any subspace must</td>
            </tr>
            <tr>
              <td>1:</td>
              <td>0 &in; W</td>
              <td>
                contain the zero vector
              </td>
            </tr>
            <tr>
              <td>2:</td>
              <td>x+y &in; W</td>
              <td>
                be closed under vector addition
              </td>
            </tr>
            <tr>
              <td>3:</td>
              <td>cx &in; W</td>
              <td>
                be closed under scalar multiplication
              </td>
            </tr>
          </table>
        </section>
      </p>
      <p>
        Proof:
        If <var>W</var> is a subspace of <var>V</var>, then
        <section>
          <ul>
            <li>
              Let 0&prime; be an additive identity in <var>W</var> such that <var>x</var>+0&prime; = <var>x</var> for each <var>x</var> &in; <var>W</var>. But also <var>x</var>+0 = <var>x</var> in the vector space <var>V</var>, thus 0&prime; = 0 by the cancellation law for vector addition. Hence 0 &in; <var>W</var>.
            </li>
            <li>
              <var>W</var> is a vector space with addition and scalar multiplication defined on <var>V</var> by definition of the subspace, hence <var>W</var> is closed under addition and scalar multiplication.
            </li>
          </ul>
          <p>
            Conversely, if the three conditions hold, then the vectors in <var>W</var> need to follow the definition of the vector space.
            <section>
              <table>
                <tr>
                  <th></th>
                  <th>vector addition</th>
                  <th>scalar multiplication</th>
                </tr>
                <tr>
                  <td>closed operations</td>
                  <td>x+y &in; W</td>
                  <td>cx &in; W</td>
                </tr>
                <tr>
                  <td>commutativity</td>
                  <td>x+y = y+x</td>
                  <td></td>
                </tr>
                <tr>
                  <td>associativity</td>
                  <td>(x+y)+z = x+(y+z)</td>
                  <td>(ab)x = a(bx)</td>
                </tr>
                <tr>
                  <td>existence of identity</td>
                  <td>&exist;0 &in; W, x+0 = x</td>
                  <td>1x = x</td>
                </tr>
                <tr>
                  <td>existence of inverse</td>
                  <td>&exist;(&minus;x) &in; W, x+(&minus;x) = 0</td>
                  <td></td>
                </tr>
                <tr>
                  <td>distributivity</td>
                  <td>a(x+y) = ax+ay</td>
                  <td>(a+b)x=ax+bx</td>
                </tr>
              </table>
            </section>
            The properties except for
            <section>
              <table>
                <tr>
                  <td>closed under vector addition</td>
                  <td>x+y &in; W</td>
                </tr>
                <tr>
                  <td>closed under scalar multiplication</td>
                  <td>cx &in; W</td>
                </tr>
                <tr>
                  <td>existence of the additive identity</td>
                  <td>&exist;0 &in; W, x+0 = x</td>
                </tr>
                <tr>
                  <td>existence of the additive inverse</td>
                  <td>&exist;(&minus;x) &in; W, x+(&minus;x) = 0</td>
                </tr>
              </table>
            </section>
            hold for all vectors in the vector space, because these properties automatically hold for the vectors in any subspace. Since the hypothesis covers the first three properties, it suffices to prove that the additive inverse of each vector in <var>W</var> lies in <var>W</var>. But if <var>x</var> &in; <var>W</var>, then
            <section>
              <table>
                <tr>
                  <td>(&minus;1)x &in; W</td>
                  <td>
                    by the third condition <var>cx</var> &in; <var>W</var>
                  </td>
                </tr>
                <tr>
                  <td>(&minus;1)x = &minus;x</td>
                  <td>
                    by Theorem (scalar multiplications in vector spaces)
                  </td>
                </tr>
              </table>
            </section>
            Thus &minus;x &in; <var>W</var>, therefore <var>W</var> is a subspace of <var>V</var>. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="vector-space-is-subspace" class="proposition">
      <p>
        <strong>Proposition (vector space is subspace)</strong>:
        <var>V</var> is a subspace of <var>V</var>, because
        <ul>
          <li>0 &in; V</li>
          <li>
            <var>V</var> is a vector space with the operations defined on <var>V</var>.
          </li>
        </ul>
      </p>
    </section>
    <section id="zero-subspace" class="proposition">
      <p>
        <strong>Proposition (zero subspace)</strong>:
        {0} is a subspace of any vector space, because
        <ul>
          <li>
            {0} is a subset of any vector space, since any vector space contains <var>0</var>,
          </li>
          <li>
            we know from Proposition (zero vector space) that {0} is a vector space.
          </li>
        </ul>
    </section>
    <section id="sum-of-subspaces" class="definition">
      <p>
        <strong>Definition (sum of subsets)</strong>:
        The sum of non-empty subsets <var>S<sub>1</sub></var> and <var>S<sub>2</sub></var> of a vector space <var>V</var>, notated <var>S<sub>1</sub></var> + <var>S<sub>2</sub></var>, is the set
        <section>
          {
            <var>x</var> + <var>y</var> :
            <var>x</var> &in; <var>S<sub>1</sub></var>,
            <var>y</var> &in; <var>S<sub>2</sub></var>
          }
        </section>
      </p>
      <p>
        <strong>Definition (direct sum [直和])</strong>:
        The direct sum of <var>W<sub>1</sub></var> and <var>W<sub>2</sub></var>
        <section>
          <var>V</var>
          = <var>W<sub>1</sub></var> &CirclePlus; <var>W<sub>2</sub></var>
        </section>
        is a vector space <var>V</var> if <var>W<sub>1</sub></var> and <var>W<sub>2</sub></var> are subspaces of <var>V</var> such that
        <section>
          <table>
            <tr>
              <td>
                <var>W<sub>1</sub></var> &cap; <var>W<sub>2</sub></var>
                = &empty;
              </td>
            </tr>
            <tr>
              <td>
                <var>W<sub>1</sub></var> &plus; <var>W<sub>2</sub></var>
                = <var>V</var>
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
  </section>
  <section id="linear-combination-equation">
    <h2>Linear Combinations</h2>
    <section id="linear-combination" class="definition">
      <p>
        <strong>Definition (linear combination [線形結合])</strong>:
        Let <var>S</var> be a set of vectors
        {<var>v<sub>1</sub></var>, <var>v<sub>2</sub></var>, &hellip;, <var>v<sub>n</sub></var>} in a vector space <var>V</var>.
        Any vector of the form
        <section>
          <var>v</var>
          = <var>a<sub>1</sub>v<sub>1</sub></var>
          + <var>a<sub>2</sub>v<sub>2</sub></var>
          + &ctdot;
          + <var>a<sub>n</sub>v<sub>n</sub></var>
        </section>
        for some scalars
        <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>,
        is called a linear combination of <var>S</var>.
      </p>
    </section>
    <section id="zero-vector-linear" class="proposition">
      <p>
        <strong>Proposition (zero vector is a linear combination)</strong>:
        The zero vector is a linear combination of any nonempty subset of <var>V</var>.
      </p>
      <p>
        Proof:
        Observe that for each <var>v</var> &in; <var>V</var>, 0<var>v</var> = 0
        &marker;
      </p>
    </section>
    <section id="linear-combination-in-subspace" class="proposition">
      <p>
        <strong>Proposition (exercise 1.3.20: linear combination in subspace)</strong>:
        If <var>W</var> is a subspace of a vector space <var>V</var> and <var>w<sub>1</sub></var>, <var>w<sub>2</sub></var>, &hellip;, <var>w<sub>n</sub></var> are in <var>W</var>, then for any scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>
        <section>
          a<sub>1</sub>w<sub>1</sub> + a<sub>2</sub>w<sub>2</sub> + &hellip; + a<sub>n</sub>w<sub>n</sub> &in; W
        </section>
        Proof:
        For every <var>i</var> &in; &naturals;,
        <section>
          <section>
            a<sub>i</sub>w<sub>i</sub> &in; <var>W</var>
          </section>
          because any subspace is closed under scalar multiplication. So it suffices to prove that
          <section>
            &sum; <sub>(i=1,2,&hellip;,n)</sub>
            a<sub>i</sub>w<sub>i</sub> &in; W
          </section>
          In order to prove it by induction, break the claim
          <section>
            <table>
              <tr>
                <td>Base claim:</td>
                <td>
                  a<sub>1</sub>w<sub>1</sub> &in; W
                </td>
              </tr>
              <tr>
                <td>Inductive claim:</td>
                <td>
                  if &sum; <sub>(i=1,2,&hellip;,n)</sub>
                  a<sub>i</sub>w<sub>i</sub> &in; W
                </td>
              </tr>
              <tr>
                <td></td>
                <td>
                  then
                  &sum; <sub>(i=1,2,&hellip;,n,n+1)</sub>
                  a<sub>i</sub>w<sub>i</sub> &in; W
                </td>
              </tr>
              <tr>
                <td></td>
                <td>
                  = {
                    (&sum; <sub>(i=1,2,&hellip;,n)</sub>
                    a<sub>i</sub>w<sub>i</sub>)
                    +
                    (a<sub>n+1</sub>w<sub>n+1</sub>)
                  }
                  &in; W
                </td>
              </tr>
            </table>
          </section>
          The base claim is already proven. The inductive claim holds because
          <ul>
            <li>
              (&sum; <sub>(i=1,2,&hellip;,n)</sub> a<sub>i</sub>w<sub>i</sub>) is a vector of <var>W</var> by the inductive hypothesis,
            </li>
            <li>
              (a<sub>n+1</sub>w<sub>n+1</sub>) is already proven to be a vector of <var>W</var>, and
            </li>
            <li>
              the inductive conclusion holds because any subspace is closed under addition. &marker;
            </li>
          </ul>
        </section>
      </p>
    </section>
    <section id="span" class="definition">
      <p>
        <strong>Definition (span [線形包])</strong>:
        The span of a subset <var>S</var> of a vector space is the set consisting of all linear combinations of vectors <var>v<sub>1</sub></var>, <var>v<sub>2</sub></var>, &hellip;, <var></var>v<sub>n</sub></var> &in; <var>S</var>, denoted
        <section>
          <table>
            <tr>
              <td>span(S)</td>
              <td>or</td>
              <td>span({v<sub>1</sub>, v<sub>2</sub>, &hellip;, v<sub>n</sub>})</td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="zero-span" class="description">
      <p>
        <strong>Zero span</strong>:
        For convenience, we define
        <section>
          span(&empty;) = {0} &mdash; the zero vector space
        </section>
      </p>
    </section>
    <section id="spanning-set" class="description">
      <p>
        <strong>Spanning sets</strong>:
        If span(S) = <var>V</var>, then
        <section>
          <var>S</var> is called a spanning set for <var>V</var>.
        </section>
      </p>
    </section>
    <section id="span-contains-spanning-set" class="proposition">
      <p>
        <strong>Lemma (1.5.1: span contains spanning set)</strong>:
        Let <var>S</var> be a subset of a vector space <var>V</var>, then
        <section>
          span(S) is a subspace of <var>V</var> such that <var>S</var> &subseteq; span(S).
        </section>
      </p>
      <p>
        Proof:
        <section>
          <p>
            If <var>S</var> = &empty;, then span(S) = {0} by definition, which is a subspace of any vector space by Proposition (zero subspace). It is immediate that &empty; &in; span(S) = {0}.
          </p>
          <p>
            If <var>S</var> = {0}, then it is immediate that span(S) = {0}, which is a subspace of any vector space by Proposition (zero subspace). <var>S</var> &subseteq; span(S) holds because {0} = span(S).
          </p>
          <p>
            If <var>S</var> contains two or more vectors, we prove that subspace conditions hold for span(S). Let <var>x</var>, <var>y</var> &in; span(S).
            <section>
              <p>
                <var>0</var> &in; span(S) holds.
                <section>
                  0x is a linear combination that is, 0x &in; span(S). Since 0x = <var>0</var>, <var>0</var> &in; span(S).
                </section>
              </p>
              <p>
                <var>x</var>+<var>y</var> &in; span(S) and <var>cx</var> &in; span(S) for any scalar c hold.
                <section>
                  Since <var>S</var> &ne; &empty;, there exist vectors <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>m</sub></var>, <var>v<sub>1</sub></var>, <var>v<sub>2</sub></var>, &hellip;, <var>v<sub>n</sub></var> &in; <var>S</var> such that
                  <section>
                    <table>
                      <tr>
                        <td>
                          x = a<sub>1</sub>u<sub>1</sub>
                            + a<sub>2</sub>u<sub>2</sub> + &ctdot;
                            + a<sub>m</sub>u<sub>m</sub>
                        </td>
                        <td>and</td>
                        <td>
                          y = b<sub>1</sub>v<sub>1</sub>
                            + b<sub>2</sub>v<sub>2</sub> + &ctdot;
                            + b<sub>n</sub>v<sub>n</sub>
                        </td>
                      </tr>
                    </table>
                  </section>
                  for scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>m</sub></var>, <var>b<sub>1</sub></var>, <var>b<sub>2</sub></var>, &hellip;, <var>b<sub>n</sub></var>. Then
                  <section>
                    <table>
                      <tr>
                        <td>
                          x + y
                          = a<sub>1</sub>u<sub>1</sub>
                          + a<sub>2</sub>u<sub>2</sub>
                          + &ctdot; + a<sub>m</sub>u<sub>m</sub>
                          + b<sub>1</sub>v<sub>1</sub>
                          + b<sub>2</sub>v<sub>2</sub>
                          + &ctdot; + b<sub>n</sub>v<sub>n</sub>
                        </td>
                      </tr>
                      <tr>
                        <td>
                          cx
                          = (ca<sub>1</sub>)u<sub>1</sub>
                          + (ca<sub>2</sub>)u<sub>2</sub> + &ctdot;
                          + (ca<sub>m</sub>)u<sub>m</sub>
                        </td>
                      </tr>
                    </table>
                  </section>
                  are linear combinations of the vectors in <var>S</var>, that is <var>x</var>+<var>y</var> &in; span(S) and <var>cx</var> &in; span(S). Thus span(S) is closed under vector addition and scalar multiplication.
                </section>
              </p>
            </section>
            When <var>S</var> contains two or more vectors, u = 1u &in; span(S) for any <var>u</var> &in; <var>S</var>, it follows S &subseteq; span(S). &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="spanning-set-subspace-contains-span" class="proposition">
      <p>
        <strong>Lemma (1.5.2: spanning set contains span)</strong>:
        Let <var>S</var> be a subset of a vector space <var>V</var>, and let <var>W</var> be a subspace of <var>V</var>, then
        <section>
          If <var>S</var> &subseteq; <var>W</var>, then
          span(S) &subseteq; <var>W</var>.
        </section>
        Proof:
        <section>
          <p>
            If <var>S</var> = &empty;, then it is immediate that &empty; &subset; <var>W</var> and that span(&empty;) = {0} &subseteq; <var>W</var> since any subspace contains the zero vector by Theorem (subspace conditions).
          </p>
          <p>
            If <var>S</var> = {0}, then it is immediate similarly that {0} &subseteq; <var>W</var> and span({0}) = {0} &subseteq; <var>W</var>.
          </p>
          <p>
            If <var>S</var> contains two or more vectors, then let <var>W</var> denote any subspace of <var>V</var> such that <var>S</var> &subseteq; <var>W</var>. If <var>w</var> &in; span(S), then <var>w</var> has the form
            <section>
              w = c<sub>1</sub>w<sub>1</sub> + c<sub>2</sub>w<sub>2</sub> + &hellip; + c<sub>k</sub>w<sub>k</sub>
            </section>
            for some vectors w<sub>1</sub>, w<sub>2</sub>, &hellip;, w<sub>k</sub> and some scalars <var>c<sub>1</sub></var>, <var>c<sub>2</sub></var>, &hellip;, <var>c<sub>k</sub></var> &in; <var>F</var>.
            Since <var>S</var> &subseteq; <var>W</var>, we have
            <section>
              w<sub>1</sub>, w<sub>2</sub>, &hellip;, w<sub>k</sub> &in; W
            </section>
            Therefore, <var>w</var> &in; <var>W</var> by Lemma (linear combination in subspace), it follows that span(S) &subseteq; <var>W</var>. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="generate-definition" class="definition">
      <p>
        <strong>Definition (generate)</strong>:
        Let <var>V</var> be a vector space and <var>S</var> a subset of <var>V</var>, then
        <section>
          if span(S) = V, we say the vectors of S generate (or span) V.
        </section>
      </p>
    </section>
    <section id="generate-example3" class="description">
      <p>
        <strong>Example (vectors generate a vector space)</strong>:
        The vectors (1,1,0), (1,0,1), and (0,1,1) generate &reals;<sup>3</sup> since an arbitrary vector (a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>) in &reals;<sup>3</sup> is a linear combination of the three given vectors; in fact, the scalars <var>r</var>, <var>s</var> and <var>t</var> for which
        <section>
          <table>
            <tr>
              <td colspan="3">
                r(1,1,0) + s(1,0,1) + t(0,1,1)
                = (a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>)
              </td>
            </tr>
            <tr>
              <td colspan="3">are</td>
            </tr>
            <tr>
              <td>r = &frac12; (a<sub>1</sub> + a<sub>2</sub> &minus; a<sub>3</sub>),</td>
              <td>s = &frac12; (a<sub>1</sub> &minus; a<sub>2</sub> + a<sub>3</sub>),</td>
              <td>t = &frac12; (&minus;a<sub>1</sub> + a<sub>2</sub> + a<sub>3</sub>)</td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="spanning-set-spans-itself" class="proposition">
      <p>
        <strong>
          Proposition (exercise 1.4.12: spanning set spans itself)
        </strong>:
        A subset <var>W</var> of a vector space <var>V</var> is a subspace of <var>V</var> if and only if
        <section>
          span(W) = W
        </section>
        Proof:
        If a subset <var>W</var> of a vector space <var>V</var> is a subspace of <var>V</var>, then span(W) = <var>W</var>, because
        <section>
          <section>
            <table>
              <tr>
                <td>W &subseteq; span(W)</td>
                <td>by Lemma (span contains spanning set)</td>
              </tr>
              <tr>
                <td>span(W) &subseteq; W</td>
                <td>by Lemma (spanning set contains span)</td>
              </tr>
            </table>
          </section>
          Conversely, we know from Lemma (span is subspace) that span(W) is a subspace of <var>V</var> (since W &subseteq; V), hence <var>W</var> is too.
          &marker;
        </section>
      </p>
    </section>
    <section id="subset-and-span" class="proposition">
      <p>
        <strong>Proposition (exercise 1.4.13: subset and span)</strong>:
        Let <var>S<sub>1</sub></var> and <var>S<sub>2</sub></var> be subsets of a vector space <var>V</var>, then
        <section>
          if S<sub>1</sub> &subseteq; S<sub>2</sub>,
          then span(S<sub>1</sub>) &subseteq; span(S<sub>2</sub>)
        </section>
        Proof:
        Let <var>v<sub>1</sub></var>, <var>v<sub>2</sub></var>, &hellip;, <var>v<sub>n</sub></var> &in; S<sub>1</sub>, then some linear combination <var>v</var> in span(S<sub>1</sub>)
        <section>
          <p>
            <section>
              v = a<sub>1</sub>v<sub>1</sub> + a<sub>2</sub>v<sub>2</sub> + &ctdot; + a<sub>n</sub>v<sub>n</sub>
            </section>
            is also a linear combination of <var>S<sub>2</sub></var> because <var>v<sub>1</sub></var>, <var>v<sub>2</sub></var>, &hellip;, <var>v<sub>n</sub></var> &in; S<sub>2</sub>.
            &marker;
          </p>
        </section>
      </p>
    </section>
  </section>
  <section id="linear-dependence">
    <h2>Linear Dependence and Linear Independence</h2>
    <section id="intro-linear-dependence" class="description">
      <p>
        Let <var>W</var> be a subspace of a vector space <var>V</var>. It is desirable to find a small finite subset <var>S</var> of <var>W</var> that generates <var>W</var>. (&hellip;) The search for this subset is related to the question of whether or not some vector in <var>S</var> is a linear combination of the other vectors in <var>S</var>.
      </p>
      <p>
        Rather than asking whether some vector in <var>S</var> is a linear combination of the other vectors in <var>S</var>, it is more efficient to ask whether the zero vector can be expressed as a linear combination of the vectors in <var>S</var> with coefficients that are not all zero.
      </p>
    </section>
    <section id="define-linearly-dependent" class="definition">
      <p>
        <strong>Definition (linearly dependent [線形従属])</strong>:
        A subset <var>S</var> of a vector space <var>V</var> is called linearly dependent if there exist a finite number of distinct vectors <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> &in; <var>S</var> and scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>, not all zero, such that
        <section>
          a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
        </section>
      </p>
    </section>
    <section id="trivial-representation" class="description">
      <p>
        <strong>Trivial representation</strong>:
        For any vectors <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var>, we have
        <section>
          a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
        </section>
        if <var>a<sub>1</sub></var> = <var>a<sub>2</sub></var> = &ctdot; = <var>a<sub>n</sub></var> = 0. We call this the trivial representation of <var>0</var> as a linear combination of  <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var>.
      </p>
    </section>
    <section id="zero-vector-linearly-dependent" class="proposition">
      <p>
        <strong>Proposition (zero vector linearly dependent)</strong>:
        Any set that contains the zero vector is linearly dependent.
      </p>
      <p>
        Proof: It is immediate that <var>a0</var> = <var>0</var> for any scalar <var>a</var>. Because it is a nontrivial representation of the zero vector as the sole linear combination of vectors in the set, the set is linearly dependent.
      </p>
    </section>
    <section id="linearly-dependent-sets" class="proposition">
      <p>
        <strong>Proposition (linearly dependent sets)</strong>:
        A set is linearly dependent if only if
        <section>
          at least one of the vectors in the set is expressible as a linear combination of the other vectors in the set.
        </section>
        Proof:
        Let <var>S</var> be a set and {u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>n</sub>} &in; <var>S</var>.
        <section>
          <p>
            If <var>S</var> is linearly dependent, then there exist a nontrivial representation of <var>0</var> as a linear combination of vectors in <var>S</var> for some scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>, not all zero, such that
            <section>
              a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
            </section>
            Let <var>a<sub>1</sub></var> be a nonzero scalar, then a vector <var>u<sub>1</sub></var> &in; <var>S</var>
            <section>
              u<sub>1</sub> = &minus;1 &frasl; a<sub>1</sub> (
                a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
              )
            </section>
            can be expressed as a linear combination of the other vectors in <var>S</var>.
          </p>
          <p>
            Conversely, if at least one of the vectors in <var>S</var>, say <var>u<sub>1</sub></var>, is expressible as a linear combination of the other vectors <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</var> for some scalars <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var> such that
            <section>
              u<sub>1</sub> =
              a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
            </section>
            then from the expression, we have a representation of <var>0</var> as a linear combination of vectors in <var>S</var> such that
            <section>
              0 = (&minus;1)u<sub>1</sub> +
              a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
            </section>
            Because the coefficient of <var>u<sub>1</sub></var> is nonzero, <var>S</var> is linear dependent. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="linearly-independent" class="definition">
      <p>
        <strong>Definition (linearly independent [線形独立])</strong>:
        A subset <var>S</var> of a vector space that is not linearly dependent is called linearly independent, that is
        <section>
          <p>
            the only representations of the zero vector as linear combinations of its vectors are trivial representations.
          </p>
        </section>
        For convenience, we define that
        <section>
          the empty set &empty; is linearly independent.
        </section>
      </p>
    </section>
    <section id="single-vector-linearly-independent" class="proposition">
      <p>
        <strong>Proposition (single vector linearly independent)</strong>:
        Consider a set containing only one nonzero vector <var>v</var>. Then {v} is linearly independent.
      </p>
      <p>
        Proof: Since the sole vector <var>v</var> in the set is not zero vector, <var>av</var> = 0 for some scalar <var>a</var> &mdash; the only representation of the zero vector as linear combination of the vector in the set &mdash; holds only when <var>a</var> = 0. Hence the set is linearly independent. &marker;
      </p>
    </section>
    <section id="superset-dependent" class="proposition">
      <p>
        <strong>Theorem (1.6: superset linearly dependent)</strong>:
        Let <var>S<sub>1</sub></var> and <var>S<sub>2</sub></var> be subsets of a vector space <var>V</var> such that S<sub>1</sub> &subseteq; S<sub>2</sub> &subseteq; V, then
        <section>
          <table>
            <tr>
              <td>S<sub>1</sub> is linearly dependent</td>
              <td>&Implies;</td>
              <td>S<sub>2</sub> is linearly dependent</td>
            </tr>
          </table>
        </section>
        Proof:
        <section>
          Since <var>S<sub>1</sub></var> is linearly dependent, there exist a finite number of distinct vectors <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> &in; <var>S<sub>1</sub></var> and scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>, not all zero, such that
          <section>
            a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> = 0
          </section>
          <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> &in; <var>S<sub>2</sub></var>, hence <var>S<sub>2</sub></var> is also linear dependent.
        </section>
      </p>
    </section>
    <section id="subset-independent" class="proposition">
      <p>
        <strong>Corollary (subset linearly independent)</strong>:
        Let <var>S<sub>1</sub></var> and <var>S<sub>2</sub></var> be subsets of a vector space <var>V</var> such that S<sub>1</sub> &subseteq; S<sub>2</sub> &subseteq; V, then
        <section>
          <table>
            <tr>
              <td>S<sub>2</sub> is linearly independent</td>
              <td>&Implies;</td>
              <td>S<sub>1</sub> is linearly independent</td>
            </tr>
          </table>
        </section>
        Proof: It holds because
        <ul>
          <li>
            a subset that is not linearly dependent is linearly independent, and
          </li>
          <li>
            the second implication is the contrapositive of first implication.
            &marker;
          </li>
        </ul>
      </p>
    </section>
    <section id="the-smallest-generating-set" class="description">
      <p>
        <strong>The smallest generating set</strong>:
        The issue of whether <var>S</var> is the smallest generating set for its span is related to the question of whether <var>S</var> is linearly dependent.
      </p>
      <p>
        If <var>S</var> is linearly dependent set containing two or more vectors, then some vector <var>v</var> &in; <var>S</var> can be written as a linear combination of the other vectors in <var>S</var>. In other words,
        <section>
          <table>
            <tr>
              <td>
                if <var>S</var> is linearly dependent set, then
              </td>
            </tr>
            <tr>
              <td>
                the subset obtained by removing some <var>v</var> &in; <var>S</var> has the same span as <var>S</var>.
              </td>
            </tr>
          </table>
        </section>
        It follows that
        <section>
          <table>
            <tr>
              <td>
                if no proper subset of <var>S</var> generates the span of <var>S</var>, then
              </td>
            </tr>
            <tr>
              <td>
                <var>S</var> must be linearly independent.
              </td>
            </tr>
          </table>
        </section>
      </p>
    </section>
    <section id="linearly-dependent-span" class="proposition">
      <p>
        <strong>Theorem (1.7: linearly dependent span)</strong>:
        Let <var>S</var> be a linearly independent subset of a vector space <var>V</var> and
        <section>
          <table>
            <tr>
              <td>
                <var>u</var> &notin; <var>S</var>
              </td>
              <td>and</td>
              <td>
                <var>u</var> &in; <var>V</var>
              </td>
            </tr>
          </table>
        </section>
        then <var>S</var> &cup; {u} is linearly dependent if and only if
        <section>
          <var>u</var> &in; span(S)
        </section>
        Proof:
        <section>
          <p>
            If <var>S</var> &cup; {u} is linearly dependent, then there are vectors <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> &in; <var>S</var> such that <var>au</var> + <var>a<sub>1</sub>u<sub>1</sub></var> + <var>a<sub>2</sub>u<sub>2</sub></var> + &ctdot; + <var>a<sub>n</sub>u<sub>n</sub></var> = 0 for some nonzero scalars <var>a</var>, <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>n</sub></var>, so
            <section>
              u
              = &minus;1 &frasl; a
                ( a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub> )
            </section>
            is a linear combination of vectors in <var>S</var>, it follows that <var>u</var> &in; span(S).
          </p>
          <p>
            Conversely, if <var>u</var> &in; span(S), then there exist vectors <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>m</sub></var> &in; <var>S</var> and scalars <var>a<sub>1</sub></var>, <var>a<sub>2</sub></var>, &hellip;, <var>a<sub>m</sub></var>
            <section>
              <table>
                <tr>
                  <td>such that</td>
                  <td>
                    <var>u</var>
                    = <var>a<sub>1</sub>u<sub>1</sub></var>
                    + <var>a<sub>2</sub>u<sub>2</sub></var> + &ctdot;
                    + <var>a<sub>m</sub>u<sub>m</sub></var>
                  </td>
                </tr>
                <tr>
                  <td>if follows</td>
                  <td>
                    0
                    = a<sub>1</sub>u<sub>1</sub>
                    + a<sub>2</sub>u<sub>2</sub> + &ctdot;
                    + a<sub>m</sub>u<sub>m</sub> + (&minus;1)u
                  </td>
                </tr>
              </table>
            </section>
            Note that <var>u</var> &ne; <var>u<sub>i</sub></var> (<var>i</var> = 1, 2, &hellip;, <var>m</var>), because <var>u</var> &notin; <var>S</var>. And also note that the coefficient of <var>u</var> in this linear combination is &minus;1 (nonzero). Thus {u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>m</sub>, u} is linearly dependent, it follows that <var>S</var> &cup; {u} is also linearly dependent by Theorem (linearly dependent span).
            &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="scalar-multiple-linearly-dependent" class="proposition">
      <p>
        <strong>
          Proposition (exercise 1.5.9: scalar multiple linearly dependent)
        </strong>:
        Let <var>u</var> and <var>v</var> be distinct vectors in a vector space <var>V</var>. Then {u, v} is linearly dependent if and only if
        <section>
          <var>u</var> and <var>v</var> is a multiple of the other
        </section>
      </p>
      <p>
        Proof:
        <section>
          <p>
            If {u, v} is linearly dependent, there exists a nontrivial representation of <var>0</var> as a linear combination for some nonzero scalars <var>a</var> and <var>b</var> such that
            <section>
              <table>
                <tr>
                  <td>au + bv = 0</td>
                  <td>it follows</td>
                </tr>
                <tr>
                  <td>u = (&minus; b &frasl; a) v</td>
                  <td>because <var>a</var> &ne; 0</td>
                </tr>
              </table>
            </section>
            Thus, <var>u</var> and <var>v</var> is a multiple of the other.
            Conversely, if <var>u</var> and <var>v</var> is a multiple of the other, then
            <section>
              <table>
                <tr>
                  <td>u = av</td>
                  <td>it follows</td>
                </tr>
                <tr>
                  <td>0 = (&minus;1)u + av</td>
                  <td>which is nontrivial</td>
                </tr>
              </table>
            </section>
            Thus, {u, v} is linearly dependent. &marker;
          </p>
        </section>
      </p>
    </section>
  </section>
  <section id="bases-and-dimension">
    <h2>Bases and Dimension</h2>
    <section id="define-basis" class="definition">
      <p>
        <strong>Definition (basis [基底])</strong>:
        A subset <var>&Beta;</var> of a vector space <var>V</var> is called a basis for <var>V</var> if
        <ul>
          <li><var>&Beta; is linearly independent</var></li>
          <li>
            <var>&Beta;</var> generates <var>V</var>
            &ctdot; i.e., 
            span(&Beta;) = <var>V</var>
          </li>
        </ul>
        We also say that <var>&Beta;</var> forms a basis for <var>V</var>.
      </p>
    </section>
    <section id="basis-is-building-block" class="description">
      <p>
        With a linearly independent generating set &mdash;a basis &mdash; for a vector space, every vector in the vector space can be expressed in one and only one way as a linear combination of the vectors in the basis. This makes linearly independent generating sets the building blocks of vector spaces.
      </p>
    </section>
    <section id="basis-of-zero-vector-space" class="proposition">
      <p>
        <strong>Proposition (example 1.6.1: basis of zero vector space)</strong>:
        The empty set &empty; is a basis for the zero vector space &mdash; {0}.
      </p>
      <p>
        Proof:
        Recall that span(&empty;) = {0} by definition of the zero span.
        <ul>
          <li>
            &empty; is linearly independent by definition of linear independence.
          </li>
          <li>
            span(&empty;) = {0} by definition.
          </li>
        </ul>
      </p>
    </section>
    <section id="basis-unique-linear-combination" class="proposition">
      <p>
        <strong>Theorem (1.8: basis unique linear combination)</strong>:
        Let <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> be distinct vectors in a vector space <var>V</var>, then, <var>&Beta;</var> = { <var>u<sub>1</sub></var>, <var>u<sub>2</sub></var>, &hellip;, <var>u<sub>n</sub></var> } is a basis for <var>V</var> if and only if
        <section>
          each vector <var>u</var> &in; <var>V</var> can be uniquely expressed as a linear combination of vectors of <var>&Beta;</var>.
        </section>
      </p>
      <p>
        Proof:
        Let <var>V</var> be a vector space generated by <var>&Beta;</var> &mdash; that is, span(&Beta;) = <var>V</var>.
        <section>
          <p>
            Let <var>&Beta;</var> be a basis for <var>V</var>. If <var>u</var> &in; <var>V</var>, then <var>u</var> &in; span(&Beta;) because span(&Beta;) = <var>V</var>. Thus <var>u</var> is a linear combination of the vector of <var>&Beta;</var>. Suppose that
            <section>
              <table>
                <tr>
                  <td>u</td>
                  <td>
                    = a<sub>1</sub>u<sub>1</sub> + a<sub>2</sub>u<sub>2</sub> + &ctdot; + a<sub>n</sub>u<sub>n</sub>
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td>
                    = b<sub>1</sub>u<sub>1</sub> + b<sub>2</sub>u<sub>2</sub> + &ctdot; + b<sub>n</sub>u<sub>n</sub>
                  </td>
                </tr>
              </table>
            </section>
            are two such representations of <var>u</var>. Subtracting the second equation from the first gives
            <section>
              0 = (a<sub>1</sub> &minus; b<sub>1</sub>) u<sub>1</sub> + (a<sub>2</sub> &minus; b<sub>2</sub>) u<sub>2</sub> + &ctdot; + (a<sub>n</sub> &minus; b<sub>n</sub>) u<sub>n</sub>
            </section>
            Since <var>&Beta;</var> is linear independent by definition of the basis, it follows that
            <section>
              (a<sub>1</sub> &minus; b<sub>1</sub>) = (a<sub>2</sub> &minus; b<sub>2</sub>) = &ctdot; = (a<sub>n</sub> &minus; b<sub>n</sub>) = 0
            </section>
            Hence
              a<sub>1</sub> = b<sub>1</sub>, a<sub>2</sub> = b<sub>2</sub>, &hellip;, a<sub>n</sub> = b<sub>n</sub>,
            and so <var>u</var> &in; <var>V</var> can be uniquely expressed as a linear combination of vectors of <var>&Beta;</var>.
          </p>
          <p>
            Conversely let <var>u</var> &in; <var>V</var> is uniquely expressible as a linear combination of vectors of <var>&Beta;</var>. In order to prove by contradiction that <var>&Beta;</var> is a basis for <var>V</var>, suppose that <var>&Beta;</var> is not a basis of <var>V</var>, that is, <var>&Beta;</var> is linearly dependent.
            <ul>
              <li>
                We know from Theorem (sets and linearly dependent) that since <var>&Beta;</var> &subseteq; span(&Beta;) = <var>V</var>, the set <var>V</var> is also linearly dependent.
              </li>
              <li>
                Hence from Theorem (linearly dependent sets), at least one of the vectors in <var>V</var>, say <var>u</var>, is expressible as a linear combination of the other vectors in <var>V</var> &mdash; a contradiction to the hypothesis of this implication.
              </li>
            </ul>
            Hence <var>&Beta;</var> is linearly independent and a basis of <var>V</var>. &marker;
          </p>
        </section>
      </p>
    </section>
    <section id="finite-spanning-set" class="proposition">
      <p>
        <strong>Theorem (1.9: finite spanning set)</strong>:
        If a vector space <var>V</var> is generated by a finite set <var>S</var>, then some subset of <var>S</var> is a basis for <var>V</var>. Hence <var>V</var> has a finite basis.
      </p>
      <p>
        Proof:
        <section>
          <p>
            If <var>S</var> = &empty;, then <var>S</var>  is a basis of <var>V</var> = {0}, because
            <ul>
              <li>
                &empty; is linearly independent by definition
              </li>
              <li>
                span(&empty;) = {0} = <var>V</var>
              </li>
            </ul>
          </p>
          <p>
            If <var>S</var> = {0}, then &empty;, a subset of <var>S</var>, is a basis of <var>V</var> = {0}, because
            <ul>
              <li>
                &empty; is linearly independent
              </li>
              <li>
                span({0}) = {0} = <var>V</var>
              </li>
            </ul>
          </p>
          <p>
            If <var>S</var> contains a single nonzero vector, say <var>S</var> = {u}, then <var>V</var> = span{u}, and {u} is a basis for <var>V</var> = span{u}, because
            <ul>
              <li>
                {u} &subseteq; {u} = <var>S</var>
              </li>
              <li>
                {u} is linearly independent by Corollary (single vector linearly independent)
              </li>
            </ul>
          </p>
          <p>
            If <var>S</var> contains <var>k</var> vectors such that { u<sub>1</sub>, u<sub>2</sub>, &hellip;, u<sub>k</sub> } is linearly independent, 
          </p>
        </section>
      </p>
    </section>
  </section>
  <section id="reference">
    <h2>References</h2>
    <ul>
      <li>
        Friedberg, S.H., et al. (2016). <i>Linear Algebra, 5th Edition</i>. Pearson.
      </li>
    </ul>
  </section>
</article>
</body>
</html>
